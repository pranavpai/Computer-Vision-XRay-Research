{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3-qu1ofoz70"
      },
      "source": [
        "# <p style=\"text-align: center; font-size: 30px;\">ðŸ“Š <b>Statistical ML: Group 10 - XRay Project</b> ðŸ“Š</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUZvSUP1oz71"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xv3ZFAnXoz72"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import warnings\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "import matplotlib as plt\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "from torchvision.transforms import ColorJitter, GaussianBlur\n",
        "import torchvision.transforms.functional as TF\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "import glob\n",
        "import pickle\n",
        "import random\n",
        "import cv2\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.models.detection.faster_rcnn import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "from torchvision.ops import MultiScaleRoIAlign\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from collections import defaultdict\n",
        "import itertools\n",
        "from functools import partial\n",
        "import gc\n",
        "import copy\n",
        "from PIL import Image, ImageDraw, ImageFont, ImageEnhance\n",
        "from torchvision.models import resnet50\n",
        "import math\n",
        "import torch.nn.init as init\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "import traceback\n",
        "from itertools import product\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncGVwt8Uoz72"
      },
      "source": [
        "## Setting-Up & Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HqSlLfVmoz72"
      },
      "outputs": [],
      "source": [
        "# Set the environment variable for the VS Code session\n",
        "os.environ['XRAY_DATA_PATH'] = '/Users/pranavpai/Code/StatsML/Statisitcal_Machine_Learning_Project/XRay'\n",
        "#\n",
        "# Use the XRAY_DATA_PATH environment variable\n",
        "base_path = os.getenv('XRAY_DATA_PATH')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uC9IyMuUoz73",
        "outputId": "b2d73fae-2f2a-43e4-9645-c44503f5b520"
      },
      "outputs": [],
      "source": [
        "# Function to convert XML to CSV\n",
        "def xml_to_csv(xml_folder, output_csv):\n",
        "    \"\"\"\n",
        "    Convert XML annotation files to a CSV file and save it to the specified folder.\n",
        "\n",
        "    Parameters:\n",
        "    xml_folder (str): Path to the folder containing XML files.\n",
        "    output_csv (str): Path where the output CSV file will be saved.\n",
        "\n",
        "    Returns:\n",
        "    None: Writes the CSV file to the specified location. If the CSV file exists, exits early and prints a message.\n",
        "    \"\"\"\n",
        "    if os.path.exists(output_csv):\n",
        "        print(f\"{output_csv} already exists. Skipping conversion.\")\n",
        "        return\n",
        "\n",
        "    with open(output_csv, 'w', newline='') as csvfile:\n",
        "        csvwriter = csv.writer(csvfile)\n",
        "        # Write the header row with all the relevant properties\n",
        "        csvwriter.writerow(['filename', 'width', 'height', 'depth', 'class', 'pose', 'truncated', 'difficult', 'xmin', 'ymin', 'xmax', 'ymax'])\n",
        "\n",
        "        # Iterate over all XML files in the folder\n",
        "        for xml_file in os.listdir(xml_folder):\n",
        "            if xml_file.endswith('.xml'):\n",
        "                tree = ET.parse(os.path.join(xml_folder, xml_file))\n",
        "                root = tree.getroot()\n",
        "\n",
        "                # Extract filename, width, height, and depth\n",
        "                filename = root.find('filename').text\n",
        "                size = root.find('size')\n",
        "                width = int(size.find('width').text)\n",
        "                height = int(size.find('height').text)\n",
        "                depth = int(size.find('depth').text)\n",
        "\n",
        "                # Iterate over all object elements in the XML\n",
        "                for obj in root.findall('object'):\n",
        "                    label = obj.find('name').text\n",
        "                    pose = obj.find('pose').text\n",
        "                    truncated = int(obj.find('truncated').text)\n",
        "                    difficult = int(obj.find('difficult').text)\n",
        "                    bbox = obj.find('bndbox')\n",
        "                    xmin = int(bbox.find('xmin').text)\n",
        "                    ymin = int(bbox.find('ymin').text)\n",
        "                    xmax = int(bbox.find('xmax').text)\n",
        "                    ymax = int(bbox.find('ymax').text)\n",
        "\n",
        "                    # Write data to CSV\n",
        "                    csvwriter.writerow([filename, width, height, depth, label, pose, truncated, difficult, xmin, ymin, xmax, ymax])\n",
        "\n",
        "    print(f\"{output_csv} has been created successfully.\")\n",
        "\n",
        "if base_path is None:\n",
        "    raise EnvironmentError(\"Environment variable 'XRAY_DATA_PATH' is not set.\")\n",
        "\n",
        "# Construct paths using the base_path from the environment variable\n",
        "train_xml_folder = os.path.join(base_path, 'Train', 'Annotations')\n",
        "test_xml_folder = os.path.join(base_path, 'Test', 'Annotations')\n",
        "\n",
        "# Output CSV paths\n",
        "train_output_csv = os.path.join(base_path, 'Train', 'train_annotations.csv')\n",
        "test_output_csv = os.path.join(base_path, 'Test', 'test_annotations.csv')\n",
        "\n",
        "# Convert XML to CSV for train and test datasets\n",
        "xml_to_csv(train_xml_folder, train_output_csv)\n",
        "xml_to_csv(test_xml_folder, test_output_csv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5ENfmK1oz73",
        "outputId": "dbea6270-d681-4893-99fc-fcbcf2b2846a"
      },
      "outputs": [],
      "source": [
        "# Load and display stats for Annotation CSV, and return DataFrame\n",
        "def compute_csv_stats(csv_path, name):\n",
        "    \"\"\"\n",
        "    Load a CSV file, compute basic statistics, and return the data as a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    csv_path (str): The path to the CSV file containing annotations.\n",
        "    name (str): Name to use in the printed output for the dataset.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing the loaded CSV data with computed stats.\n",
        "    Returns None if an error occurs while loading the CSV.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        # Load the CSV into a DataFrame\n",
        "        df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Display basic stats\n",
        "        print(f\"--- Stats for {name} ---\")\n",
        "        print(f\"Total Rows: {len(df)}\")\n",
        "        print(f\"Unique Images: {df['filename'].nunique()}\")\n",
        "\n",
        "        # Show a summary of the dataset (describe numeric fields)\n",
        "        print(\"\\nSummary Statistics:\")\n",
        "        print(df.describe())\n",
        "\n",
        "        # Add a blank line after each section for readability\n",
        "        print(\"\\n\" * 3)\n",
        "\n",
        "        # Return the DataFrame for later use\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {name}: {e}\")\n",
        "        return None  # Return None if there's an error\n",
        "\n",
        "# Now you can call the function and store the DataFrames\n",
        "df_train_annotations = compute_csv_stats(train_output_csv, 'Training Annotations')\n",
        "df_test_annotations = compute_csv_stats(test_output_csv, 'Testing Annotations')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "N9r2YOPSoz73",
        "outputId": "060d5cba-a8e6-4fa9-bcb7-251d4d8ac57c"
      },
      "outputs": [],
      "source": [
        "# First 5 Rows of the Annotation dataframe (Train)\n",
        "df_train_annotations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "RqjyerKWoz73",
        "outputId": "a1bb1b74-7c23-4ba6-adf4-395eafcb955e"
      },
      "outputs": [],
      "source": [
        "# First 5 Rows of the Annotation dataframe (Test)\n",
        "df_test_annotations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aQthJSzoz73",
        "outputId": "4733c5c9-c4c2-44a9-f3a8-47856b3a6e4e"
      },
      "outputs": [],
      "source": [
        "# Function to create a DataFrame for image statistics and display stats using PyTorch\n",
        "def get_image_dataframe(image_folder, name):\n",
        "    \"\"\"\n",
        "    Create a DataFrame containing image statistics and the actual image tensors for easy access.\n",
        "\n",
        "    Parameters:\n",
        "    image_folder (str): The path to the folder containing images.\n",
        "    name (str): Name to use in the printed output for the dataset.\n",
        "\n",
        "    Returns:\n",
        "    pd.DataFrame: A DataFrame containing image filename, width, height, mode, and tensor of images.\n",
        "    \"\"\"\n",
        "\n",
        "    image_stats = []\n",
        "\n",
        "    # Iterate over all images in the folder\n",
        "    for image_file in os.listdir(image_folder):\n",
        "        if image_file.endswith(\".jpg\"):  # Process only jpg images\n",
        "            image_path = os.path.join(image_folder, image_file)\n",
        "\n",
        "            try:\n",
        "                # Open the image using torchvision\n",
        "                image = read_image(image_path)  # Loads the image as a tensor (C x H x W)\n",
        "\n",
        "                # Get image dimensions\n",
        "                height, width = image.shape[1], image.shape[2]  # (height, width from tensor)\n",
        "\n",
        "                # Mode isn't as explicit in PyTorch as it is in PIL, so we assume all images are RGB for simplicity\n",
        "                mode = \"RGB\" if image.shape[0] == 3 else \"Grayscale\"\n",
        "\n",
        "                # Append the image stats along with the actual image tensor\n",
        "                image_stats.append([image_file, width, height, mode, image])\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {image_file}: {e}\")\n",
        "\n",
        "    # Convert the collected image stats into a DataFrame\n",
        "    df_image_stats = pd.DataFrame(image_stats, columns=['filename', 'width', 'height', 'mode', 'tensor'])\n",
        "\n",
        "    # Display basic stats\n",
        "    print(f\"--- Stats for {name} ---\")\n",
        "    print(f\"Unique Images: {df_image_stats['filename'].nunique()}\")\n",
        "\n",
        "    # Show a summary of image dimensions\n",
        "    print(\"\\nSummary of Image Dimensions:\")\n",
        "    print(df_image_stats[['width', 'height']].describe())\n",
        "\n",
        "    # Add a blank line after each section for readability\n",
        "    print(\"\\n\" * 3)\n",
        "\n",
        "    return df_image_stats\n",
        "\n",
        "# Define the paths to the image folders\n",
        "train_image_folder = os.path.join(base_path, 'Train', 'Images')\n",
        "test_image_folder = os.path.join(base_path, 'Test', 'Images')\n",
        "\n",
        "# Get DataFrame for train and test images and store the image tensors in the DataFrame\n",
        "df_train_images = get_image_dataframe(train_image_folder, 'Training Images')\n",
        "df_test_images = get_image_dataframe(test_image_folder, 'Testing Images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "LpuU4ExYoz73",
        "outputId": "16fbeff9-52b3-4dca-c060-a33c66ee939c"
      },
      "outputs": [],
      "source": [
        "# Function to visualize an image tensor\n",
        "def visualize_image(image_tensor, title=\"Image\"):\n",
        "    \"\"\"\n",
        "    Visualize a tensor image using matplotlib within the given subplot.\n",
        "\n",
        "    Parameters:\n",
        "    image_tensor (torch.Tensor): The image tensor to visualize.\n",
        "    title (str): The title of the image being displayed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check if the tensor is grayscale or RGB\n",
        "        if image_tensor.shape[0] == 3:  # RGB\n",
        "            # Convert tensor to numpy (C x H x W) -> (H x W x C)\n",
        "            image_np = image_tensor.permute(1, 2, 0).numpy()\n",
        "        elif image_tensor.shape[0] == 1:  # Grayscale\n",
        "            # Squeeze the extra dimension for grayscale\n",
        "            image_np = image_tensor.squeeze(0).numpy()\n",
        "            plt.gray()  # Set the colormap to gray for visualization\n",
        "        else:\n",
        "            raise ValueError(\"Unexpected number of channels in the image tensor.\")\n",
        "\n",
        "        # Plot the image in the current axis\n",
        "        plt.imshow(image_np)\n",
        "        plt.title(title)\n",
        "        plt.axis('off')  # Hide axis\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error visualizing image: {e}\")\n",
        "\n",
        "# Example: Visualize an image from the training set with a check for valid index\n",
        "i = 7000  # or any other index\n",
        "\n",
        "# Ensure 'i' is within bounds\n",
        "if i < len(df_train_images):\n",
        "    visualize_image(df_train_images['tensor'][i], title=df_train_images['filename'][i])\n",
        "else:\n",
        "    print(f\"Index {i} is out of range. The dataset contains {len(df_train_images)} images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 499
        },
        "id": "z8xeSMr2oz74",
        "outputId": "2128fdb9-0360-4901-ab32-fb829f8fc81d"
      },
      "outputs": [],
      "source": [
        "def visualize_image_with_bboxes(annotations_df, images_df, filename, ax=None):\n",
        "    \"\"\"\n",
        "    Visualize an image with bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "    - annotations_df (pd.DataFrame): DataFrame containing annotations.\n",
        "    - images_df (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - filename (str): Filename of the image to visualize.\n",
        "    - ax (matplotlib.axes.Axes, optional): Axes object to plot on.\n",
        "    \"\"\"\n",
        "    # Get all annotations for the specified filename\n",
        "    annotations = annotations_df[annotations_df['filename'] == filename.replace('_block', '')]\n",
        "\n",
        "    if annotations.empty:\n",
        "        print(f\"No annotations found for {filename}.\")\n",
        "        if ax is not None:\n",
        "            ax.axis('off')\n",
        "        return\n",
        "\n",
        "    # Get the corresponding image tensor\n",
        "    image_row = images_df[images_df['filename'] == filename]\n",
        "    if image_row.empty:\n",
        "        print(f\"Image {filename} not found in images_df.\")\n",
        "        if ax is not None:\n",
        "            ax.axis('off')\n",
        "        return\n",
        "    image_data = image_row['tensor'].values[0]\n",
        "\n",
        "    # Convert tensor to image (assumes image is in the format of [C, H, W])\n",
        "    transform = transforms.ToPILImage()\n",
        "    image = transform(image_data)\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(8, 6))\n",
        "\n",
        "    # Plot the image\n",
        "    ax.imshow(image)\n",
        "\n",
        "    # Iterate over all bounding boxes\n",
        "    for _, row in annotations.iterrows():\n",
        "        xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n",
        "        class_label = row['class']\n",
        "\n",
        "        # Draw bounding box\n",
        "        rect = plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                             linewidth=2, edgecolor='red', facecolor='none')\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        # Add class label\n",
        "        ax.text(xmin, ymin, class_label, color='white', fontsize=12,\n",
        "                 bbox=dict(facecolor='red', alpha=0.5))\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_title(filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "visualize_image_with_bboxes(df_train_annotations, df_train_images, 'P05164.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "14rn4c_soz74",
        "outputId": "2c78ed1e-d638-41ff-fdc2-a05a7ef7cefc"
      },
      "outputs": [],
      "source": [
        "# First 5 Rows of the Image dataframe (Train)\n",
        "df_train_images.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "MvKYMa1Yoz74",
        "outputId": "fd719447-b5ee-4e3b-bf43-79709e9d81e0"
      },
      "outputs": [],
      "source": [
        "# First 5 Rows of the Image dataframe (Test)\n",
        "df_test_images.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57dggmLioz74",
        "outputId": "0c0e2d5a-8e19-4eaa-8acd-11dc264a0ec6"
      },
      "outputs": [],
      "source": [
        "# Checking the output of the tensor\n",
        "df_train_images.iloc[0]['tensor']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHmtiH5Hoz74",
        "outputId": "34de6cf5-8660-44e2-8fb6-5342aee2c3af"
      },
      "outputs": [],
      "source": [
        "def compute_item_range(df_train_annotations):\n",
        "    items_per_image = df_train_annotations.groupby('filename').size()\n",
        "    min_items = items_per_image.min()\n",
        "    max_items = items_per_image.max()\n",
        "    item_counts = items_per_image.value_counts().sort_index()\n",
        "\n",
        "    print(f\"Minimum number of items in any image: {min_items}\")\n",
        "    print(f\"Maximum number of items in any image: {max_items}\")\n",
        "    print(\"\\nNumber of images at each occlusion level (number of items):\")\n",
        "    for num_items, count in item_counts.items():\n",
        "        print(f\"{num_items} item: {count} image\")\n",
        "    return min_items, max_items, item_counts\n",
        "\n",
        "min_items, max_items, item_counts = compute_item_range(df_train_annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tgzSdv3joz74",
        "outputId": "835b3450-7b61-4e7e-e191-70cc72841354"
      },
      "outputs": [],
      "source": [
        "visualize_image_with_bboxes(df_train_annotations, df_train_images, 'P00035.jpg')\n",
        "visualize_image_with_bboxes(df_train_annotations, df_train_images, 'P00800.jpg')\n",
        "visualize_image_with_bboxes(df_train_annotations, df_train_images, 'P00410.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb_gbHsToz74"
      },
      "source": [
        "## Data Pre-Processing and Feature Generation Steps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9eyGhksoz74"
      },
      "source": [
        "### White Space Removal and Resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VeyjwuOGoz74"
      },
      "outputs": [],
      "source": [
        "torch_to_np = lambda tse : tse.permute(1, 2, 0).numpy()\n",
        "np_to_torch = lambda npa : torch.from_numpy(npa).permute(2, 0, 1)\n",
        "\n",
        "def show_image(pic, convert = False):\n",
        "  print(f\"dtype : {pic.dtype}\")\n",
        "  print(f\"Shape : {pic.shape}\")\n",
        "  print(f\"Aspect Ratio (h/w) : {round(pic.shape[0]/pic.shape[1], 2)}\")\n",
        "  plt.imshow(pic)\n",
        "  plt.show()\n",
        "\n",
        "def remove_white(im, do_vertical = True, do_horizontal = True, v_whiteness_threshold = 80, h_whiteness_threshold = 80, seg_wid = 10, whiteness_tolerance = 33):\n",
        "  if (do_vertical):\n",
        "    img_hig = im.shape[0]\n",
        "    num_of_seg = int(img_hig/seg_wid)\n",
        "    seg_num, iter, br, all_where = 0, 0, 0, 0\n",
        "    v_dic = dict(zip([i for i in range(num_of_seg)], [-1 for i in range(num_of_seg)]))\n",
        "    while (seg_num <= num_of_seg-br+2):\n",
        "      if (all_where >= img_hig):\n",
        "        break\n",
        "      this_seg = im[seg_num*seg_wid:seg_num*seg_wid+seg_wid,:]\n",
        "      # print(f\"VER this_seg = {this_seg.shape}\") ###############################\n",
        "      if (get_whiteness(this_seg, whiteness_tolerance)>v_whiteness_threshold):\n",
        "        br += 1\n",
        "        im = np.append(im[:seg_num*seg_wid,:], im[seg_num*seg_wid+seg_wid:,:], 0)\n",
        "      else:\n",
        "        v_dic[iter] = iter - br\n",
        "        seg_num += 1\n",
        "      iter += 1\n",
        "      all_where += seg_wid\n",
        "\n",
        "  # print(f\"BEFORE HOR shape of im.shape = {im.shape}\")\n",
        "  if (do_horizontal):\n",
        "    img_wid = im.shape[1]\n",
        "    num_of_seg = int(img_wid/seg_wid)\n",
        "    seg_num, iter, br, all_where = 0, 0, 0, 0\n",
        "    h_dic = dict(zip([i for i in range(num_of_seg)], [-1 for i in range(num_of_seg)]))\n",
        "    while (seg_num <= num_of_seg-br+2):\n",
        "      if (all_where >= img_wid):\n",
        "        break\n",
        "      this_seg = im[:,seg_num*seg_wid:seg_num*seg_wid+seg_wid]\n",
        "      # print(f\"HOR this_seg = {this_seg.shape}\") ###############################\n",
        "      if (get_whiteness(this_seg, whiteness_tolerance)>h_whiteness_threshold):\n",
        "        br += 1\n",
        "        im = np.append(im[:,:seg_num*seg_wid], im[:,seg_num*seg_wid+seg_wid:], 1)\n",
        "      else:\n",
        "        h_dic[iter] = iter - br\n",
        "        seg_num += 1\n",
        "      iter += 1\n",
        "      all_where += seg_wid\n",
        "  max_val_v_dic = max(v_dic.values())\n",
        "  for k, v in v_dic.items():\n",
        "    if (v == 0):\n",
        "      for i in range(k):\n",
        "        v_dic[i] = 0\n",
        "    if (v == max_val_v_dic):\n",
        "      for i in range(k, len(v_dic)):\n",
        "        v_dic[i] = max_val_v_dic\n",
        "  max_val_h_dic = max(h_dic.values())\n",
        "  for k, v in h_dic.items():\n",
        "    if (v == 0):\n",
        "      for i in range(k):\n",
        "        h_dic[i] = 0\n",
        "    if (v == max_val_h_dic):\n",
        "      for i in range(k, len(h_dic)):\n",
        "        h_dic[i] = max_val_h_dic\n",
        "    return (seg_wid, h_dic, v_dic, im)\n",
        "\n",
        "def get_whiteness(im, whiteness_tolerance = 30, mean = (104, 117, 123)):\n",
        "  mn_np = np.array(mean)\n",
        "  if isinstance(im, torch.Tensor):\n",
        "    im = im.numpy()\n",
        "  im  = im + mn_np\n",
        "  im = im.reshape((im.shape[0]*im.shape[1],3))\n",
        "  white_count = 0\n",
        "  for tup in im:\n",
        "    if (255-sum(tup)/3 <= whiteness_tolerance):\n",
        "    # if (sum(tup)/3 <= whiteness_tolerance):\n",
        "      white_count += 1\n",
        "  white_per = 100*white_count/len(im)\n",
        "  # print(f\"white_per = {white_per}\")\n",
        "  return round(white_per, 2)\n",
        "\n",
        "class BaseTransform:\n",
        "    def __init__(self, resize= (584, 688), rgb_means=(104, 117, 123), transform_ = \"medium\"):\n",
        "        self.resize = resize\n",
        "        self.rgb_means = torch.tensor(rgb_means).view(3, 1, 1)\n",
        "        self.transform_ = transform_\n",
        "\n",
        "    def __call__(self, img):\n",
        "        img = self.resize_image(img)\n",
        "\n",
        "        if self.transform_ == \"light\":\n",
        "            img = img + self.rgb_means\n",
        "        elif self.transform_ == \"dark\":\n",
        "            img = img - self.rgb_means\n",
        "\n",
        "        return img\n",
        "\n",
        "    def resize_image(self, img):\n",
        "        img = F.interpolate(img.unsqueeze(0), size= self.resize, mode='bilinear', align_corners=False).squeeze(0)\n",
        "        return img\n",
        "\n",
        "def reflect(old_w, w_max):\n",
        "    mid_w = w_max//2\n",
        "    if(mid_w < old_w):\n",
        "        new_w = old_w - 2*(old_w - mid_w)\n",
        "    else:\n",
        "        new_w = old_w + 2*(mid_w - old_w)\n",
        "    return new_w\n",
        "\n",
        "def get_nearest_obj(tran_dic, seg_X):\n",
        "  seg_X = int(seg_X)\n",
        "  if (tran_dic[seg_X] != -1):\n",
        "    return seg_X\n",
        "  bfl = list(tran_dic.keys())[:seg_X]\n",
        "  bfl = bfl[::-1]\n",
        "  bf_counter = 0\n",
        "  bf_seg = -1\n",
        "  for bf in bfl:\n",
        "    bf_counter += 1\n",
        "    if (tran_dic[bf] != -1):\n",
        "      bf_seg = bf\n",
        "      break\n",
        "  if (bf_seg == -1):\n",
        "    bf_counter = 9999\n",
        "\n",
        "  afl = list(tran_dic.keys())[seg_X+1:]\n",
        "  af_counter = 0\n",
        "  af_seg = -1\n",
        "  for af in afl:\n",
        "    af_counter += 1\n",
        "    if (tran_dic[af] != -1):\n",
        "      af_seg = af\n",
        "      break\n",
        "  if (af_seg == -1):\n",
        "    af_counter = 9999\n",
        "\n",
        "  if (bf_counter <= af_counter):\n",
        "    new_seg = bf_seg\n",
        "  else:\n",
        "    new_seg = af_seg\n",
        "  return new_seg\n",
        "\n",
        "def rotated_this(old_x, old_y, xmax, ymax):\n",
        "  new_org_x, new_org_y = (0, xmax)\n",
        "  return (new_org_x+old_y, new_org_y-old_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JDaIujFMoz74",
        "outputId": "d960b83f-cfe3-4802-b404-d61cfb0bd674"
      },
      "outputs": [],
      "source": [
        "this_pic = df_train_images['tensor'][14]\n",
        "print(this_pic.shape)\n",
        "show_image(this_pic.permute(1, 2, 0), convert = False)\n",
        "hld = 99.9\n",
        "seg_size, h_tran, v_tran, no_whi_np = remove_white(im = this_pic.permute(1, 2, 0), v_whiteness_threshold = hld, h_whiteness_threshold = hld, seg_wid = 1)\n",
        "print(f\"#### white removed no_whi_np.shape = {no_whi_np.shape}\")\n",
        "show_image(no_whi_np)\n",
        "\n",
        "\n",
        "this_pic = df_train_images['tensor'][15]\n",
        "print(this_pic.shape)\n",
        "show_image(this_pic.permute(1, 2, 0), convert = False)\n",
        "hld = 99.9\n",
        "seg_size, h_tran, v_tran, no_whi_np = remove_white(im = this_pic.permute(1, 2, 0), v_whiteness_threshold = hld, h_whiteness_threshold = hld, seg_wid = 1)\n",
        "print(f\"#### white removed no_whi_np.shape = {no_whi_np.shape}\")\n",
        "show_image(no_whi_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "McJOnaTNoz74",
        "outputId": "35145d70-9990-4aca-d831-a0671bb6399c"
      },
      "outputs": [],
      "source": [
        "## RESIZING TEST IMAGES\n",
        "\n",
        "test_count = len(df_test_images)\n",
        "image_tensor_list = []\n",
        "filenames_list = []\n",
        "object_locations_list = []\n",
        "\n",
        "for fi in range(test_count):\n",
        "  ######################################################################################################\n",
        "  ################################################### ORIGINAL ###################################################\n",
        "  ######################################################################################################  \n",
        "  ann_pic = df_test_images['tensor'][fi]\n",
        "  ann_pic_name = df_test_images['filename'][fi]\n",
        "  # print(f\"{fi}/{test_count} <> {ann_pic_name}\")\n",
        "\n",
        "  # show_image(torch_to_np(ann_pic))\n",
        "  filenames_list.append(ann_pic_name)\n",
        "\n",
        "\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### ORIGINAL WITH BOXES ###################################################\n",
        "  ######################################################################################################\n",
        "\n",
        "\n",
        "  # xml_file_path = os.path.join(r\"C:\\Users\\Gaura\\OneDrive\\Desktop\\GAURAV\\Unimleb\\COMP90051\\Statisitcal_Machine_Learning_Project\\XRay\\Train\\Annotations\", ann_pic_name[:-4] + \".xml\")\n",
        "  xml_file_path = os.path.join(base_path, 'Test', 'Annotations', ann_pic_name[:-4] + \".xml\")\n",
        "\n",
        "  tree = ET.parse(xml_file_path)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  # i = 0\n",
        "  # obj_id = []\n",
        "  # for child in root:\n",
        "  #   if (child.tag == \"object\"):\n",
        "  #     obj_id.append(i)\n",
        "  #   i += 1\n",
        "\n",
        "  # dim_dic = {}\n",
        "  # count = 0\n",
        "  # for i in obj_id:\n",
        "  #   dim_dic[count] = {}\n",
        "  #   dim_dic[count][\"xmin\"] = int(root[i][4][0].text)\n",
        "  #   dim_dic[count][\"ymin\"] = int(root[i][4][1].text)\n",
        "  #   dim_dic[count][\"xmax\"] = int(root[i][4][2].text)\n",
        "  #   dim_dic[count][\"ymax\"] = int(root[i][4][3].text)\n",
        "  #   dim_dic[count][\"name\"] = root[i][0].text\n",
        "  #   count += 1\n",
        "  # dim_dic\n",
        "\n",
        "  # boxes = []\n",
        "  # labels = []\n",
        "  # for key, value in dim_dic.items():\n",
        "  #     boxes.append([value['xmin'], value['ymin'], value['xmax'], value['ymax']])\n",
        "  #     labels.append(value['name'])\n",
        "\n",
        "  # boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
        "  # image_with_boxes = draw_bounding_boxes(ann_pic, boxes_tensor, labels=labels, colors=\"red\", width=3)\n",
        "\n",
        "  # show_image(torch_to_np(image_with_boxes))\n",
        "\n",
        "  # image_with_boxes_pil = torchvision.transforms.ToPILImage()(image_with_boxes)\n",
        "  # plt.imshow(image_with_boxes_pil)\n",
        "  # plt.title(\"Image with Bounding Boxes\")\n",
        "  # plt.axis(\"off\")  \n",
        "  # plt.show()\n",
        "\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### NO WHITE SPACES ###################################################\n",
        "  ######################################################################################################\n",
        "\n",
        "  hld = 99.9\n",
        "  seg_size, h_tran, v_tran, no_whi_np = remove_white(im = torch_to_np(ann_pic), v_whiteness_threshold = hld, h_whiteness_threshold = hld, seg_wid = 1)\n",
        "  # show_image(no_whi_np)\n",
        "\n",
        "\n",
        "  ########## EXTRACTING object location from annotation files ##########\n",
        "  i = 0\n",
        "  obj_id = []\n",
        "  for child in root:\n",
        "    if (child.tag == \"object\"):\n",
        "      obj_id.append(i)\n",
        "    i += 1\n",
        "\n",
        "  dim_dic = {}\n",
        "  count = 0\n",
        "  for i in obj_id:\n",
        "    dim_dic[count] = {}\n",
        "    dim_dic[count][\"xmin\"] = int(root[i][4][0].text)\n",
        "    dim_dic[count][\"ymin\"] = int(root[i][4][1].text)\n",
        "    dim_dic[count][\"xmax\"] = int(root[i][4][2].text)\n",
        "    dim_dic[count][\"ymax\"] = int(root[i][4][3].text)\n",
        "    dim_dic[count][\"name\"] = root[i][0].text\n",
        "    count += 1\n",
        "\n",
        "  dim_dic_c = {}\n",
        "  count = 0\n",
        "  for i in obj_id:\n",
        "    dim_dic_c[count] = {}\n",
        "    dim_dic_c[count][\"xmin\"] = int(root[i][4][0].text)\n",
        "    dim_dic_c[count][\"ymin\"] = int(root[i][4][1].text)\n",
        "    dim_dic_c[count][\"xmax\"] = int(root[i][4][2].text)\n",
        "    dim_dic_c[count][\"ymax\"] = int(root[i][4][3].text)\n",
        "    dim_dic_c[count][\"name\"] = root[i][0].text\n",
        "    count += 1\n",
        "  bbox0_list = []\n",
        "  for bb in dim_dic.values():\n",
        "    temp = []\n",
        "    temp.append(bb[\"ymin\"])\n",
        "    temp.append(bb[\"xmin\"])\n",
        "    temp.append(bb[\"ymax\"])\n",
        "    temp.append(bb[\"xmax\"])\n",
        "    bbox0_list.append(temp)\n",
        "  bbox0 = np.array(bbox0_list)\n",
        "  c_bbox = bbox0\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### Objects location after Removing white space ###################################################\n",
        "  ##################################################################################################\n",
        "\n",
        "\n",
        "  w_bbox = []\n",
        "  for box in c_bbox: \n",
        "    temp_box = []\n",
        "    y_min = box[0]\n",
        "    x_min = box[1]\n",
        "    y_max = box[2]\n",
        "    x_max = box[3]\n",
        "    y_min_seg = y_min//seg_size\n",
        "    x_min_seg = x_min//seg_size  \n",
        "    y_max_seg = y_max//seg_size\n",
        "    x_max_seg = x_max//seg_size\n",
        "\n",
        "    # y_min_seg = y_min//seg_size\n",
        "    # x_min_seg = x_min//seg_size  \n",
        "    # y_max_seg = y_max//seg_size\n",
        "    # x_max_seg = x_max//seg_size\n",
        "\n",
        "    if (x_min_seg <= 0):\n",
        "      x_min_seg = min(h_tran.keys())\n",
        "    if (y_min_seg <= 0):\n",
        "      y_min_seg = min(v_tran.keys())\n",
        "    if (x_max_seg >= len(h_tran)):\n",
        "      x_max_seg = max(h_tran.keys())\n",
        "    if (y_max_seg >= len(v_tran)):\n",
        "      y_max_seg = max(v_tran.keys())\n",
        "\n",
        "    if (v_tran[y_min_seg] == -1):\n",
        "      y_min_seg = get_nearest_obj(tran_dic = v_tran, seg_X = y_min_seg)\n",
        "    if (h_tran[x_min_seg] == -1):\n",
        "      x_min_seg = get_nearest_obj(tran_dic = h_tran, seg_X = x_min_seg)\n",
        "    if (v_tran[y_max_seg] == -1):\n",
        "      y_max_seg = get_nearest_obj(tran_dic = v_tran, seg_X = y_max_seg)\n",
        "    if (h_tran[x_max_seg] == -1):\n",
        "      x_max_seg = get_nearest_obj(tran_dic = h_tran, seg_X = x_max_seg)\n",
        "\n",
        "    w_y_min = seg_size*v_tran[y_min_seg] + y_min%seg_size\n",
        "    w_x_min = seg_size*h_tran[x_min_seg] + x_min%seg_size\n",
        "    w_y_max = seg_size*v_tran[y_max_seg] + y_max%seg_size\n",
        "    w_x_max = seg_size*h_tran[x_max_seg] + x_max%seg_size\n",
        "\n",
        "    temp_box.append(w_y_min)\n",
        "    temp_box.append(w_x_min)\n",
        "    temp_box.append(w_y_max)\n",
        "    temp_box.append(w_x_max)\n",
        "    w_bbox.append(temp_box)\n",
        "\n",
        "  # boxes = []\n",
        "  labels = []\n",
        "  for key, value in dim_dic.items():\n",
        "      # boxes.append([value['xmin'], value['ymin'], value['xmax'], value['ymax']])\n",
        "      labels.append(value['name'])\n",
        "\n",
        "  rot_c_bbox = []\n",
        "  for box in w_bbox:\n",
        "      rot_x1, rot_y1 = rotated_this(old_x = box[1], old_y =box[0], xmax=no_whi_np.shape[1], ymax=no_whi_np.shape[0])\n",
        "      rot_x2, rot_y2 = rotated_this(old_x = box[3], old_y =box[2], xmax=no_whi_np.shape[1], ymax=no_whi_np.shape[0])\n",
        "      rot_c_bbox.append([ rot_y2, rot_x1, rot_y1, rot_x2])\n",
        "  W_boxes = [[int(x) for x in row] for row in rot_c_bbox]\n",
        "\n",
        "\n",
        "  h_max, w_max, _ = no_whi_np.shape\n",
        "  res = []\n",
        "  for bx in W_boxes:\n",
        "      wmin_, hmin_, wmax_, hmax_ = bx[0], bx[1], bx[2], bx[3],\n",
        "      this_box_X = [reflect(old_w = wmax_, w_max = w_max),  hmin_, reflect(old_w = wmin_, w_max = w_max),  hmax_]\n",
        "      res.append(this_box_X)\n",
        "\n",
        "  W_boxes = res\n",
        "\n",
        "  # boxes_tensor = torch.tensor(W_boxes, dtype=torch.float32)\n",
        "  # image_with_boxes = draw_bounding_boxes(np_to_torch(no_whi_np), boxes_tensor, labels=labels, colors=\"red\", width=3)\n",
        "  # show_image(torch_to_np(image_with_boxes))\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### Resizing to 584 x 688 ###################################################\n",
        "  ##################################################################################################\n",
        "\n",
        "  # transform = BaseTransform(transform_= \"light\")\n",
        "  transform = BaseTransform(transform_= \"medium\")\n",
        "  # transform = BaseTransform(transform_= \"dark\")\n",
        "  transformed_pic = transform(np_to_torch(no_whi_np))\n",
        "\n",
        "  transformed_pic_np = torch_to_np(transformed_pic)\n",
        "  Hratio = transformed_pic_np.shape[1]/no_whi_np.shape[1]\n",
        "  Wratio = transformed_pic_np.shape[0]/no_whi_np.shape[0]\n",
        "\n",
        "  ratioLst = [Hratio, Wratio, Hratio, Wratio]\n",
        "  w_bbox_r = []\n",
        "  for box in W_boxes:\n",
        "      box = [abs(int(a * b)) for a, b in zip(box, ratioLst)] \n",
        "      w_bbox_r.append(box)\n",
        "  \n",
        "  final_obj_location_dict = {}\n",
        "  for label_index in range(len(labels)):\n",
        "    final_obj_location_dict[label_index] = [labels[label_index], w_bbox_r[label_index]]\n",
        "\n",
        "  image_tensor_list.append(transformed_pic)\n",
        "  object_locations_list.append(final_obj_location_dict)\n",
        "  print(f\"{fi}/{test_count} <> {ann_pic_name} <> final_obj_location_dict = {final_obj_location_dict}\")\n",
        "  # boxes_tensor = torch.tensor(w_bbox_r, dtype=torch.float32)\n",
        "  # resized_image_with_boxes = draw_bounding_boxes(transformed_pic, boxes_tensor, labels=labels, colors=\"red\", width=3)\n",
        "  # show_image(torch_to_np(resized_image_with_boxes), convert = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###RESIZING TRAIN IMAGES\n",
        "\n",
        "train_count = len(df_train_images)\n",
        "image_tensor_list = []\n",
        "filenames_list = []\n",
        "object_locations_list = []\n",
        "\n",
        "for fi in range(train_count):\n",
        "  ######################################################################################################\n",
        "  ################################################### ORIGINAL ###################################################\n",
        "  ######################################################################################################  \n",
        "  ann_pic = df_train_images['tensor'][fi]\n",
        "  ann_pic_name = df_train_images['filename'][fi]\n",
        "  # print(f\"{fi}/{train_count} <> {ann_pic_name}\")\n",
        "\n",
        "  # show_image(torch_to_np(ann_pic))\n",
        "  filenames_list.append(ann_pic_name)\n",
        "\n",
        "\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### ORIGINAL WITH BOXES ###################################################\n",
        "  ######################################################################################################\n",
        "\n",
        "\n",
        "  # xml_file_path = os.path.join(r\"C:\\Users\\Gaura\\OneDrive\\Desktop\\GAURAV\\Unimleb\\COMP90051\\Statisitcal_Machine_Learning_Project\\XRay\\Train\\Annotations\", ann_pic_name[:-4] + \".xml\")\n",
        "  xml_file_path = os.path.join(base_path, 'Train', 'Annotations', ann_pic_name[:-4] + \".xml\")\n",
        "\n",
        "  tree = ET.parse(xml_file_path)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  # i = 0\n",
        "  # obj_id = []\n",
        "  # for child in root:\n",
        "  #   if (child.tag == \"object\"):\n",
        "  #     obj_id.append(i)\n",
        "  #   i += 1\n",
        "\n",
        "  # dim_dic = {}\n",
        "  # count = 0\n",
        "  # for i in obj_id:\n",
        "  #   dim_dic[count] = {}\n",
        "  #   dim_dic[count][\"xmin\"] = int(root[i][4][0].text)\n",
        "  #   dim_dic[count][\"ymin\"] = int(root[i][4][1].text)\n",
        "  #   dim_dic[count][\"xmax\"] = int(root[i][4][2].text)\n",
        "  #   dim_dic[count][\"ymax\"] = int(root[i][4][3].text)\n",
        "  #   dim_dic[count][\"name\"] = root[i][0].text\n",
        "  #   count += 1\n",
        "  # dim_dic\n",
        "\n",
        "  # boxes = []\n",
        "  # labels = []\n",
        "  # for key, value in dim_dic.items():\n",
        "  #     boxes.append([value['xmin'], value['ymin'], value['xmax'], value['ymax']])\n",
        "  #     labels.append(value['name'])\n",
        "\n",
        "  # boxes_tensor = torch.tensor(boxes, dtype=torch.float32)\n",
        "  # image_with_boxes = draw_bounding_boxes(ann_pic, boxes_tensor, labels=labels, colors=\"red\", width=3)\n",
        "\n",
        "  # show_image(torch_to_np(image_with_boxes))\n",
        "\n",
        "  # image_with_boxes_pil = torchvision.transforms.ToPILImage()(image_with_boxes)\n",
        "  # plt.imshow(image_with_boxes_pil)\n",
        "  # plt.title(\"Image with Bounding Boxes\")\n",
        "  # plt.axis(\"off\")  \n",
        "  # plt.show()\n",
        "\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### NO WHITE SPACES ###################################################\n",
        "  ######################################################################################################\n",
        "\n",
        "  hld = 99.9\n",
        "  seg_size, h_tran, v_tran, no_whi_np = remove_white(im = torch_to_np(ann_pic), v_whiteness_threshold = hld, h_whiteness_threshold = hld, seg_wid = 1)\n",
        "  # show_image(no_whi_np)\n",
        "\n",
        "\n",
        "  ########## EXTRACTING object location from annotation files ##########\n",
        "  i = 0\n",
        "  obj_id = []\n",
        "  for child in root:\n",
        "    if (child.tag == \"object\"):\n",
        "      obj_id.append(i)\n",
        "    i += 1\n",
        "\n",
        "  dim_dic = {}\n",
        "  count = 0\n",
        "  for i in obj_id:\n",
        "    dim_dic[count] = {}\n",
        "    dim_dic[count][\"xmin\"] = int(root[i][4][0].text)\n",
        "    dim_dic[count][\"ymin\"] = int(root[i][4][1].text)\n",
        "    dim_dic[count][\"xmax\"] = int(root[i][4][2].text)\n",
        "    dim_dic[count][\"ymax\"] = int(root[i][4][3].text)\n",
        "    dim_dic[count][\"name\"] = root[i][0].text\n",
        "    count += 1\n",
        "\n",
        "  dim_dic_c = {}\n",
        "  count = 0\n",
        "  for i in obj_id:\n",
        "    dim_dic_c[count] = {}\n",
        "    dim_dic_c[count][\"xmin\"] = int(root[i][4][0].text)\n",
        "    dim_dic_c[count][\"ymin\"] = int(root[i][4][1].text)\n",
        "    dim_dic_c[count][\"xmax\"] = int(root[i][4][2].text)\n",
        "    dim_dic_c[count][\"ymax\"] = int(root[i][4][3].text)\n",
        "    dim_dic_c[count][\"name\"] = root[i][0].text\n",
        "    count += 1\n",
        "  bbox0_list = []\n",
        "  for bb in dim_dic.values():\n",
        "    temp = []\n",
        "    temp.append(bb[\"ymin\"])\n",
        "    temp.append(bb[\"xmin\"])\n",
        "    temp.append(bb[\"ymax\"])\n",
        "    temp.append(bb[\"xmax\"])\n",
        "    bbox0_list.append(temp)\n",
        "  bbox0 = np.array(bbox0_list)\n",
        "  c_bbox = bbox0\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### Objects location after Removing white space ###################################################\n",
        "  ##################################################################################################\n",
        "\n",
        "\n",
        "  w_bbox = []\n",
        "  for box in c_bbox: \n",
        "    temp_box = []\n",
        "    y_min = box[0]\n",
        "    x_min = box[1]\n",
        "    y_max = box[2]\n",
        "    x_max = box[3]\n",
        "    y_min_seg = y_min//seg_size\n",
        "    x_min_seg = x_min//seg_size  \n",
        "    y_max_seg = y_max//seg_size\n",
        "    x_max_seg = x_max//seg_size\n",
        "\n",
        "    # y_min_seg = y_min//seg_size\n",
        "    # x_min_seg = x_min//seg_size  \n",
        "    # y_max_seg = y_max//seg_size\n",
        "    # x_max_seg = x_max//seg_size\n",
        "\n",
        "    if (x_min_seg <= 0):\n",
        "      x_min_seg = min(h_tran.keys())\n",
        "    if (y_min_seg <= 0):\n",
        "      y_min_seg = min(v_tran.keys())\n",
        "    if (x_max_seg >= len(h_tran)):\n",
        "      x_max_seg = max(h_tran.keys())\n",
        "    if (y_max_seg >= len(v_tran)):\n",
        "      y_max_seg = max(v_tran.keys())\n",
        "\n",
        "    if (v_tran[y_min_seg] == -1):\n",
        "      y_min_seg = get_nearest_obj(tran_dic = v_tran, seg_X = y_min_seg)\n",
        "    if (h_tran[x_min_seg] == -1):\n",
        "      x_min_seg = get_nearest_obj(tran_dic = h_tran, seg_X = x_min_seg)\n",
        "    if (v_tran[y_max_seg] == -1):\n",
        "      y_max_seg = get_nearest_obj(tran_dic = v_tran, seg_X = y_max_seg)\n",
        "    if (h_tran[x_max_seg] == -1):\n",
        "      x_max_seg = get_nearest_obj(tran_dic = h_tran, seg_X = x_max_seg)\n",
        "\n",
        "    w_y_min = seg_size*v_tran[y_min_seg] + y_min%seg_size\n",
        "    w_x_min = seg_size*h_tran[x_min_seg] + x_min%seg_size\n",
        "    w_y_max = seg_size*v_tran[y_max_seg] + y_max%seg_size\n",
        "    w_x_max = seg_size*h_tran[x_max_seg] + x_max%seg_size\n",
        "\n",
        "    temp_box.append(w_y_min)\n",
        "    temp_box.append(w_x_min)\n",
        "    temp_box.append(w_y_max)\n",
        "    temp_box.append(w_x_max)\n",
        "    w_bbox.append(temp_box)\n",
        "\n",
        "  # boxes = []\n",
        "  labels = []\n",
        "  for key, value in dim_dic.items():\n",
        "      # boxes.append([value['xmin'], value['ymin'], value['xmax'], value['ymax']])\n",
        "      labels.append(value['name'])\n",
        "\n",
        "  rot_c_bbox = []\n",
        "  for box in w_bbox:\n",
        "      rot_x1, rot_y1 = rotated_this(old_x = box[1], old_y =box[0], xmax=no_whi_np.shape[1], ymax=no_whi_np.shape[0])\n",
        "      rot_x2, rot_y2 = rotated_this(old_x = box[3], old_y =box[2], xmax=no_whi_np.shape[1], ymax=no_whi_np.shape[0])\n",
        "      rot_c_bbox.append([ rot_y2, rot_x1, rot_y1, rot_x2])\n",
        "  W_boxes = [[int(x) for x in row] for row in rot_c_bbox]\n",
        "\n",
        "\n",
        "  h_max, w_max, _ = no_whi_np.shape\n",
        "  res = []\n",
        "  for bx in W_boxes:\n",
        "      wmin_, hmin_, wmax_, hmax_ = bx[0], bx[1], bx[2], bx[3],\n",
        "      this_box_X = [reflect(old_w = wmax_, w_max = w_max),  hmin_, reflect(old_w = wmin_, w_max = w_max),  hmax_]\n",
        "      res.append(this_box_X)\n",
        "\n",
        "  W_boxes = res\n",
        "\n",
        "  # boxes_tensor = torch.tensor(W_boxes, dtype=torch.float32)\n",
        "  # image_with_boxes = draw_bounding_boxes(np_to_torch(no_whi_np), boxes_tensor, labels=labels, colors=\"red\", width=3)\n",
        "  # show_image(torch_to_np(image_with_boxes))\n",
        "\n",
        "  ######################################################################################################\n",
        "  ################################################### Resizing to 584 x 688 ###################################################\n",
        "  ##################################################################################################\n",
        "\n",
        "  # transform = BaseTransform(transform_= \"light\")\n",
        "  transform = BaseTransform(transform_= \"medium\")\n",
        "  # transform = BaseTransform(transform_= \"dark\")\n",
        "  transformed_pic = transform(np_to_torch(no_whi_np))\n",
        "\n",
        "  transformed_pic_np = torch_to_np(transformed_pic)\n",
        "  Hratio = transformed_pic_np.shape[1]/no_whi_np.shape[1]\n",
        "  Wratio = transformed_pic_np.shape[0]/no_whi_np.shape[0]\n",
        "\n",
        "  ratioLst = [Hratio, Wratio, Hratio, Wratio]\n",
        "  w_bbox_r = []\n",
        "  for box in W_boxes:\n",
        "      box = [abs(int(a * b)) for a, b in zip(box, ratioLst)] \n",
        "      w_bbox_r.append(box)\n",
        "  \n",
        "  final_obj_location_dict = {}\n",
        "  for label_index in range(len(labels)):\n",
        "    final_obj_location_dict[label_index] = [labels[label_index], w_bbox_r[label_index]]\n",
        "\n",
        "  image_tensor_list.append(transformed_pic)\n",
        "  object_locations_list.append(final_obj_location_dict)\n",
        "  print(f\"{fi}/{train_count} <> {ann_pic_name} <> final_obj_location_dict = {final_obj_location_dict}\")\n",
        "  # boxes_tensor = torch.tensor(w_bbox_r, dtype=torch.float32)\n",
        "  # resized_image_with_boxes = draw_bounding_boxes(transformed_pic, boxes_tensor, labels=labels, colors=\"red\", width=3)\n",
        "  # show_image(torch_to_np(resized_image_with_boxes), convert = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "vqSvEp84oz75"
      },
      "outputs": [],
      "source": [
        "# # Saving files in pickle\n",
        "\n",
        "# data = {\n",
        "#     \"file_names\": image_tensor_list,\n",
        "#     \"object_locations\": object_locations_list,\n",
        "#     \"images\": image_tensor_list\n",
        "# }\n",
        "\n",
        "# with open('data.pkl', 'wb') as f:\n",
        "#     pickle.dump(data, f)\n",
        "\n",
        "# print(\"Data saved to pickle file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYpBYA5soz75"
      },
      "source": [
        "### Loading the Resized Images (pkl file) and Updating our dataframe (TRAINING & TESTING DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "7mbzK6Jnoz75"
      },
      "outputs": [],
      "source": [
        "def load_pickle_file(base_path, file_name):\n",
        "    # Create the full path to the pickle file\n",
        "    pickle_file_path = os.path.join(base_path, file_name)\n",
        "\n",
        "    # Open the pickle file and load the data\n",
        "    with open(pickle_file_path, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "\n",
        "    # Print the keys of the loaded pickle data for reference\n",
        "    print(f\"Keys in {file_name}: {data.keys()}\")\n",
        "\n",
        "    # Return the loaded data\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TIv5mhIIoz75"
      },
      "outputs": [],
      "source": [
        "def extract_pickle_data(data):\n",
        "    # Extract filenames, resized images, and object locations from the loaded data\n",
        "    filenames = data['file_names']\n",
        "    resized_images = data['images']\n",
        "    object_location = data['object_locations']\n",
        "\n",
        "    # Return the extracted data\n",
        "    return filenames, resized_images, object_location"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "kQmB9vkkoz75"
      },
      "outputs": [],
      "source": [
        "# Function to update either training or testing image DataFrame\n",
        "def update_image_dataframe(df_images, filenames, resized_images, fixed_width=688, fixed_height=584):\n",
        "    # Loop through each filename in the list to update the corresponding image tensor\n",
        "    for i, filename in enumerate(filenames):\n",
        "        # Find the row in df_images for the current filename\n",
        "        image_row_idx = df_images.index[df_images['filename'] == filename]\n",
        "\n",
        "        # Ensure the filename exists in the DataFrame before updating\n",
        "        if not image_row_idx.empty:\n",
        "            # Get the corresponding resized tensor\n",
        "            resized_tensor = resized_images[i]\n",
        "\n",
        "            # Check if the resized image is a tensor before updating the dataframe\n",
        "            if isinstance(resized_tensor, torch.Tensor):\n",
        "                df_images.at[image_row_idx[0], 'tensor'] = resized_tensor\n",
        "                # Set the fixed width and height values for the image\n",
        "                df_images.at[image_row_idx[0], 'width'] = int(fixed_width)\n",
        "                df_images.at[image_row_idx[0], 'height'] = int(fixed_height)\n",
        "            else:\n",
        "                # Log a message if the resized image is not a tensor\n",
        "                print(f\"Resized image for {filename} is not a tensor.\")\n",
        "        else:\n",
        "            # Log a message if the filename is not found in df_images\n",
        "            print(f\"File {filename} not found in df_images.\")\n",
        "\n",
        "    # Return the updated DataFrame\n",
        "    return df_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "pTWBm0nnoz75"
      },
      "outputs": [],
      "source": [
        "# Function to update either training or testing annotation DataFrame\n",
        "def update_annotation_dataframe(df_annotations, filenames, object_location, fixed_width=688, fixed_height=584):\n",
        "    # Loop through each image filename and its corresponding bounding boxes\n",
        "    for filename, bbox_dict in zip(filenames, object_location):\n",
        "        # Ensure the filename exists in df_annotations\n",
        "        if filename in df_annotations['filename'].values:\n",
        "            # Initialize a counter to track the number of bounding boxes processed for each class\n",
        "            class_counter = {}\n",
        "\n",
        "            # Loop through the bounding boxes and update coordinates for the current image\n",
        "            for bbox_index, (class_label, bbox_coords) in bbox_dict.items():\n",
        "                # Find all rows in df_annotations matching the filename and class label\n",
        "                matching_rows = df_annotations[(df_annotations['filename'] == filename) &\n",
        "                                               (df_annotations['class'] == class_label)]\n",
        "\n",
        "                # Ensure there are rows to update for the current class and filename\n",
        "                if not matching_rows.empty:\n",
        "                    # Initialize or increment the counter for the current class label\n",
        "                    if class_label not in class_counter:\n",
        "                        class_counter[class_label] = 0\n",
        "                    current_idx = class_counter[class_label]\n",
        "\n",
        "                    # Ensure we have enough rows for the number of bounding boxes\n",
        "                    if current_idx < len(matching_rows):\n",
        "                        # Get the index of the row to update and update the bounding box coordinates\n",
        "                        row_to_update = matching_rows.index[current_idx]\n",
        "                        df_annotations.loc[row_to_update, ['xmin', 'ymin', 'xmax', 'ymax']] = bbox_coords\n",
        "\n",
        "                        # Increment the class counter\n",
        "                        class_counter[class_label] += 1\n",
        "\n",
        "                        # Update the height and width for the image in the annotation dataframe\n",
        "                        df_annotations.loc[df_annotations['filename'] == filename, 'height'] = int(fixed_height)\n",
        "                        df_annotations.loc[df_annotations['filename'] == filename, 'width'] = int(fixed_width)\n",
        "                    else:\n",
        "                        # Log a warning if there are more bounding boxes than entries for a class\n",
        "                        print(f\"Warning: More bounding boxes than entries for class '{class_label}' in {filename}.\")\n",
        "                else:\n",
        "                    # Log a message if no matching rows are found for the class and filename\n",
        "                    print(f\"No matching class '{class_label}' found in {filename}.\")\n",
        "\n",
        "    # Return the updated annotation DataFrame\n",
        "    return df_annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl33w7tUoz75",
        "outputId": "587358d8-756c-4aad-a937-ae0ce55d76b8"
      },
      "outputs": [],
      "source": [
        "# Load the pickle data\n",
        "data_train = load_pickle_file(base_path, 'WSR_train_data.pkl')\n",
        "data_test = data = load_pickle_file(base_path, 'WSR_test_data.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Tc8Oql5Aoz75"
      },
      "outputs": [],
      "source": [
        "# Extract necessary fields\n",
        "train_filenames, train_resized_images, train_object_location = extract_pickle_data(data_train)\n",
        "test_filenames, test_resized_images, test_object_location = extract_pickle_data(data_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YGl6Majoz75",
        "outputId": "990ae62d-eee7-44c8-ab72-6db146d69bed"
      },
      "outputs": [],
      "source": [
        "print(data_test.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DLB4tP4oz75",
        "outputId": "1bb190ed-8829-4b1b-dc0e-3354ba9f99d8"
      },
      "outputs": [],
      "source": [
        "print(data_test['images'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2RtQ7S3oz75",
        "outputId": "8b93f70d-400b-4980-91f0-b1fa2ad36e31"
      },
      "outputs": [],
      "source": [
        "# Check the structure of object_locations\n",
        "for i, (filename, bbox_dict) in enumerate(zip(train_filenames, train_object_location)):\n",
        "    print(f\"Image: {filename}\")\n",
        "    print(\"Bounding boxes:\")\n",
        "    print(bbox_dict)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "    if i >= 5:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5LDF77noz75",
        "outputId": "41a47f95-1827-4d0f-8a47-45087ae76b09"
      },
      "outputs": [],
      "source": [
        "# Check the structure of object_locations\n",
        "for i, (filename, bbox_dict) in enumerate(zip(test_filenames, test_object_location)):\n",
        "    print(f\"Image: {filename}\")\n",
        "    print(\"Bounding boxes:\")\n",
        "    print(bbox_dict)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "\n",
        "    if i >= 5:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "wmOfKqsCoz75"
      },
      "outputs": [],
      "source": [
        "# Update the image dataframe\n",
        "df_train_images = update_image_dataframe(df_train_images, train_filenames, train_resized_images)\n",
        "df_test_images = update_image_dataframe(df_test_images, test_filenames, test_resized_images)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VGPsw1f_oz75"
      },
      "outputs": [],
      "source": [
        "# Update the annotation dataframe\n",
        "df_train_annotations = update_annotation_dataframe(df_train_annotations, train_filenames, train_object_location)\n",
        "df_test_annotations = update_annotation_dataframe(df_test_annotations, test_filenames, test_object_location)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "twgIyu_Aoz75",
        "outputId": "210d3b4c-6781-4449-cd85-093cedb898c0"
      },
      "outputs": [],
      "source": [
        "# Df check for resized annotaions\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P00002.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "sote13QXoz75",
        "outputId": "47bfe6db-e8b8-4e6d-ecbb-34171a05a9ac"
      },
      "outputs": [],
      "source": [
        "# Df check for resized image\n",
        "df_train_images[df_train_images['filename'] == 'P00002.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "sR49qSRuoz75",
        "outputId": "c0a667f7-2c8c-4c8f-8d24-c6cef447b633"
      },
      "outputs": [],
      "source": [
        "# Image check with new BoundingBox\n",
        "visualize_image_with_bboxes(df_train_annotations, df_train_images, 'P00002.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "QnHEAK8soz78",
        "outputId": "cf643614-dd15-4a1a-ea01-ed490171583c"
      },
      "outputs": [],
      "source": [
        "# Checking the updated df of training images\n",
        "df_train_images.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Bzf85RXUoz78",
        "outputId": "9c8bdcc5-ca88-4719-8cae-64304bc22cf0"
      },
      "outputs": [],
      "source": [
        "# Checking the updated df of training annotations\n",
        "df_train_annotations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "wwExBeK3oz78",
        "outputId": "7436c55a-eae2-4e4a-ea75-da96bc28c883"
      },
      "outputs": [],
      "source": [
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "VPhVJnGxoz78",
        "outputId": "97b0f07c-d42b-4587-d482-01372094d12c"
      },
      "outputs": [],
      "source": [
        "df_test_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "u4wun-Qsoz78",
        "outputId": "8c312ae0-4390-4d74-edf8-9f0555e868f4"
      },
      "outputs": [],
      "source": [
        "df_test_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "_iAkIBt0oz78",
        "outputId": "7eaa7d50-5f0c-4325-95b8-cb1d0e6e9f6d"
      },
      "outputs": [],
      "source": [
        "df_test_annotations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "y1gUBptgoz78",
        "outputId": "02170301-6cd8-4227-9adf-d29afa634747"
      },
      "outputs": [],
      "source": [
        "df_test_images.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Taking a subset due to limited GPU memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TRAIN SET (Our subest is 1% of the train dataset)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "\n",
        "# Calculate the number of images to select (1% of total)\n",
        "num_images_to_select = int(0.01 * len(df_train_images))\n",
        "\n",
        "# Randomly select 30% of the images\n",
        "selected_filenames = random.sample(list(df_train_images['filename']), num_images_to_select)\n",
        "\n",
        "# Filter df_test_images to keep only the selected images\n",
        "df_train_images = df_train_images[df_train_images['filename'].isin(selected_filenames)].reset_index(drop=True)\n",
        "\n",
        "# Filter df_test_annotations to keep only annotations matching the selected images\n",
        "df_train_annotations = df_train_annotations[df_train_annotations['filename'].isin(selected_filenames)].reset_index(drop=True)\n",
        "\n",
        "# Print the resulting dataframes to verify\n",
        "print(f\"Number of selected images: {len(df_train_images)}\")\n",
        "print(f\"Number of corresponding annotations: {len(df_train_annotations)}\")\n",
        "\n",
        "# Display first few records to verify\n",
        "print(df_train_images.head())\n",
        "print(df_train_annotations.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TEST SET (Our subest is 7% of the test dataset)\n",
        "\n",
        "# Set seed for reproducibility\n",
        "random_seed = 42\n",
        "random.seed(random_seed)\n",
        "\n",
        "# Calculate the number of images to select (10% of total)\n",
        "num_images_to_select = int(0.07 * len(df_test_images))\n",
        "\n",
        "# Randomly select 30% of the images\n",
        "selected_filenames = random.sample(list(df_test_images['filename']), num_images_to_select)\n",
        "\n",
        "# Filter df_test_images to keep only the selected images\n",
        "df_test_images = df_test_images[df_test_images['filename'].isin(selected_filenames)].reset_index(drop=True)\n",
        "\n",
        "# Filter df_test_annotations to keep only annotations matching the selected images\n",
        "df_test_annotations = df_test_annotations[df_test_annotations['filename'].isin(selected_filenames)].reset_index(drop=True)\n",
        "\n",
        "# Print the resulting dataframes to verify\n",
        "print(f\"Number of selected images: {len(df_test_images)}\")\n",
        "print(f\"Number of corresponding annotations: {len(df_test_annotations)}\")\n",
        "\n",
        "# Display first few records to verify\n",
        "print(df_test_images.head())\n",
        "print(df_test_annotations.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b60VQLe7oz78"
      },
      "source": [
        "### Adding ColourJitter'd images to the dataset for generalizibility (10 Added Transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jSc-lHVwoz78",
        "outputId": "8e1b4646-7095-4396-8177-641b6c43ff71"
      },
      "outputs": [],
      "source": [
        "# Use this to add 1000 ColorJitter transformations to the dataset\n",
        "def apply_color_jitter_augmentation(df_train_images, df_train_annotations):\n",
        "    \"\"\"\n",
        "    Applies ColorJitter transformations to 1000 random images from the training dataset.\n",
        "    If the augmentation has already been performed, the function will output a message and some samples.\n",
        "    \n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image data and tensors.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing annotations for images.\n",
        "    \n",
        "    Returns:\n",
        "    - df_train_images (pd.DataFrame): Updated DataFrame with augmented images added.\n",
        "    - df_train_annotations (pd.DataFrame): Updated DataFrame with annotations for augmented images added.\n",
        "    \"\"\"\n",
        "    # Check if augmentation has already been performed by looking for filenames ending with '_jit.jpg'\n",
        "    augmented_filenames = df_train_images[df_train_images['filename'].str.contains('_jit.jpg')]\n",
        "    \n",
        "    if not augmented_filenames.empty:\n",
        "        print(\"ColorJitter augmentation has already been implemented.\")\n",
        "        # Optionally, display some of the already augmented images\n",
        "        display_jittered_images(df_train_images)\n",
        "        return df_train_images, df_train_annotations\n",
        "    \n",
        "    else:\n",
        "        print(\"Starting ColorJitter augmentation...\")\n",
        "        \n",
        "        # Step 1: Select 1000 Random Images from the Training Dataset\n",
        "        # Get unique filenames from the DataFrame\n",
        "        unique_filenames = df_train_images['filename'].unique()\n",
        "        \n",
        "        if len(unique_filenames) < 10:\n",
        "            num_images = len(unique_filenames)\n",
        "            print(f\"Only {num_images} unique images available. Selecting all of them.\")\n",
        "        else:\n",
        "            num_images = 10\n",
        "        \n",
        "        np.random.seed(47) # Set Seed for reproducibility\n",
        "        \n",
        "        random_filenames = np.random.choice(unique_filenames, size=num_images, replace=False)\n",
        "        \n",
        "        # Filter the DataFrame to get the selected images\n",
        "        selected_images_df = df_train_images[df_train_images['filename'].isin(random_filenames)].copy()\n",
        "        \n",
        "        # Step 2: Apply ColorJitter Transformations\n",
        "        # Define the ColorJitter transformation\n",
        "        color_jitter = ColorJitter(brightness=(0.8, 2),\n",
        "                                   contrast=(0.8, 2),\n",
        "                                   saturation=(0.8, 2),\n",
        "                                   hue=(-0.1, 0.1))\n",
        "        \n",
        "        augmented_images = []\n",
        "        augmented_annotations = []\n",
        "        \n",
        "        for idx, row in selected_images_df.iterrows():\n",
        "            original_filename = row['filename']\n",
        "            tensor = row['tensor']\n",
        "            \n",
        "            # Apply ColorJitter transformation\n",
        "            transformed_tensor = color_jitter(tensor)\n",
        "            \n",
        "            # Create a new filename by appending '_jit.jpg'\n",
        "            new_filename = original_filename.replace('.jpg', '_jit.jpg')\n",
        "            \n",
        "            # Update the image data\n",
        "            new_row = row.copy()\n",
        "            new_row['filename'] = new_filename\n",
        "            new_row['tensor'] = transformed_tensor\n",
        "            augmented_images.append(new_row)\n",
        "            \n",
        "            # Step 3: Add the Transformed Images Back to the Dataset\n",
        "            # The annotations remain the same except for the filename\n",
        "            annotations = df_train_annotations[df_train_annotations['filename'] == original_filename].copy()\n",
        "            annotations['filename'] = new_filename\n",
        "            augmented_annotations.append(annotations)\n",
        "        \n",
        "        # Convert the list of augmented images and annotations to DataFrames\n",
        "        augmented_images_df = pd.DataFrame(augmented_images)\n",
        "        augmented_annotations_df = pd.concat(augmented_annotations, ignore_index=True)\n",
        "        \n",
        "        # Add the augmented images and annotations to the original DataFrames\n",
        "        df_train_images = pd.concat([df_train_images, augmented_images_df], ignore_index=True)\n",
        "        df_train_annotations = pd.concat([df_train_annotations, augmented_annotations_df], ignore_index=True)\n",
        "        \n",
        "        print(\"ColorJitter augmentation completed.\")\n",
        "        print(\"Here are some examples of how it looks:\")\n",
        "        \n",
        "        # Step 4: Output a Few Transformed Images\n",
        "        display_jittered_images(df_train_images, original_filenames=random_filenames)\n",
        "        \n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "def display_jittered_images(df_train_images, original_filenames=None, num_examples=5):\n",
        "    \"\"\"\n",
        "    Displays a comparison between original and augmented images.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image data and tensors.\n",
        "    - original_filenames (list): List of original filenames to display. If None, selects the first few non-augmented examples.\n",
        "    - num_examples (int): Number of examples to display.\n",
        "    \"\"\"\n",
        "    displayed_count = 0  # To track how many examples we've displayed\n",
        "\n",
        "    if original_filenames is None or len(original_filenames) == 0:\n",
        "        print(\"Here are some examples of how it looks:\")\n",
        "        # Get unique original filenames (without '_jit.jpg')\n",
        "        original_filenames = df_train_images[~df_train_images['filename'].str.contains('_jit.jpg')]['filename'].unique()\n",
        "\n",
        "        if len(original_filenames) == 0:\n",
        "            print(\"No image found.\")\n",
        "            return\n",
        "\n",
        "    for filename in original_filenames:\n",
        "        # Get original image tensor\n",
        "        original_image_row = df_train_images[df_train_images['filename'] == filename]\n",
        "        if original_image_row.empty:\n",
        "            continue\n",
        "        original_tensor = original_image_row.iloc[0]['tensor']\n",
        "\n",
        "        # Get augmented image tensor\n",
        "        augmented_filename = filename.replace('.jpg', '_jit.jpg')\n",
        "        augmented_image_row = df_train_images[df_train_images['filename'] == augmented_filename]\n",
        "        if augmented_image_row.empty:\n",
        "            continue  # Skip if there's no corresponding augmented image\n",
        "\n",
        "        augmented_tensor = augmented_image_row.iloc[0]['tensor']\n",
        "\n",
        "        # Create a figure with two subplots for side-by-side comparison\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "        # Use visualize_image directly inside each subplot\n",
        "        plt.sca(axs[0])\n",
        "        visualize_image(original_tensor, title=f\"Original: {filename}\")\n",
        "\n",
        "        plt.sca(axs[1])\n",
        "        visualize_image(augmented_tensor, title=f\"Augmented: {augmented_filename}\")\n",
        "\n",
        "        # Display the images\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        displayed_count += 1\n",
        "        if displayed_count >= num_examples:\n",
        "            break  # Stop once we've displayed the requested number of examples\n",
        "\n",
        "\n",
        "\n",
        "df_train_images, df_train_annotations = apply_color_jitter_augmentation(df_train_images, df_train_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "H6D9LyrZoz79",
        "outputId": "d68cb436-5153-46d0-9ab0-fc2cd91dea05"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added annotations\n",
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "7y2EJL_zoz79",
        "outputId": "91b76e4a-ad9a-42d2-97db-1e5aee9158c0"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added images\n",
        "df_train_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "2U0Nf1X3oz79",
        "outputId": "d30f05fe-4019-4fac-d55c-2a5d6629b49c"
      },
      "outputs": [],
      "source": [
        "# Df image Check for _jit.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P03803_jit.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "xfPqDu9Moz79",
        "outputId": "82caf31c-7958-430c-cbad-c2c545af6c46"
      },
      "outputs": [],
      "source": [
        "# Df check for OG image of _jit.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P03803.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "UNUNPPH2oz79",
        "outputId": "f325faf3-662a-40d8-fbdd-75f306456f19"
      },
      "outputs": [],
      "source": [
        "# Df annotation check of _jit.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P03803_jit.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "rDKIbpP6oz79",
        "outputId": "7e8509ff-f4fa-4474-8852-745e4ebebda9"
      },
      "outputs": [],
      "source": [
        "# Df Check OG annotation for _jit.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P03803.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "LvmJ_lj6oz79",
        "outputId": "63cd1bbd-5971-4d05-dcad-f123e9536acb"
      },
      "outputs": [],
      "source": [
        "# Visualising a ColourJittered Image of with Bounding Boxes\n",
        "visualize_image_with_bboxes(df_train_annotations, df_train_images, 'P03803_jit.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee11wo8hoz79"
      },
      "source": [
        "### Adding Gaussian Blur images to the dataset for generalizibility (10 Added Transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U585l-rgoz79",
        "outputId": "e162a75d-044c-43f1-893b-ebd1adb8527e"
      },
      "outputs": [],
      "source": [
        "def apply_gaussian_blur_augmentation(df_train_images, df_train_annotations):\n",
        "    \"\"\"\n",
        "    Applies Gaussian blur augmentation to 500 random images from the training dataset.\n",
        "    If the augmentation has already been performed, the function will output a message and exit.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image data and tensors.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing annotations for images.\n",
        "\n",
        "    Returns:\n",
        "    - df_train_images (pd.DataFrame): Updated DataFrame with augmented images added.\n",
        "    - df_train_annotations (pd.DataFrame): Updated DataFrame with annotations for augmented images added.\n",
        "    \"\"\"\n",
        "    np.random.seed(48) # Set Seed for reproducibility\n",
        "    \n",
        "    # Check if augmentation has already been performed by looking for filenames ending with '_blur.jpg'\n",
        "    augmented_filenames = df_train_images[df_train_images['filename'].str.contains('_blur.jpg')]\n",
        "\n",
        "    if not augmented_filenames.empty:\n",
        "        print(\"Gaussian blur augmentation has already been implemented.\")\n",
        "        # Optionally, display some of the already augmented images\n",
        "        display_gaussian_blur_augmented_images(df_train_images)\n",
        "        return df_train_images, df_train_annotations\n",
        "    else:\n",
        "        print(\"Starting Gaussian blur augmentation...\")\n",
        "\n",
        "        # Step 1: Select 500 Random Images from the Training Dataset\n",
        "        # Get unique filenames from the DataFrame\n",
        "        unique_filenames = df_train_images['filename'].unique()\n",
        "\n",
        "        # Randomly select 500 unique filenames\n",
        "        if len(unique_filenames) < 10:\n",
        "            num_images = len(unique_filenames)\n",
        "            print(f\"Only {num_images} unique images available. Selecting all of them.\")\n",
        "        else:\n",
        "            num_images = 10\n",
        "\n",
        "        random_filenames = np.random.choice(unique_filenames, size=num_images, replace=False)\n",
        "\n",
        "        # Filter the DataFrame to get the selected images\n",
        "        selected_images_df = df_train_images[df_train_images['filename'].isin(random_filenames)].copy()\n",
        "\n",
        "        # Initialize lists to store augmented images and annotations\n",
        "        augmented_images = []\n",
        "        augmented_annotations = []\n",
        "\n",
        "        # Process each selected image\n",
        "        for idx, row in selected_images_df.iterrows():\n",
        "            original_filename = row['filename']\n",
        "            tensor = row['tensor']\n",
        "\n",
        "            # Generate random kernel size and sigma for Gaussian blur\n",
        "            kernel_size = np.random.choice([3, 7]) \n",
        "            sigma = np.random.uniform(0.1, 4.0)     \n",
        "\n",
        "            # Define the GaussianBlur transformation\n",
        "            gaussian_blur = GaussianBlur(kernel_size=kernel_size, sigma=sigma)\n",
        "\n",
        "            # Apply Gaussian blur transformation\n",
        "            try:\n",
        "                transformed_tensor = gaussian_blur(tensor)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing image {original_filename}: {e}\")\n",
        "                continue  # Skip to the next image\n",
        "\n",
        "            # Create a new filename by appending '_blur.jpg'\n",
        "            new_filename = original_filename.replace('.jpg', '_blur.jpg')\n",
        "\n",
        "            # Check if the augmented image already exists to avoid duplication\n",
        "            if new_filename in df_train_images['filename'].values:\n",
        "                print(f\"Augmented image {new_filename} already exists. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Update the image data\n",
        "            new_row = row.copy()\n",
        "            new_row['filename'] = new_filename\n",
        "            new_row['tensor'] = transformed_tensor\n",
        "            augmented_images.append(new_row)\n",
        "\n",
        "            # Replicate the annotations with the new filename\n",
        "            annotations = df_train_annotations[df_train_annotations['filename'] == original_filename].copy()\n",
        "            annotations['filename'] = new_filename\n",
        "            augmented_annotations.append(annotations)\n",
        "\n",
        "        # Convert the list of augmented images and annotations to DataFrames\n",
        "        if augmented_images:\n",
        "            augmented_images_df = pd.DataFrame(augmented_images)\n",
        "            augmented_annotations_df = pd.concat(augmented_annotations, ignore_index=True)\n",
        "\n",
        "            # Add the augmented images and annotations to the original DataFrames\n",
        "            df_train_images = pd.concat([df_train_images, augmented_images_df], ignore_index=True)\n",
        "            df_train_annotations = pd.concat([df_train_annotations, augmented_annotations_df], ignore_index=True)\n",
        "\n",
        "            print(f\"Gaussian blur augmentation completed. {len(augmented_images)} images were augmented.\")\n",
        "            print(\"Here are some examples of how it looks:\")\n",
        "        else:\n",
        "            print(\"No new images were augmented.\")\n",
        "\n",
        "        # Display a few examples of the original and augmented images\n",
        "        display_gaussian_blur_augmented_images(df_train_images, original_filenames=random_filenames)\n",
        "\n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "def display_gaussian_blur_augmented_images(df_train_images, original_filenames=None, num_examples=5):\n",
        "    \"\"\"\n",
        "    Displays a comparison between original and Gaussian blurred images.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image data and tensors.\n",
        "    - original_filenames (list, optional): List of original filenames to display. If None, selects randomly.\n",
        "    - num_examples (int): Number of examples to display.\n",
        "    \"\"\"\n",
        "    \n",
        "    displayed_count = 0  # To track how many examples we've displayed\n",
        "\n",
        "    if original_filenames is None or len(original_filenames) == 0:\n",
        "        print(\"Here are some examples of how it looks:\")\n",
        "        # Get unique original filenames (without '_blur.jpg')\n",
        "        original_filenames = df_train_images[~df_train_images['filename'].str.contains('_blur.jpg')]['filename'].unique()\n",
        "\n",
        "        if len(original_filenames) == 0:\n",
        "            print(\"No image found.\")\n",
        "            return\n",
        "\n",
        "    for filename in original_filenames:\n",
        "        # Get original image tensor\n",
        "        original_image_row = df_train_images[df_train_images['filename'] == filename]\n",
        "        if original_image_row.empty:\n",
        "            continue\n",
        "        original_tensor = original_image_row.iloc[0]['tensor']\n",
        "\n",
        "        # Get augmented image tensor\n",
        "        augmented_filename = filename.replace('.jpg', '_blur.jpg')\n",
        "        augmented_image_row = df_train_images[df_train_images['filename'] == augmented_filename]\n",
        "        if augmented_image_row.empty:\n",
        "            continue  # Skip if there's no corresponding augmented image\n",
        "\n",
        "        augmented_tensor = augmented_image_row.iloc[0]['tensor']\n",
        "\n",
        "        # Create a figure with two subplots for side-by-side comparison\n",
        "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "        # Use visualize_image directly inside each subplot\n",
        "        plt.sca(axs[0])\n",
        "        visualize_image(original_tensor, title=f\"Original: {filename}\")\n",
        "\n",
        "        plt.sca(axs[1])\n",
        "        visualize_image(augmented_tensor, title=f\"Augmented: {augmented_filename}\")\n",
        "\n",
        "        # Display the images\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        displayed_count += 1\n",
        "        if displayed_count >= num_examples:\n",
        "            break  # Stop once we've displayed the requested number of examples\n",
        "\n",
        "\n",
        "df_train_images, df_train_annotations = apply_gaussian_blur_augmentation(df_train_images, df_train_annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "_RaCQ3KFoz79",
        "outputId": "69af0e93-9de8-4013-ca58-f96ee6061895"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added annotations\n",
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "bV59B6_poz79",
        "outputId": "513c8071-2206-4824-ec2f-bc3c91accf4a"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added images\n",
        "df_train_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "W3xynydboz79",
        "outputId": "7db2e289-520d-4a30-c54d-d2170886923f"
      },
      "outputs": [],
      "source": [
        "# Df image Check for _blur.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P04093_blur.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "_5wVqx6Ooz79",
        "outputId": "5b6ed632-99aa-419e-f7f9-2a6c49487324"
      },
      "outputs": [],
      "source": [
        "# Df image Check for OG image of _blur.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P04093.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "utZHHOXqoz79",
        "outputId": "b108c270-ca05-4d6f-a1c0-2fc03d6dfdc6"
      },
      "outputs": [],
      "source": [
        "# Df annotation check of _blur.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P04093_blur.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "wZxQL6EWoz79",
        "outputId": "e918c55c-5555-4214-e2ee-da0ee0efc3c9"
      },
      "outputs": [],
      "source": [
        "# Df check for OG annotaions of _blur.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P04093.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1j5y3n3oz79",
        "outputId": "cf6a21f3-6065-49e3-f692-70026cfa6b1d"
      },
      "outputs": [],
      "source": [
        "# Visualising a Gaussian Blurred Image with Bounding Boxes\n",
        "visualize_image_with_bboxes(df_train_annotations, df_train_images, 'P04093_blur.jpg')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8k-LZhioz79"
      },
      "source": [
        "### Adding Flipped images to the dataset for generalizibility (30 Added Transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "M4psenCxoz79",
        "outputId": "8f5e8d94-5c54-4fec-837d-4136cc29bec2"
      },
      "outputs": [],
      "source": [
        "def image_flips(df_train_images, df_train_annotations):\n",
        "    \"\"\"\n",
        "    Performs flip augmentation (horizontal, vertical, both) on randomly selected images,\n",
        "    adjusts bounding boxes, and visualizes the results.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box coordinates and filenames.\n",
        "\n",
        "    Returns:\n",
        "    - df_train_images (pd.DataFrame): Updated DataFrame with augmented images added.\n",
        "    - df_train_annotations (pd.DataFrame): Updated DataFrame with augmented annotations added.\n",
        "    \"\"\"\n",
        "    # Set dimensions for all images\n",
        "    img_width = 688\n",
        "    img_height = 584\n",
        "    np.random.seed(49)\n",
        "\n",
        "    # Check if augmentation has already been performed by looking for filenames containing '_flip.jpg'\n",
        "    augmented_filenames = df_train_images[df_train_images['filename'].str.contains('_hflip.jpg|_vflip.jpg|_hvflip.jpg')]\n",
        "\n",
        "    # Get original filenames (excluding augmented ones)\n",
        "    original_filenames = df_train_images[~df_train_images['filename'].str.contains('_hflip.jpg|_vflip.jpg|_hvflip.jpg')]['filename'].unique()\n",
        "\n",
        "    # List to store original filenames that have all corresponding flips\n",
        "    original_filenames_with_flips = []\n",
        "\n",
        "    # Iterate through each original filename and check if all flips exist\n",
        "    for original_filename in original_filenames:\n",
        "        base_filename = original_filename.replace('.jpg', '')\n",
        "        \n",
        "        # Check if all corresponding flips exist in the augmented_filenames\n",
        "        hflip = f\"{base_filename}_hflip.jpg\"\n",
        "        vflip = f\"{base_filename}_vflip.jpg\"\n",
        "        hvflip = f\"{base_filename}_hvflip.jpg\"\n",
        "        \n",
        "        if hflip in df_train_images['filename'].values and vflip in df_train_images['filename'].values and hvflip in df_train_images['filename'].values:\n",
        "            original_filenames_with_flips.append(original_filename)\n",
        "\n",
        "    if original_filenames_with_flips:\n",
        "        print(\"Flip augmentation has already been implemented.\")\n",
        "        print(\"Here are some samples of the augmentation:\")\n",
        "        # Select 5 random original images that have flips to display the flips\n",
        "        if len(original_filenames_with_flips) >= 5:\n",
        "            selected_images = np.random.choice(original_filenames_with_flips, size=5, replace=False)\n",
        "        else:\n",
        "            selected_images = original_filenames_with_flips\n",
        "        \n",
        "        for original_filename in selected_images:\n",
        "            # Pass the original filename to the display function\n",
        "            display_flip_image(df_train_images, df_train_annotations, original_filename)\n",
        "        \n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "    print(\"Starting Flipping Augmentation...\")\n",
        "\n",
        "    # Randomly select images from the DataFrame\n",
        "    num_images_to_flip = 10\n",
        "    if len(original_filenames) >= num_images_to_flip:\n",
        "        selected_images = np.random.choice(original_filenames, size=num_images_to_flip, replace=False)\n",
        "    else:\n",
        "        selected_images = original_filenames\n",
        "        print(f\"Only {len(selected_images)} images are available for augmentation.\")\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_annotations = []\n",
        "\n",
        "    for filename in selected_images:\n",
        "        # Fetch image row from df_train_images\n",
        "        image_row = df_train_images[df_train_images['filename'] == filename]\n",
        "        if image_row.empty:\n",
        "            continue\n",
        "        img_tensor = image_row['tensor'].values[0]\n",
        "        \n",
        "        # Fetch associated bounding boxes from df_train_annotations\n",
        "        annotation_rows = df_train_annotations[df_train_annotations['filename'] == filename]\n",
        "\n",
        "        # Perform flips and adjust bounding boxes\n",
        "        for flip_type in ['hflip', 'vflip', 'hvflip']:\n",
        "            if flip_type == 'hflip':\n",
        "                flipped_tensor = TF.hflip(img_tensor)\n",
        "            elif flip_type == 'vflip':\n",
        "                flipped_tensor = TF.vflip(img_tensor)\n",
        "            elif flip_type == 'hvflip':\n",
        "                flipped_tensor = TF.hflip(TF.vflip(img_tensor))\n",
        "            new_filename = filename.replace('.jpg', f'_{flip_type}.jpg')\n",
        "            annotations = adjust_flips_bounding_boxes(annotation_rows, img_width, img_height, flip_type, new_filename)\n",
        "            \n",
        "            # Create new image row with all metadata copied over\n",
        "            new_image_row = image_row.copy()\n",
        "            new_image_row['filename'] = new_filename\n",
        "            new_image_row['tensor'] = [flipped_tensor]  # Ensure it's a list to maintain DataFrame structure\n",
        "            # Append the flipped image to augmented_images\n",
        "            augmented_images.append(new_image_row)\n",
        "\n",
        "            # Append adjusted annotations to augmented_annotations\n",
        "            augmented_annotations.append(pd.DataFrame(annotations))\n",
        "    \n",
        "    # Concatenate augmented images and annotations\n",
        "    if augmented_images:\n",
        "        df_augmented_images = pd.concat(augmented_images, ignore_index=True)\n",
        "        df_augmented_annotations = pd.concat(augmented_annotations, ignore_index=True)\n",
        "\n",
        "        # Append augmented data to original dataframes\n",
        "        df_train_images = pd.concat([df_train_images, df_augmented_images], ignore_index=True)\n",
        "        df_train_annotations = pd.concat([df_train_annotations, df_augmented_annotations], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No images were augmented.\")\n",
        "\n",
        "    # For visualization: select 5 random images from the original selected images to display the flips\n",
        "    if len(selected_images) >= 5:\n",
        "        selected_for_visualization = np.random.choice(selected_images, size=5, replace=False)\n",
        "    else:\n",
        "        selected_for_visualization = selected_images\n",
        "    \n",
        "    # Display side-by-side comparisons for the selected images\n",
        "    for original_filename in selected_for_visualization:\n",
        "        display_flip_image(df_train_images, df_train_annotations, original_filename)\n",
        "\n",
        "    return df_train_images, df_train_annotations\n",
        "\n",
        "def adjust_flips_bounding_boxes(annotation_rows, img_width, img_height, flip_type, new_filename):\n",
        "    \"\"\"\n",
        "    Adjust bounding boxes according to the flip type.\n",
        "\n",
        "    Parameters:\n",
        "    - annotation_rows (pd.DataFrame): DataFrame containing annotations for a single image.\n",
        "    - img_width (int): Width of the image.\n",
        "    - img_height (int): Height of the image.\n",
        "    - flip_type (str): Type of flip ('hflip', 'vflip', 'hvflip').\n",
        "    - new_filename (str): Filename for the augmented image.\n",
        "\n",
        "    Returns:\n",
        "    - adjusted_annotations (list): List of dictionaries with adjusted annotations.\n",
        "    \"\"\"\n",
        "    adjusted_annotations = []\n",
        "    \n",
        "    for _, row in annotation_rows.iterrows():\n",
        "        xmin, xmax, ymin, ymax = row['xmin'], row['xmax'], row['ymin'], row['ymax']\n",
        "        if flip_type == 'hflip':\n",
        "            # Horizontal flip: adjust x-coordinates\n",
        "            new_xmin = img_width - xmax\n",
        "            new_xmax = img_width - xmin\n",
        "            new_ymin, new_ymax = ymin, ymax\n",
        "        elif flip_type == 'vflip':\n",
        "            # Vertical flip: adjust y-coordinates\n",
        "            new_xmin, new_xmax = xmin, xmax\n",
        "            new_ymin = img_height - ymax\n",
        "            new_ymax = img_height - ymin\n",
        "        elif flip_type == 'hvflip':\n",
        "            # Both horizontal and vertical flip\n",
        "            new_xmin = img_width - xmax\n",
        "            new_xmax = img_width - xmin\n",
        "            new_ymin = img_height - ymax\n",
        "            new_ymax = img_height - ymin\n",
        "\n",
        "        # Ensure coordinates are in the correct order\n",
        "        new_xmin, new_xmax = sorted([new_xmin, new_xmax])\n",
        "        new_ymin, new_ymax = sorted([new_ymin, new_ymax])\n",
        "        \n",
        "        # Copy over all metadata from the original annotation\n",
        "        new_annotation = row.copy()\n",
        "        new_annotation['filename'] = new_filename\n",
        "        new_annotation['xmin'] = new_xmin\n",
        "        new_annotation['xmax'] = new_xmax\n",
        "        new_annotation['ymin'] = new_ymin\n",
        "        new_annotation['ymax'] = new_ymax\n",
        "\n",
        "        adjusted_annotations.append(new_annotation)\n",
        "\n",
        "    return adjusted_annotations\n",
        "\n",
        "def display_flip_image(df_train_images, df_train_annotations, original_filename):\n",
        "    \"\"\"\n",
        "    Display the original and flipped images with their bounding boxes side by side.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box coordinates and filenames.\n",
        "    - original_filename (str): Filename of the original image.\n",
        "    \"\"\"\n",
        "    flip_types = ['original', 'hflip', 'vflip', 'hvflip']\n",
        "    filenames = [original_filename] + [original_filename.replace('.jpg', f'_{flip}.jpg') for flip in ['hflip', 'vflip', 'hvflip']]\n",
        "    \n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "    \n",
        "    for i, filename in enumerate(filenames):\n",
        "        visualize_image_with_bboxes(df_train_annotations, df_train_images, filename, ax=axes[i])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "df_train_images, df_train_annotations = image_flips(df_train_images, df_train_annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Ol1Bu64Ioz7-",
        "outputId": "149b83ad-e313-4cb4-9cf3-99e42a132d9d"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added images\n",
        "df_train_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(df_train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "iXo1vWeYoz7-",
        "outputId": "b0ca1622-8948-416d-a7df-4a277c11a012"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added annotations\n",
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xLdUHcABoz7-",
        "outputId": "2a81ed5e-cb83-41e8-e4cb-0aa08be4848b"
      },
      "outputs": [],
      "source": [
        "# Df image Check for _hflip.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P06202_hflip.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "9flsFxiboz7-",
        "outputId": "34a16852-d5de-434b-c261-ad55deedec18"
      },
      "outputs": [],
      "source": [
        "# Df image Check for _vflip.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P06202_vflip.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "R3kR3nW3oz7-",
        "outputId": "2cb6aa06-dbff-453e-da68-72a7ade309c8"
      },
      "outputs": [],
      "source": [
        "# Df image Check for _hvflip.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P06202_hvflip.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "7o0TsWmeoz7-",
        "outputId": "d13e83dc-51bf-446a-8edf-54172f5b3638"
      },
      "outputs": [],
      "source": [
        "# Df image Check for OG image of _blur.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P06202.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "xQ6_qE27oz7-",
        "outputId": "6293c141-5f75-4a25-88af-f56b877a6586"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for _hflip.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P06202.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "T95ZUOp_oz7-",
        "outputId": "bdf11008-b2d8-46ab-95b2-7cd6b87859f9"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for _vflip.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P06202_vflip.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UYLxvOj7oz7-",
        "outputId": "fbf9dfa6-8d33-475c-910d-c926fbfe8062"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for _hvflip.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P06202_hflip.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "AQIV-dJuoz7-",
        "outputId": "0435b7c0-a2c1-491d-9740-6d22017e50e6"
      },
      "outputs": [],
      "source": [
        "# Df check for OG annotaions of _blur.jpg\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P06202_hvflip.jpg']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGDaFgdSoz7-"
      },
      "source": [
        "### Adding Gaussian Noise images to the dataset for generalizibility (20 Added Transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-R3vCZHfoz7-",
        "outputId": "1a83f729-71d0-4e21-be5f-158272eb22ea"
      },
      "outputs": [],
      "source": [
        "def add_gaussian_noise_to_images(df_train_images, df_train_annotations, mean_range=(0.5, 1.5), sigma_range=(20.0, 70.0), num_images=20):\n",
        "    \"\"\"\n",
        "    Adds Gaussian noise to random images in the dataset and appends them back to the dataset\n",
        "    with \"_noise\" appended to their filenames. The mean and sigma for the noise are randomly selected\n",
        "    within the specified ranges for each image.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box annotations.\n",
        "    - mean_range (tuple): Range (min, max) of the mean of the Gaussian noise.\n",
        "    - sigma_range (tuple): Range (min, max) of the standard deviation (sigma) of the Gaussian noise.\n",
        "    - num_images (int): Number of images to augment with Gaussian noise.\n",
        "\n",
        "    Returns:\n",
        "    - df_train_images (pd.DataFrame): Updated DataFrame with augmented images added.\n",
        "    - df_train_annotations (pd.DataFrame): Updated DataFrame with annotations for augmented images added.\n",
        "    \"\"\"\n",
        "    \n",
        "    random.seed(50)\n",
        "    torch.manual_seed(50)\n",
        "    \n",
        "    # Check if Gaussian noise has already been added\n",
        "    if df_train_images['filename'].str.contains('_noise').any():\n",
        "        print(\"Gaussian Noise Already Added\")\n",
        "        print(\"Here are some samples\")\n",
        "        # Display some samples\n",
        "        noised_filenames = df_train_images[df_train_images['filename'].str.contains('_noise')]['filename'].unique()\n",
        "        sample_filenames = random.sample(list(noised_filenames), min(5, len(noised_filenames)))\n",
        "        for filename in sample_filenames:\n",
        "            # Get original filename\n",
        "            original_filename = filename.replace('_noise', '')\n",
        "            display_noise_image(df_train_images, df_train_annotations, original_filename, filename)\n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "    # Exclude images that have already been augmented\n",
        "    original_filenames = df_train_images[~df_train_images['filename'].str.contains('_noise')]['filename'].unique()\n",
        "\n",
        "    # Randomly select images to augment\n",
        "    if len(original_filenames) >= num_images:\n",
        "        selected_filenames = random.sample(list(original_filenames), num_images)\n",
        "    else:\n",
        "        selected_filenames = original_filenames\n",
        "        print(f\"Only {len(selected_filenames)} images are available for augmentation.\")\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_annotations = []\n",
        "\n",
        "    for filename in selected_filenames:\n",
        "        # Fetch image tensor from df_train_images\n",
        "        image_row = df_train_images[df_train_images['filename'] == filename]\n",
        "        img_tensor = image_row.iloc[0]['tensor']\n",
        "        \n",
        "        # Randomly select mean and sigma from the specified ranges\n",
        "        mean = random.uniform(*mean_range)\n",
        "        sigma = random.uniform(*sigma_range)\n",
        "        \n",
        "        # Add Gaussian noise\n",
        "        noise = torch.randn(img_tensor.size(), device=img_tensor.device) * sigma + mean\n",
        "        noised_tensor = img_tensor.float() + noise\n",
        "        noised_tensor = torch.clamp(noised_tensor, 0, 255).type(img_tensor.dtype)\n",
        "        \n",
        "        # Create new filename\n",
        "        new_filename = filename.replace('.jpg', '_noise.jpg')\n",
        "        \n",
        "        # Create new image row with all metadata copied over\n",
        "        new_image_row = image_row.copy()\n",
        "        new_image_row['filename'] = new_filename\n",
        "        new_image_row['tensor'] = [noised_tensor]  # Ensure it's a list to maintain DataFrame structure\n",
        "        augmented_images.append(new_image_row)\n",
        "        \n",
        "        # Copy annotations without changes, just update the filename\n",
        "        annotations = df_train_annotations[df_train_annotations['filename'] == filename].copy()\n",
        "        annotations['filename'] = new_filename\n",
        "        augmented_annotations.append(annotations)\n",
        "        \n",
        "    # Concatenate augmented images and annotations\n",
        "    if augmented_images:\n",
        "        df_augmented_images = pd.concat(augmented_images, ignore_index=True)\n",
        "        df_augmented_annotations = pd.concat(augmented_annotations, ignore_index=True)\n",
        "        \n",
        "        # Append augmented data to original dataframes\n",
        "        df_train_images = pd.concat([df_train_images, df_augmented_images], ignore_index=True)\n",
        "        df_train_annotations = pd.concat([df_train_annotations, df_augmented_annotations], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No images were augmented.\")\n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "    # Display side-by-side comparisons for 5 samples\n",
        "    sample_filenames = random.sample(list(selected_filenames), min(5, len(selected_filenames)))\n",
        "    for original_filename in sample_filenames:\n",
        "        new_filename = original_filename.replace('.jpg', '_noise.jpg')\n",
        "        display_noise_image(df_train_images, df_train_annotations, original_filename, new_filename)\n",
        "\n",
        "    return df_train_images, df_train_annotations\n",
        "\n",
        "def display_noise_image(df_train_images, df_train_annotations, original_filename, augmented_filename):\n",
        "    \"\"\"\n",
        "    Displays the original and augmented images with bounding boxes side by side.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box annotations.\n",
        "    - original_filename (str): Filename of the original image.\n",
        "    - augmented_filename (str): Filename of the augmented image.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "    \n",
        "    for ax, filename in zip(axes, [original_filename, augmented_filename]):\n",
        "        visualize_image_with_bboxes(df_train_annotations, df_train_images, filename, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "df_train_images, df_train_annotations = add_gaussian_noise_to_images(df_train_images, df_train_annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "f8ysSMGioz7-",
        "outputId": "905e739b-0962-4937-8419-0e63902dc69c"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added images\n",
        "df_train_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "f6ChXMUxoz7-",
        "outputId": "ce4bcc10-66d0-41ae-84f5-90ab3d1b3b6b"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added annotations\n",
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "TjECnMLnoz7-",
        "outputId": "28a274fe-2976-43a1-9087-db01f822fdc1"
      },
      "outputs": [],
      "source": [
        "# Df image Check OG image of noise\n",
        "df_train_images[df_train_images['filename'] == 'P03423.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "V-wJukKToz7-",
        "outputId": "6c2ac718-6d0a-4e63-8461-11f9a360fd52"
      },
      "outputs": [],
      "source": [
        "# Df image Check for noise.jpg\n",
        "df_train_images[df_train_images['filename'] == 'P03423_noise.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "pdfw0zceoz7-",
        "outputId": "39d75ea1-cd19-4ec1-92a1-a959a7a43cda"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for OG image\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P03423.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "Zzr6Np8eoz7-",
        "outputId": "db0038c7-2c73-4ad6-fe7d-ce3d2034c71a"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for noise.jpg image\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P03423_noise.jpg']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9TXVUDoz7-"
      },
      "source": [
        "### Adding Gaussian Noise to the Bounding Boxes only to the dataset for generalizibility (10 Added Transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S0ayVaYyoz7-",
        "outputId": "a543ddba-02d3-4c64-8a86-1eda908c7b79"
      },
      "outputs": [],
      "source": [
        "def add_gaussian_noise_to_bboxes(df_train_images, df_train_annotations, mean_range=(0.0, 1.0), sigma_range=(5.0, 15.0), num_images=10):\n",
        "    \"\"\"\n",
        "    Adds Gaussian noise within the bounding boxes of random images in the dataset\n",
        "    and appends them back to the dataset with \"_noisebb\" appended to their filenames.\n",
        "    The mean and sigma for the noise are randomly selected within the specified ranges.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box annotations.\n",
        "    - mean_range (tuple): Range (min, max) for the mean of the Gaussian noise.\n",
        "    - sigma_range (tuple): Range (min, max) for the standard deviation (sigma) of the Gaussian noise.\n",
        "    - num_images (int): Number of images to augment.\n",
        "\n",
        "    Returns:\n",
        "    - df_train_images (pd.DataFrame): Updated DataFrame with augmented images added.\n",
        "    - df_train_annotations (pd.DataFrame): Updated DataFrame with augmented annotations added.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Set seed for reproducibility\n",
        "    random.seed(51)\n",
        "    torch.manual_seed(51)\n",
        "    \n",
        "    # Check if Gaussian noise has already been added to bounding boxes\n",
        "    if df_train_images['filename'].str.contains('_noisebb').any():\n",
        "        print(\"Gaussian Noise Already Added to BB\")\n",
        "        print(\"Here are some samples:\")\n",
        "        # Display some samples\n",
        "        noised_filenames = df_train_images[df_train_images['filename'].str.contains('_noisebb')]['filename'].unique()\n",
        "        sample_filenames = random.sample(list(noised_filenames), min(5, len(noised_filenames)))\n",
        "        for filename in sample_filenames:\n",
        "            # Get original filename\n",
        "            original_filename = filename.replace('_noisebb', '')\n",
        "            display_bb_noise_image(df_train_images, df_train_annotations, original_filename, filename)\n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "    # Exclude images that have already been augmented\n",
        "    original_filenames = df_train_images[~df_train_images['filename'].str.contains('_noisebb')]['filename'].unique()\n",
        "\n",
        "    # Randomly select images to augment\n",
        "    if len(original_filenames) >= num_images:\n",
        "        selected_filenames = random.sample(list(original_filenames), num_images)\n",
        "    else:\n",
        "        selected_filenames = original_filenames\n",
        "        print(f\"Only {len(selected_filenames)} images are available for augmentation.\")\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_annotations = []\n",
        "\n",
        "    for filename in selected_filenames:\n",
        "        # Fetch image tensor from df_train_images\n",
        "        image_row = df_train_images[df_train_images['filename'] == filename]\n",
        "        img_tensor = image_row.iloc[0]['tensor'].clone()  # Clone to avoid modifying the original tensor\n",
        "\n",
        "        # Fetch associated annotations\n",
        "        annotations = df_train_annotations[df_train_annotations['filename'] == filename]\n",
        "\n",
        "        # Randomly select mean and sigma from the specified ranges\n",
        "        mean = random.uniform(*mean_range)\n",
        "        sigma = random.uniform(*sigma_range)\n",
        "\n",
        "        # Add Gaussian noise within each bounding box\n",
        "        for _, ann_row in annotations.iterrows():\n",
        "            xmin = int(ann_row['xmin'])\n",
        "            xmax = int(ann_row['xmax'])\n",
        "            ymin = int(ann_row['ymin'])\n",
        "            ymax = int(ann_row['ymax'])\n",
        "\n",
        "            # Ensure coordinates are within image dimensions\n",
        "            xmin = max(0, xmin)\n",
        "            xmax = min(img_tensor.shape[2] - 1, xmax)\n",
        "            ymin = max(0, ymin)\n",
        "            ymax = min(img_tensor.shape[1] - 1, ymax)\n",
        "\n",
        "            # Generate noise for the bounding box area\n",
        "            noise = torch.randn(img_tensor[:, ymin:ymax+1, xmin:xmax+1].size()) * sigma + mean\n",
        "            noise = noise.to(img_tensor.dtype)\n",
        "\n",
        "            # Add noise to the bounding box area\n",
        "            img_tensor[:, ymin:ymax+1, xmin:xmax+1] += noise\n",
        "\n",
        "            # Clamp the values to valid pixel range\n",
        "            img_tensor = torch.clamp(img_tensor, 0, 255)\n",
        "\n",
        "        # Create new filename\n",
        "        new_filename = filename.replace('.jpg', '_noisebb.jpg')\n",
        "\n",
        "        # Create new image row with all metadata copied over\n",
        "        new_image_row = image_row.copy()\n",
        "        new_image_row['filename'] = new_filename\n",
        "        new_image_row['tensor'] = [img_tensor]\n",
        "        augmented_images.append(new_image_row)\n",
        "\n",
        "        # Copy annotations without changes, just update the filename\n",
        "        new_annotations = annotations.copy()\n",
        "        new_annotations['filename'] = new_filename\n",
        "        augmented_annotations.append(new_annotations)\n",
        "\n",
        "    # Concatenate augmented images and annotations\n",
        "    if augmented_images:\n",
        "        df_augmented_images = pd.concat(augmented_images, ignore_index=True)\n",
        "        df_augmented_annotations = pd.concat(augmented_annotations, ignore_index=True)\n",
        "\n",
        "        # Append augmented data to original dataframes\n",
        "        df_train_images = pd.concat([df_train_images, df_augmented_images], ignore_index=True)\n",
        "        df_train_annotations = pd.concat([df_train_annotations, df_augmented_annotations], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No images were augmented.\")\n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "    # Display side-by-side comparisons for 5 samples\n",
        "    sample_filenames = random.sample(list(selected_filenames), min(5, len(selected_filenames)))\n",
        "    for original_filename in sample_filenames:\n",
        "        new_filename = original_filename.replace('.jpg', '_noisebb.jpg')\n",
        "        display_bb_noise_image(df_train_images, df_train_annotations, original_filename, new_filename)\n",
        "\n",
        "    return df_train_images, df_train_annotations\n",
        "\n",
        "def display_bb_noise_image(df_train_images, df_train_annotations, original_filename, augmented_filename):\n",
        "    \"\"\"\n",
        "    Displays the original and augmented images with bounding boxes side by side.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box annotations.\n",
        "    - original_filename (str): Filename of the original image.\n",
        "    - augmented_filename (str): Filename of the augmented image.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    for ax, filename in zip(axes, [original_filename, augmented_filename]):\n",
        "        visualize_image_with_bboxes(df_train_annotations, df_train_images, filename, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "df_train_images, df_train_annotations = add_gaussian_noise_to_bboxes(df_train_images, df_train_annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "nop_Sojhoz7_",
        "outputId": "4211fc13-b7f2-459f-ae80-cf6deed650ba"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added images\n",
        "df_train_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "LBLuHJYsoz7_",
        "outputId": "3483df94-911f-4626-f1d0-e611ba6329ee"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added annotations\n",
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "H3uRxggtoz7_",
        "outputId": "699444a8-6e0b-4609-f5cd-c0a4986f83a9"
      },
      "outputs": [],
      "source": [
        "# Df image Check OG image of noise\n",
        "df_train_images[df_train_images['filename'] == 'P02689.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xLVjTKsVoz7_",
        "outputId": "50b9902a-2ab0-4077-fddb-b2e2cef5351b"
      },
      "outputs": [],
      "source": [
        "# Df image Check noisebb image of noise\n",
        "df_train_images[df_train_images['filename'] == 'P02689_noisebb.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "gzkTjuGnoz7_",
        "outputId": "2237b5ea-5477-4914-b3b1-c4c47f4b3da4"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for OG image\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P02689.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "nLLkK4zcoz7_",
        "outputId": "8f4f0e92-ff7c-4806-e56f-f981e5195826"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for noisebb image\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P02689_noisebb.jpg']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb0Ecx4Joz7_"
      },
      "source": [
        "### Adding Random Erasing Augmentation to the dataset for generalizibility (20 Added Transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4JEq02evoz7_",
        "outputId": "a4a56998-fde8-4c6f-baea-15929b2240c7"
      },
      "outputs": [],
      "source": [
        "def add_random_erasing(df_train_images, df_train_annotations, num_images=20, max_erasing_area_per_bbox=0.7):\n",
        "    \"\"\"\n",
        "    Applies Random Erasing to random images in the dataset while ensuring no more than a specified\n",
        "    percentage of any object (bounding box) is erased. The augmented images are added back to the dataset\n",
        "    with \"_erase\" appended to their filenames.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box annotations.\n",
        "    - num_images (int): Number of images to augment.\n",
        "    - max_erasing_area_per_bbox (float): Maximum proportion of any bounding box that can be erased.\n",
        "\n",
        "    Returns:\n",
        "    - df_train_images (pd.DataFrame): Updated DataFrame with augmented images added.\n",
        "    - df_train_annotations (pd.DataFrame): Updated DataFrame with annotations for augmented images added.\n",
        "    \"\"\"\n",
        "    random.seed(52)\n",
        "    torch.manual_seed(52)\n",
        "    \n",
        "    # Check if Random Erasing has already been applied\n",
        "    if df_train_images['filename'].str.contains('_erase').any():\n",
        "        print(\"Random Erasing already added to the dataset.\")\n",
        "        # Display some samples\n",
        "        erased_filenames = df_train_images[df_train_images['filename'].str.contains('_erase')]['filename'].unique()\n",
        "        sample_filenames = random.sample(list(erased_filenames), min(5, len(erased_filenames)))\n",
        "        for filename in sample_filenames:\n",
        "            # Get original filename\n",
        "            original_filename = filename.replace('_erase', '')\n",
        "            display_erased_image(df_train_images, df_train_annotations, original_filename, filename)\n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "    # Exclude images that have already been augmented\n",
        "    original_filenames = df_train_images[~df_train_images['filename'].str.contains('_erase')]['filename'].unique()\n",
        "\n",
        "    # Randomly select images to augment\n",
        "    if len(original_filenames) >= num_images:\n",
        "        selected_filenames = random.sample(list(original_filenames), num_images)\n",
        "    else:\n",
        "        selected_filenames = original_filenames\n",
        "        print(f\"Only {len(selected_filenames)} images are available for augmentation.\")\n",
        "\n",
        "    augmented_images = []\n",
        "    augmented_annotations = []\n",
        "\n",
        "    for filename in selected_filenames:\n",
        "        # Fetch image tensor from df_train_images\n",
        "        image_row = df_train_images[df_train_images['filename'] == filename]\n",
        "        img_tensor = image_row.iloc[0]['tensor'].clone()  # Clone to avoid modifying the original tensor\n",
        "\n",
        "        # Fetch associated annotations\n",
        "        annotations = df_train_annotations[df_train_annotations['filename'] == filename]\n",
        "\n",
        "        # Apply Random Erasing with conditional constraints\n",
        "        img_tensor_erased = apply_random_erasing(\n",
        "            img_tensor, annotations, max_erasing_area_per_bbox=max_erasing_area_per_bbox)\n",
        "\n",
        "        # Create new filename\n",
        "        new_filename = filename.replace('.jpg', '_erase.jpg')\n",
        "\n",
        "        # Create new image row with all metadata copied over\n",
        "        new_image_row = image_row.copy()\n",
        "        new_image_row['filename'] = new_filename\n",
        "        new_image_row['tensor'] = [img_tensor_erased]\n",
        "        augmented_images.append(new_image_row)\n",
        "\n",
        "        # Copy annotations without changes, just update the filename\n",
        "        annotations = annotations.copy()\n",
        "        annotations['filename'] = new_filename\n",
        "        augmented_annotations.append(annotations)\n",
        "\n",
        "    # Concatenate augmented images and annotations\n",
        "    if augmented_images:\n",
        "        df_augmented_images = pd.concat(augmented_images, ignore_index=True)\n",
        "        df_augmented_annotations = pd.concat(augmented_annotations, ignore_index=True)\n",
        "\n",
        "        # Append augmented data to original dataframes\n",
        "        df_train_images = pd.concat([df_train_images, df_augmented_images], ignore_index=True)\n",
        "        df_train_annotations = pd.concat([df_train_annotations, df_augmented_annotations], ignore_index=True)\n",
        "    else:\n",
        "        print(\"No images were augmented.\")\n",
        "        return df_train_images, df_train_annotations\n",
        "\n",
        "    # Display side-by-side comparisons for 5 samples\n",
        "    sample_filenames = random.sample(list(selected_filenames), min(5, len(selected_filenames)))\n",
        "    for original_filename in sample_filenames:\n",
        "        new_filename = original_filename.replace('.jpg', '_erase.jpg')\n",
        "        display_erased_image(df_train_images, df_train_annotations, original_filename, new_filename)\n",
        "\n",
        "    return df_train_images, df_train_annotations\n",
        "\n",
        "def apply_random_erasing(img_tensor, annotations, sl=0.02, sh=0.3, r1=0.3,\n",
        "                         max_erasing_area_per_bbox=0.8, attempts=100):\n",
        "    \"\"\"\n",
        "    Applies Random Erasing to an image tensor while ensuring no more than a specified percentage\n",
        "    of any object (bounding box) is erased.\n",
        "\n",
        "    Parameters:\n",
        "    - img_tensor (torch.Tensor): Image tensor of shape (C, H, W).\n",
        "    - annotations (pd.DataFrame): DataFrame containing annotations for the image.\n",
        "    - sl (float): Minimum proportion of erased area against input image.\n",
        "    - sh (float): Maximum proportion of erased area against input image.\n",
        "    - r1 (float): Minimum aspect ratio of erased area.\n",
        "    - max_erasing_area_per_bbox (float): Maximum proportion of any bounding box that can be erased.\n",
        "    - attempts (int): Number of times to attempt finding a valid erasing rectangle.\n",
        "\n",
        "    Returns:\n",
        "    - img_tensor (torch.Tensor): Image tensor with random erasing applied.\n",
        "    \"\"\"\n",
        "    # Convert the tensor to float32 if it's in Byte format\n",
        "    if img_tensor.dtype == torch.uint8:\n",
        "        img_tensor = img_tensor.float()\n",
        "\n",
        "    # Get image dimensions\n",
        "    C, H, W = img_tensor.shape\n",
        "\n",
        "    # Extract bounding boxes from annotations\n",
        "    bboxes = annotations[['xmin', 'ymin', 'xmax', 'ymax']].values  # Shape (num_bboxes, 4)\n",
        "    bboxes[:, [0, 2]] = np.clip(bboxes[:, [0, 2]], 0, W)\n",
        "    bboxes[:, [1, 3]] = np.clip(bboxes[:, [1, 3]], 0, H)\n",
        "\n",
        "    # Compute areas of bounding boxes\n",
        "    bbox_areas = (bboxes[:, 2] - bboxes[:, 0]) * (bboxes[:, 3] - bboxes[:, 1])  # Shape (num_bboxes,)\n",
        "\n",
        "    for attempt in range(attempts):\n",
        "        # Area of the image\n",
        "        area = H * W\n",
        "\n",
        "        # Target area for erasing between sl and sh of the image area\n",
        "        target_area = random.uniform(sl, sh) * area\n",
        "        # Aspect ratio between r1 and 1/r1\n",
        "        aspect_ratio = random.uniform(r1, 1/r1)\n",
        "\n",
        "        # Compute dimensions of the erasing rectangle\n",
        "        h = int(round(np.sqrt(target_area / aspect_ratio)))\n",
        "        w = int(round(np.sqrt(target_area * aspect_ratio)))\n",
        "\n",
        "        if h < H and w < W:\n",
        "            # Randomly choose the top-left corner of the rectangle\n",
        "            x1 = random.randint(0, W - w)\n",
        "            y1 = random.randint(0, H - h)\n",
        "            x2 = x1 + w\n",
        "            y2 = y1 + h\n",
        "\n",
        "            # Now, check the overlap with each bounding box\n",
        "            erase_rect = np.array([x1, y1, x2, y2])  # (xmin, ymin, xmax, ymax)\n",
        "\n",
        "            exceeds_threshold = False\n",
        "\n",
        "            for bbox, bbox_area in zip(bboxes, bbox_areas):\n",
        "                # Compute intersection\n",
        "                xi1 = max(erase_rect[0], bbox[0])\n",
        "                yi1 = max(erase_rect[1], bbox[1])\n",
        "                xi2 = min(erase_rect[2], bbox[2])\n",
        "                yi2 = min(erase_rect[3], bbox[3])\n",
        "\n",
        "                inter_width = max(0, xi2 - xi1)\n",
        "                inter_height = max(0, yi2 - yi1)\n",
        "                intersection_area = inter_width * inter_height\n",
        "\n",
        "                # Compute overlap ratio\n",
        "                overlap_ratio = intersection_area / bbox_area if bbox_area > 0 else 0\n",
        "\n",
        "                if overlap_ratio > max_erasing_area_per_bbox:\n",
        "                    exceeds_threshold = True\n",
        "                    break  # No need to check further\n",
        "\n",
        "            if not exceeds_threshold:\n",
        "                # Replace the pixels with random values\n",
        "                img_tensor[:, y1:y2, x1:x2] = torch.randn(\n",
        "                    (C, y2 - y1, x2 - x1), dtype=img_tensor.dtype, device=img_tensor.device) * img_tensor.std() + img_tensor.mean()\n",
        "                \n",
        "                # Clamp the values to be between 0 and 255\n",
        "                img_tensor = torch.clamp(img_tensor, 0, 255)\n",
        "\n",
        "                # Convert back to Byte if necessary\n",
        "                img_tensor = img_tensor.type(torch.uint8)\n",
        "\n",
        "                return img_tensor\n",
        "\n",
        "    # If no valid rectangle was found after attempts, return the original image\n",
        "    print(f\"No valid erasing was applied after {attempts} attempts for this image.\")\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "def display_erased_image(df_train_images, df_train_annotations, original_filename, augmented_filename):\n",
        "    \"\"\"\n",
        "    Displays the original and augmented images with bounding boxes side by side.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images (pd.DataFrame): DataFrame containing image tensors and filenames.\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing bounding box annotations.\n",
        "    - original_filename (str): Filename of the original image.\n",
        "    - augmented_filename (str): Filename of the augmented image.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    for ax, filename in zip(axes, [original_filename, augmented_filename]):\n",
        "        visualize_image_with_bboxes(df_train_annotations, df_train_images, filename, ax=ax)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "df_train_images, df_train_annotations = add_random_erasing(df_train_images, df_train_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "rumVu1ZPoz7_",
        "outputId": "7a3bc1a7-9a1f-440e-f741-3fa2be42ce6d"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added images\n",
        "df_train_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "arBId2ohoz7_",
        "outputId": "929aa91a-b3a4-4d03-d0dd-fd27d60d53d2"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added annotations\n",
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "MKQfJZ1Toz7_",
        "outputId": "3c67c669-2b08-4f9a-8e9a-3e415fb5d7aa"
      },
      "outputs": [],
      "source": [
        "# Df image Check OG image of noise\n",
        "df_train_images[df_train_images['filename'] == 'P06558.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "94jsgJZioz7_",
        "outputId": "4b9615cc-e0e8-43a1-b775-8d882e3194d1"
      },
      "outputs": [],
      "source": [
        "# Df image Check for erase\n",
        "df_train_images[df_train_images['filename'] == 'P06558_erase.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "oRaFqcFsoz7_",
        "outputId": "0e9b66ab-af3c-467e-9cae-01978dbf27b7"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for OG image\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P06558.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dg69rrTkoz7_",
        "outputId": "f3726b9d-1d06-4d21-a0da-4542e10e8e0a"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for erase\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P06558_erase.jpg']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ61gCVJoz7_"
      },
      "source": [
        "### Random Blocking using Hide and Seek (customized algo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "y7qQuFNVoz7_"
      },
      "outputs": [],
      "source": [
        "def calculate_channel_means(image_tensor):\n",
        "    np.random.seed(53)\n",
        "    image_tensor = image_tensor.float()\n",
        "    # Calculate the mean for each channel (assuming 3 channels: R, G, B)\n",
        "    mean_r = torch.mean(image_tensor[0, :, :]).item()  # Red channel mean\n",
        "    mean_g = torch.mean(image_tensor[1, :, :]).item()  # Green channel mean\n",
        "    mean_b = torch.mean(image_tensor[2, :, :]).item()  # Blue channel mean\n",
        "\n",
        "    return mean_r, mean_g, mean_b\n",
        "\n",
        "def get_bbox(df, filename):\n",
        "    np.random.seed(53)\n",
        "    # Filter the dataframe to find the row with the given filename\n",
        "    row = df[df['filename'] == filename]\n",
        "    rand_row  = row.iloc[np.random.randint(0,len(row))]\n",
        "    # If the image is found, extract the bounding box coordinates\n",
        "    if not row.empty:\n",
        "        xmin = int(rand_row['xmin'])\n",
        "        ymin = int(rand_row['ymin'])\n",
        "        xmax = int(rand_row['xmax'])\n",
        "        ymax = int(rand_row['ymax'])\n",
        "        return xmin, ymin, xmax, ymax\n",
        "    else:\n",
        "        raise ValueError(f\"Image {filename} not found in the DataFrame.\")\n",
        "\n",
        "def hide_patch(image_tensor, mean_r, mean_g, mean_b, probability=0.2,max_patch=0.5):\n",
        "    \n",
        "    np.random.seed(53)\n",
        "    # Initialize masked_image_tensor as a clone of the original image\n",
        "    masked_image_tensor = image_tensor.clone()\n",
        "\n",
        "    # Determine whether to apply the mask (20% probability)\n",
        "    apply_mask = np.random.rand() <= probability\n",
        "\n",
        "    if apply_mask:\n",
        "        # Define the masking region\n",
        "        height, width = image_tensor.shape[1], image_tensor.shape[2]\n",
        "        mask_size = int(height * width * np.random.uniform(0.001, max_patch))  # Random size between 0% and 40%\n",
        "        mask_side = int(np.sqrt(mask_size))  # Assuming a square mask\n",
        "\n",
        "        # Ensure mask_side does not exceed image dimensions\n",
        "        mask_side = min(mask_side, height, width)\n",
        "\n",
        "        # Random starting coordinates\n",
        "        start_y = np.random.randint(0, height - mask_side)\n",
        "        start_x = np.random.randint(0, width - mask_side)\n",
        "\n",
        "        # Create a tensor with the mean values\n",
        "        mean_value_tensor = torch.tensor([mean_r, mean_g, mean_b], dtype=torch.float32).view(3, 1, 1)\n",
        "\n",
        "        # Fill the masking region with mean values\n",
        "        masked_image_tensor[:, start_y:start_y + mask_side, start_x:start_x + mask_side] = mean_value_tensor\n",
        "\n",
        "        # Convert the masked image to numpy for OpenCV processing\n",
        "        masked_image_np = masked_image_tensor.permute(1, 2, 0).numpy()\n",
        "\n",
        "        # Create a mask for the region that needs smoothing\n",
        "        mask = np.zeros((height, width), dtype=np.uint8)\n",
        "        mask[start_y:start_y + mask_side, start_x:start_x + mask_side] = 255  # Mark the masked region\n",
        "\n",
        "        # Apply Gaussian blur to the masked region\n",
        "        blurred_mask = cv2.GaussianBlur(masked_image_np, (15, 15), 0)\n",
        "\n",
        "        # Blend the original image with the blurred masked image\n",
        "        final_image = np.where(mask[:, :, np.newaxis] == 255, blurred_mask, masked_image_np)\n",
        "    else:\n",
        "        # If no mask is applied, the final image is just the original image\n",
        "        final_image = image_tensor.permute(1, 2, 0).numpy()  # Convert to numpy for visualization\n",
        "\n",
        "    return masked_image_tensor, final_image\n",
        "\n",
        "def show_images(final_image_np):\n",
        "    \n",
        "    # Create a figure with a single subplot (1 row, 1 column)\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(15, 5))\n",
        "    if isinstance(final_image_np, torch.Tensor):\n",
        "        final_image_np = final_image_np.permute(1, 2, 0).cpu().numpy()\n",
        "    # Show the final image in the single Axes object\n",
        "    ax.imshow(final_image_np.astype(np.uint8))\n",
        "    ax.set_title('Final Image')\n",
        "    ax.axis('off')  # Hide the axes ticks and labels\n",
        "    \n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQw1hrQxoz7_",
        "outputId": "41f879f5-361a-4754-b89d-69fe78fbb6a2"
      },
      "outputs": [],
      "source": [
        "def random_blocking(df_train_images, df_train_annotations, blocked=0.1):\n",
        "\n",
        "    # Set the seed for reproducibility\n",
        "    random.seed(53)\n",
        "    np.random.seed(53)\n",
        "    # Step 1: Calculate the sample size based on the percentage provided (blocked)\n",
        "    total_images = len(df_train_images)\n",
        "    sample_size = int(total_images * blocked)\n",
        "    sample_size = max(sample_size, 1)  # Ensure at least one image is selected\n",
        "\n",
        "    # Step 2: Randomly select images\n",
        "    processed_images = []\n",
        "\n",
        "    while len(processed_images) < sample_size:\n",
        "        random_image_rows = df_train_images.sample(n=sample_size - len(processed_images))\n",
        "\n",
        "        for _, row in random_image_rows.iterrows():\n",
        "            selected_image_filename = row['filename']\n",
        "\n",
        "            # Check if filename already contains '_block'\n",
        "            if '_block' in selected_image_filename:\n",
        "                continue\n",
        "\n",
        "            # Step 4: Filter the bounding boxes for the selected image\n",
        "            filtered_bboxes = df_train_annotations[df_train_annotations['filename'] == selected_image_filename]\n",
        "\n",
        "            # Step 5: Check if any bounding boxes exist for the selected image\n",
        "            if filtered_bboxes.empty:\n",
        "                print(f\"No bounding boxes found for {selected_image_filename}.\")\n",
        "                continue\n",
        "\n",
        "            # Step 6: Calculate the area and other metrics for the bounding boxes\n",
        "            filtered_bboxes['img_size'] = filtered_bboxes['width'] * filtered_bboxes['height']\n",
        "            filtered_bboxes['bb_width'] = filtered_bboxes['xmax'] - filtered_bboxes['xmin']\n",
        "            filtered_bboxes['bb_height'] = filtered_bboxes['ymax'] - filtered_bboxes['ymin']\n",
        "            filtered_bboxes['area'] = filtered_bboxes['bb_width'] * filtered_bboxes['bb_height']\n",
        "\n",
        "            # Step 7: Get the smallest bounding box\n",
        "            smallest_bbox = min(filtered_bboxes['area'])\n",
        "\n",
        "            # Step 8: Calculate the max_patch\n",
        "            max_patch = smallest_bbox / filtered_bboxes['img_size']\n",
        "            max_patch = max_patch.values[0]\n",
        "\n",
        "            # Process the image if it meets the condition\n",
        "            processed_images.append(selected_image_filename)  # Keep track of processed images\n",
        "            image_tensor = row['tensor']\n",
        "            mean_r, mean_g, mean_b = calculate_channel_means(image_tensor)\n",
        "            masked_image_tensor, final_image = hide_patch(image_tensor, mean_r, mean_g, mean_b, probability=1, max_patch=max_patch)\n",
        "\n",
        "            # Step 9: Update the tensor in df_train_images and change the filename\n",
        "            df_train_images['tensor'] = df_train_images['tensor'].astype(object)\n",
        "            new_row = row.copy()  # Copy the current row\n",
        "            new_row['tensor'] = masked_image_tensor  # Update the tensor\n",
        "            new_row['filename'] = f\"{selected_image_filename}_block\"  # Add '_block' to the filename\n",
        "\n",
        "            # Append the new row to the dataframe\n",
        "            df_train_images = pd.concat([df_train_images, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "            # Step 10: Update df_train_annotations with the augmented image's filename (dropping extra columns)\n",
        "            new_annotations = filtered_bboxes.copy()\n",
        "            new_annotations['filename'] = f\"{selected_image_filename}_block\"  # Update the filename\n",
        "\n",
        "            # Drop the temporary columns before adding to the annotations\n",
        "            new_annotations = new_annotations.drop(columns=['img_size', 'bb_width', 'bb_height', 'area'])\n",
        "            df_train_annotations = pd.concat([df_train_annotations, new_annotations], ignore_index=True)\n",
        "\n",
        "\n",
        "    return df_train_images, df_train_annotations\n",
        "\n",
        "# Example call (Assuming df_train_images and df_train_annotations are already defined)\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "\n",
        "    # Call your function\n",
        "    df_train_images, df_train_annotations = random_blocking(df_train_images, df_train_annotations)\n",
        "\n",
        "len(df_train_images), len(df_train_annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "JMsERZTgoz7_",
        "outputId": "e37f472c-497d-49e8-8f89-1e77170de765"
      },
      "outputs": [],
      "source": [
        "#saving file names after change\n",
        "block_images = df_train_images[df_train_images['filename'].str.contains('_block')]['filename']\n",
        "\n",
        "print(block_images)\n",
        "print(len(df_train_images))\n",
        "#check for a blocked image\n",
        "visualize_image_with_bboxes(df_train_annotations,df_train_images,'P07531.jpg')\n",
        "visualize_image_with_bboxes(df_train_annotations,df_train_images,'P07531.jpg_block')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "Gyx_A99uoz7_",
        "outputId": "90cc7f6d-cbbe-403f-dcda-86e8138470a5"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added images\n",
        "df_train_images.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "A138n4K4oz7_",
        "outputId": "edb64e73-5bb8-42fb-c83a-151a62d55a9c"
      },
      "outputs": [],
      "source": [
        "# Check for the number of added annotations\n",
        "df_train_annotations.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "kvyVVipToz8A",
        "outputId": "5d38aeae-9bcf-4270-bd40-fb8df1e3bf57"
      },
      "outputs": [],
      "source": [
        "# Df image Check OG image of noise\n",
        "df_train_images[df_train_images['filename'] == 'P07531.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "__kWYzeqoz8A",
        "outputId": "26c9659f-6d92-49f1-9680-d48e581d3814"
      },
      "outputs": [],
      "source": [
        "# Df image Check for erase\n",
        "df_train_images[df_train_images['filename'] == 'P07531.jpg_block']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "Fls7cdxYoz8A",
        "outputId": "122f946d-e7e6-4e11-d4e9-03267930b795"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for OG image\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P07531.jpg']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "HF1BBj5voz8A",
        "outputId": "14d5f960-32cb-4ad1-82aa-00d6643af656"
      },
      "outputs": [],
      "source": [
        "# Df annotation Check for erase\n",
        "df_train_annotations[df_train_annotations['filename'] == 'P07531.jpg_block']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "rvbGyl_Goz8A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "favxqsF0oz8A"
      },
      "source": [
        "### Random Blocking - WITHIN BBOX (DO NOT RUN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "JHc3wrSvoz8A"
      },
      "outputs": [],
      "source": [
        "# #for bbox masking (not used as of now)\n",
        "# import cv2  # OpenCV for Gaussian blur\n",
        "\n",
        "# def mask_and_blur_patch(image_tensor, xmin=None, ymin=None, xmax=None, ymax=None,\n",
        "#                         mean_r=0.0, mean_g=0.0, mean_b=0.0, probability=0.2, max_patch=0.1):\n",
        "#     \"\"\"\n",
        "#     Applies a random masking patch to the image tensor, restricting the patch to max_patch percent\n",
        "#     of the bounding box area.\n",
        "\n",
        "#     Parameters:\n",
        "#     - image_tensor: Input image tensor of shape (C, H, W).\n",
        "#     - xmin, ymin, xmax, ymax: Coordinates for a specific bounding box.\n",
        "#     - mean_r, mean_g, mean_b: Mean values for the red, green, and blue channels.\n",
        "#     - probability: Probability of applying the mask (default is 0.2 for 20%).\n",
        "#     - max_patch: Maximum percentage of the bounding box area to be masked (e.g., 0.1 = 10%).\n",
        "\n",
        "#     Returns:\n",
        "#     - masked_image_tensor: Image tensor after applying the mask.\n",
        "#     - final_image: Final blended image after Gaussian blur on the masked region.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Check if the input is a tensor\n",
        "#     if not isinstance(image_tensor, torch.Tensor):\n",
        "#         raise ValueError(\"Input image_tensor must be a PyTorch tensor\")\n",
        "\n",
        "#     # Clone the original image tensor to avoid modifying it directly\n",
        "#     masked_image_tensor = image_tensor.clone()\n",
        "\n",
        "#     # Determine whether to apply the mask based on probability\n",
        "#     apply_mask = np.random.rand() <= probability\n",
        "\n",
        "#     if apply_mask:\n",
        "#         # Ensure the bounding box coordinates are provided\n",
        "#         if xmin is not None and ymin is not None and xmax is not None and ymax is not None:\n",
        "#             start_x, start_y, end_x, end_y = xmin, ymin, xmax, ymax\n",
        "\n",
        "#             # Calculate the bounding box area\n",
        "#             bbox_width = end_x - start_x\n",
        "#             bbox_height = end_y - start_y\n",
        "#             bbox_area = bbox_width * bbox_height\n",
        "\n",
        "#             # Calculate the maximum patch size within the bounding box (max_patch % of bbox area)\n",
        "#             max_patch_area = bbox_area * max_patch\n",
        "#             mask_side = int(np.sqrt(max_patch_area))  # Calculate side of square mask\n",
        "\n",
        "\n",
        "#             mask_start_x = np.random.randint(start_x, end_x - mask_side)\n",
        "#             mask_start_y = np.random.randint(start_y, end_y - mask_side)\n",
        "\n",
        "#             mask_end_x = mask_start_x + mask_side\n",
        "#             mask_end_y = mask_start_y + mask_side\n",
        "#         else:\n",
        "#             # If no bounding box is provided, apply a random mask to the image\n",
        "#             height, width = image_tensor.shape[1], image_tensor.shape[2]\n",
        "#             mask_size = int(height * width * np.random.uniform(0, max_patch))  # Random size\n",
        "#             mask_side = int(np.sqrt(mask_size))\n",
        "\n",
        "#             # Ensure mask_side does not exceed image dimensions\n",
        "#             mask_side = min(mask_side, height, width)\n",
        "\n",
        "#             # Random starting coordinates\n",
        "#             mask_start_y = np.random.randint(0, height - mask_side)\n",
        "#             mask_start_x = np.random.randint(0, width - mask_side)\n",
        "\n",
        "#             mask_end_x = mask_start_x + mask_side\n",
        "#             mask_end_y = mask_start_y + mask_side\n",
        "\n",
        "#         # Create a tensor with the mean values\n",
        "#         mean_value_tensor = torch.tensor([mean_r, mean_g, mean_b], dtype=torch.float32).view(3, 1, 1)\n",
        "\n",
        "#         # Fill the masking region with mean values in the cloned tensor\n",
        "#         masked_image_tensor[:, mask_start_y:mask_end_y, mask_start_x:mask_end_x] = mean_value_tensor\n",
        "\n",
        "#         # Convert the masked image tensor to numpy for OpenCV processing\n",
        "#         masked_image_np = masked_image_tensor.permute(1, 2, 0).numpy()\n",
        "\n",
        "#         # Create a mask for the region that needs smoothing\n",
        "#         mask = np.zeros((masked_image_np.shape[0], masked_image_np.shape[1]), dtype=np.uint8)\n",
        "#         mask[mask_start_y:mask_end_y, mask_start_x:mask_end_x] = 255  # Mark the masked region\n",
        "\n",
        "#         # Apply Gaussian blur to the masked region\n",
        "#         blurred_mask = cv2.GaussianBlur(masked_image_np, (15, 15), 0)\n",
        "\n",
        "#         # Blend the original image with the blurred masked image\n",
        "#         final_image_np = np.where(mask[:, :, np.newaxis] == 255, blurred_mask, masked_image_np)\n",
        "\n",
        "#         # Convert the final image back to tensor\n",
        "#         final_image = torch.from_numpy(final_image_np).permute(2, 0, 1)  # Convert back to (C, H, W)\n",
        "#     else:\n",
        "#         # If no mask is applied, keep the original image unchanged\n",
        "#         final_image = masked_image_tensor.clone()  # Ensure we return a clone\n",
        "\n",
        "#     return masked_image_tensor, final_image\n",
        "\n",
        "# #testing bbox masking\n",
        "# image_tensor = df_train_images[df_train_images['filename']=='P00785.jpg']['tensor']\n",
        "# mean_r, mean_g, mean_b = calculate_channel_means(image_tensor.values[0])\n",
        "# xmin,ymin,xmax,ymax = get_bbox(df_train_annotations,'P00785.jpg')\n",
        "# bbox = (xmin,ymin,xmax,ymax)\n",
        "# masked_image_tensor, final_image = mask_and_blur_patch(image_tensor.values[0], xmin, ymin, xmax, ymax, mean_r, mean_g, mean_b, probability=1.0, max_patch=0.2)\n",
        "# # # Show the images\n",
        "# show_images(final_image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Take out pkl with Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Export to pickle\n",
        "# df_train_images.to_pickle(\"df_train_images_subset.pkl\")\n",
        "# df_train_annotations.to_pickle(\"df_train_annotations_subset.pkl\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import from pickle\n",
        "df_train_images = pd.read_pickle(\"df_train_images_subset.pkl\")\n",
        "df_train_annotations = pd.read_pickle(\"df_train_annotations_subset.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_annotations.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_images.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(df_train_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(df_train_annotations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOT9yuGfoz8A"
      },
      "source": [
        "### Normalization - Min Max\n",
        "- Please note this code will only code if all the tensors have uniform sizing i.e. same width and height"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM7ykhPHoz8A",
        "outputId": "d3a54db2-4b1e-4d97-9a5e-01c991706a94"
      },
      "outputs": [],
      "source": [
        "def normalize_tensor(tensor):\n",
        "    \"\"\"\n",
        "    Normalize a single tensor to the range [0, 1] across its elements.\n",
        "    :param tensor: A tensor in shape (channels, height, width)\n",
        "    :return: Normalized tensor\n",
        "    \"\"\"\n",
        "    # Convert to float type to avoid casting issues\n",
        "    tensor = tensor.float()\n",
        "\n",
        "    # Flatten the tensor to (channels, height*width)\n",
        "    channels, height, width = tensor.shape\n",
        "    tensor = tensor.view(channels, -1)\n",
        "\n",
        "    # Subtract min per sample\n",
        "    tensor -= tensor.min(1, keepdim=True)[0]\n",
        "\n",
        "    # Divide by max per sample (add small epsilon to avoid division by zero)\n",
        "    tensor /= (tensor.max(1, keepdim=True)[0] + 1e-6)\n",
        "\n",
        "    # Reshape back to (channels, height, width)\n",
        "    tensor = tensor.view(channels, height, width)\n",
        "\n",
        "    return tensor\n",
        "\n",
        "def process_in_mini_batches(df, batch_size):\n",
        "    \"\"\"\n",
        "    Process and normalize tensors individually within mini-batches, updating in place.\n",
        "    :param df: DataFrame containing a column 'tensor' with tensors to normalize\n",
        "    :param batch_size: Number of tensors to process per mini-batch\n",
        "    \"\"\"\n",
        "    num_samples = len(df)\n",
        "\n",
        "    # Iterate over the DataFrame in mini-batches\n",
        "    for i in range(0, num_samples, batch_size):\n",
        "        # Select the mini-batch from the DataFrame\n",
        "        batch_df = df.iloc[i:i + batch_size]\n",
        "\n",
        "        # Normalize each tensor individually and update in place\n",
        "        for idx, tensor in batch_df['tensor'].items():\n",
        "            df.at[idx, 'normalized_tensor'] = normalize_tensor(tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "# Assume df_train_images contains a column 'tensor' with 3D tensors (channels, height, width)\n",
        "batch_size = 32  # Define your batch size\n",
        "df_train_images['normalized_tensor'] = None  # Initialize an empty column\n",
        "process_in_mini_batches(df_train_images, batch_size)\n",
        "\n",
        "# Check the normalized tensors\n",
        "print(df_train_images[:10][['tensor', 'normalized_tensor']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jt4a_iqVoz8A",
        "outputId": "00ed918f-0906-4941-85bf-ed7978fe3586"
      },
      "outputs": [],
      "source": [
        "#normalization check\n",
        "\n",
        "def check_normalization(tensor):\n",
        "    \"\"\"\n",
        "    Check if a tensor is normalized (values between 0 and 1).\n",
        "    Returns True if normalized, False otherwise.\n",
        "    \"\"\"\n",
        "    return (tensor.min() >= 0) and (tensor.max() <= 1) and not torch.any(torch.isnan(tensor))\n",
        "\n",
        "def validate_normalized_tensors(df):\n",
        "    \"\"\"\n",
        "    Validate all tensors in the DataFrame to check if they are normalized.\n",
        "    Prints the result for each tensor.\n",
        "    \"\"\"\n",
        "    for index, row in df.iterrows():\n",
        "        tensor = row['normalized_tensor']\n",
        "        is_normalized = check_normalization(tensor)\n",
        "        print(f\"Tensor at index {index} normalized: {is_normalized}\")\n",
        "\n",
        "# Example usage\n",
        "validate_normalized_tensors(df_train_images[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-uM4k0loz8A"
      },
      "source": [
        "### One hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "yTGkbIlBoz8A",
        "outputId": "8c6c33b6-18a6-4168-8124-181f24f398c4"
      },
      "outputs": [],
      "source": [
        "df_train_annotations = pd.get_dummies(df_train_annotations, columns=['class'])\n",
        "print(df_train_annotations.columns)\n",
        "df_train_annotations['class'] = df_train_annotations.apply(lambda row: next((col.split('_')[1] for col in df_train_annotations.columns if col.startswith('class_') and row[col]), 'unknown'), axis=1)\n",
        "df_train_annotations.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvSOw0N4oz8A"
      },
      "source": [
        "### Checking for Class Imbalance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "VQ1xU0CMoz8A",
        "outputId": "e4204e13-7ad9-40c2-fcc9-d31480240202"
      },
      "outputs": [],
      "source": [
        "# Function to check for class imbalance\n",
        "def check_class_imbalance(df_train_annotations):\n",
        "    \"\"\"\n",
        "    This function checks for class imbalance in the dataset,\n",
        "    plots a class distribution graph, and calculates the percentage of each class.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing image annotations with a 'class' column.\n",
        "\n",
        "    Outputs:\n",
        "    - Prints the count and percentage of each class.\n",
        "    \"\"\"\n",
        "\n",
        "# Calculate the class distribution\n",
        "    class_counts = df_train_annotations['class'].value_counts()\n",
        "\n",
        "    # Calculate the percentage for each class\n",
        "    class_percentages = (class_counts / len(df_train_annotations)) * 100\n",
        "\n",
        "    # Plot the distribution graph\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    ax = class_counts.plot(kind='bar', color='skyblue')\n",
        "    plt.title('Class Distribution in Training Data')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45)\n",
        "\n",
        "    # Annotate the bars with their counts\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                    ha='center', va='center', xytext=(0, 9), textcoords='offset points')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Output the class counts and percentages\n",
        "    for class_name, count in class_counts.items():\n",
        "        percentage = class_percentages[class_name]\n",
        "        print(f\"Class: {class_name}, Count: {count}, Percentage: {percentage:.2f}%\")\n",
        "\n",
        "# Example usage\n",
        "check_class_imbalance(df_train_annotations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K92fHATwoz8A"
      },
      "source": [
        "### Adjusting class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7xJrMQiQoz8A"
      },
      "outputs": [],
      "source": [
        "def calculate_class_weights(df_train_annotations, scaling_factor=None):\n",
        "    \"\"\"\n",
        "    Calculates class weights based on the inverse frequency of classes.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_annotations (pd.DataFrame): DataFrame containing annotations with a 'class' column.\n",
        "    - scaling_factor (float, optional): Optional scaling factor to adjust the class weights.\n",
        "\n",
        "    Returns:\n",
        "    - class_weights (dict): Dictionary mapping class labels to their calculated weights.\n",
        "    \"\"\"\n",
        "    # Count the frequency of each class\n",
        "    class_counts = df_train_annotations['class'].value_counts()\n",
        "\n",
        "    # Calculate inverse frequencies\n",
        "    inverse_freq = 1.0 / class_counts\n",
        "\n",
        "    # Normalize weights to sum to 1\n",
        "    class_weights = inverse_freq / inverse_freq.sum()\n",
        "\n",
        "    # Apply scaling factor if provided\n",
        "    if scaling_factor is not None:\n",
        "        class_weights *= scaling_factor\n",
        "\n",
        "    # Convert to dictionary\n",
        "    class_weights = class_weights.to_dict()\n",
        "\n",
        "    return class_weights\n",
        "\n",
        "class_weights = calculate_class_weights(df_train_annotations, scaling_factor=1.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuvwVEROoz8A",
        "outputId": "fa92d6db-ed9d-4c91-83d1-25dc0bd34cdb"
      },
      "outputs": [],
      "source": [
        "class_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNqDqDMloz8A"
      },
      "source": [
        "### Defining and Segregating Images into three Occlusion levels (low, medium, high) [TEST SET]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FUNCTIONS\n",
        "\n",
        "torch_to_np = lambda tse: tse.permute(1, 2, 0).numpy()\n",
        "np_to_torch = lambda npa: torch.from_numpy(npa).permute(2, 0, 1)\n",
        "\n",
        "sorted_join = lambda st: \"\".join(sorted(list(set(\"\".join(sorted(st))))))\n",
        "\n",
        "get_area = lambda arbb: (arbb[2]-arbb[0])*(arbb[3]-arbb[1])\n",
        "\n",
        "def calculate_intersection_area(box1, box2, get_intersection_box=False):\n",
        "    # Box format: [x_min, y_min, x_max, y_max]\n",
        "    x_min1, y_min1, x_max1, y_max1 = box1\n",
        "    x_min2, y_min2, x_max2, y_max2 = box2\n",
        "    \n",
        "    inter_x_min = max(x_min1, x_min2)\n",
        "    inter_y_min = max(y_min1, y_min2)\n",
        "    inter_x_max = min(x_max1, x_max2)\n",
        "    inter_y_max = min(y_max1, y_max2)\n",
        "    \n",
        "    inter_width = max(0, inter_x_max - inter_x_min)\n",
        "    inter_height = max(0, inter_y_max - inter_y_min)\n",
        "    \n",
        "    intersection_area = inter_width * inter_height\n",
        "    if get_intersection_box:\n",
        "        return intersection_area, [inter_x_min, inter_y_min, inter_x_max, inter_y_max]\n",
        "        \n",
        "    return intersection_area\n",
        "\n",
        "# GRAPH CLASSES\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, label, box):\n",
        "        self.label = label  \n",
        "        self.box = box  \n",
        "        self.area = self.calculate_area(box)  \n",
        "        self.children = [] \n",
        "    \n",
        "    def calculate_area(self, box):\n",
        "        x_min, y_min, x_max, y_max = box\n",
        "        width = x_max - x_min\n",
        "        height = y_max - y_min\n",
        "        return width * height\n",
        "\n",
        "    def add_child(self, child_node):\n",
        "        self.children.append(child_node)  \n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Node({self.label}, Box: {self.box}, Area: {self.area}, Children: {[child.label for child in self.children]})\"\n",
        "\n",
        "\n",
        "class BoundingBoxGraph:\n",
        "    def __init__(self):\n",
        "        self.nodes = {}\n",
        "    \n",
        "    def add_node(self, label, box):\n",
        "        node = Node(label, box)  \n",
        "        self.nodes[label] = node \n",
        "\n",
        "    def add_edge(self, parent_label, child_label):\n",
        "        if parent_label in self.nodes and child_label in self.nodes:\n",
        "            self.nodes[parent_label].add_child(self.nodes[child_label])\n",
        "    \n",
        "    # def display_graph(self):\n",
        "    #     G = nx.DiGraph()\n",
        "\n",
        "    #     for label, node in self.nodes.items():\n",
        "    #         G.add_node(label, box=node.box, area=node.area)\n",
        "    #         for child in node.children:\n",
        "    #             G.add_edge(label, child.label)\n",
        "\n",
        "    #     pos = nx.shell_layout(G)  \n",
        "\n",
        "    #     box_labels = {node: f\"Box: {self.nodes[node].box}\" for node in self.nodes}\n",
        "    #     area_labels = {node: f\"Area: {self.nodes[node].area}\" for node in self.nodes}\n",
        "\n",
        "    #     nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=400, font_size=9, font_weight='bold', font_color='red',  horizontalalignment='right')\n",
        "    #     nx.draw_networkx_labels(G, pos, labels=area_labels, font_size=8, verticalalignment='center_baseline', font_color='green', horizontalalignment='left')\n",
        "\n",
        "    #     plt.show()\n",
        "    \n",
        "    def is_blood_line(self, nd1, nd2):\n",
        "        nd1.children\n",
        "\n",
        "    def get_nodes(self):\n",
        "        return self.nodes\n",
        "    \n",
        "    def get_node_dict(self):\n",
        "        n_dict = {}\n",
        "        for nd in list(self.nodes.items()):\n",
        "            n_dict[nd[0]] = nd[1]\n",
        "        return n_dict\n",
        "\n",
        "\n",
        "    def is_descendant(self, node1_label, node2_label):\n",
        "        \"\"\"\n",
        "        Check if node1 is a descendant of node2.\n",
        "        This function traverses the graph starting from node2 and looks for node1 in its children.\n",
        "        \"\"\"\n",
        "        if node2_label not in self.nodes or node1_label not in self.nodes:\n",
        "            return False  \n",
        "\n",
        "        node2 = self.nodes[node2_label]\n",
        "        \n",
        "        def check_descendant(node, target_label):\n",
        "            if node.label == target_label:\n",
        "                return True\n",
        "            for child in node.children:\n",
        "                if check_descendant(child, target_label):\n",
        "                    return True\n",
        "            return False\n",
        "        \n",
        "        return check_descendant(node2, node1_label)\n",
        "\n",
        "    def get_far_descendants(self, element):\n",
        "        # Convert the input element to a set of characters\n",
        "        element_set = set(element)\n",
        "        \n",
        "        descendants = []\n",
        "        \n",
        "        for item in self.get_nodes().keys():\n",
        "            item_set = set(item)\n",
        "            \n",
        "            if element_set.issubset(item_set) and len(item_set) > len(element_set):\n",
        "                descendants.append(item)\n",
        "        \n",
        "        return descendants\n",
        "\n",
        "    \n",
        "    def get_all_descendants(self, node_label):\n",
        "        \"\"\"\n",
        "        Get all descendants of a node in a list.\n",
        "        \"\"\"\n",
        "        if node_label not in self.nodes:\n",
        "            return []  \n",
        "\n",
        "        node = self.nodes[node_label]\n",
        "        descendants = []\n",
        "\n",
        "        def collect_descendants(node):\n",
        "            for child in node.children:\n",
        "                descendants.append(child.label)  \n",
        "                collect_descendants(child)  \n",
        "\n",
        "        collect_descendants(node)\n",
        "\n",
        "        return descendants\n",
        "        \n",
        "    def print_graph(self):\n",
        "        # Print all the nodes with their children\n",
        "        for node in self.nodes.values():\n",
        "            print(node)\n",
        "\n",
        "# CONVERT DATA FRAMES TO REQUIRED STRUCTURES\n",
        "\n",
        "# Assuming df_test_images and df_test_annotations are your data frames\n",
        "\n",
        "# Extract filenames and images from df_test_images\n",
        "filenames = df_test_images['filename'].tolist()\n",
        "resized_images = df_test_images['tensor'].tolist()\n",
        "\n",
        "# Build object_location\n",
        "\n",
        "# Ensure 'class' column exists in df_test_annotations\n",
        "if 'class' not in df_test_annotations.columns:\n",
        "    raise ValueError(\"The 'class' column is missing in df_test_annotations.\")\n",
        "\n",
        "object_location = []\n",
        "\n",
        "for filename in filenames:\n",
        "    annotations = df_test_annotations[df_test_annotations['filename'] == filename]\n",
        "\n",
        "    image_annotations = {}\n",
        "\n",
        "    for idx, row in annotations.iterrows():\n",
        "        label = row['class']  # Use 'class' column\n",
        "        bbox = [row['xmin'], row['ymin'], row['xmax'], row['ymax']]\n",
        "        image_annotations[idx] = (label, bbox)  # Use idx from iterrows as a unique index\n",
        "\n",
        "    object_location.append(image_annotations)\n",
        "\n",
        "# LOOP\n",
        "\n",
        "occlusion_dict = {}\n",
        "file_len = len(filenames)\n",
        "for fi in range(file_len):\n",
        "    occ_pic = resized_images[fi]\n",
        "    occ_fn = filenames[fi]\n",
        "    occ_ann = object_location[fi]\n",
        "    print(f\"{fi+1}/{file_len} <> {occ_fn}\")\n",
        "    \n",
        "    item_to_code = {}\n",
        "    code_to_item = {}\n",
        "    code_ann = {}\n",
        "    for idx, (index, val) in enumerate(occ_ann.items()):\n",
        "        item, bbox = val\n",
        "        this_code = chr(65+idx)  # Use idx to generate code\n",
        "        this_item = item + str(idx)  # Combine label and idx for unique identifier\n",
        "        item_to_code[this_item] = this_code\n",
        "        code_to_item[this_code] = this_item\n",
        "\n",
        "        code_ann[index] = [this_code, bbox]\n",
        "\n",
        "    graph = BoundingBoxGraph()\n",
        "    for k, v in code_ann.items():          \n",
        "        graph.add_node(v[0], v[1])\n",
        "    all_nodes = list(graph.get_nodes().keys())\n",
        "    combinations = itertools.combinations(all_nodes, 2)\n",
        "    sorted_combinations = [tuple(sorted(pair)) for pair in combinations]\n",
        "    sorted_combinations = sorted(sorted_combinations)\n",
        "    ndict = graph.get_node_dict()\n",
        "    generation_dict = {}\n",
        "    pop_list = list(graph.get_node_dict())\n",
        "    combinations = list(itertools.combinations(all_nodes, 2))\n",
        "    i = 0\n",
        "    while len(combinations) > i:\n",
        "        if i > 200:\n",
        "            break\n",
        "        n1, n2 = combinations[i]\n",
        "        node_1 = ndict[n1]\n",
        "        node_2 = ndict[n2]\n",
        "        if (calculate_intersection_area(node_1.box, node_2.box) > 0 and (sorted_join(n1+n2) not in all_nodes) and (not graph.is_descendant(n1, n2)) and (not graph.is_descendant(n2, n1))):\n",
        "            new_node_name = sorted_join(n1+n2)\n",
        "            new_area, new_box = calculate_intersection_area(node_1.box, node_2.box, True)\n",
        "            graph.add_node(new_node_name, new_box)\n",
        "            graph.add_edge(n1, new_node_name)  \n",
        "            graph.add_edge(n2, new_node_name)  \n",
        "            for old_node_name in all_nodes:\n",
        "                combinations.append((old_node_name, new_node_name))\n",
        "\n",
        "            all_nodes.append(new_node_name)\n",
        "            ndict = graph.get_node_dict()\n",
        "        i += 1\n",
        "    for iternode in all_nodes:\n",
        "        if len(iternode) not in generation_dict:\n",
        "            generation_dict[len(iternode)] = [iternode]\n",
        "        else:\n",
        "            generation_dict[len(iternode)].append(iternode)\n",
        "    if len(generation_dict) > 1:\n",
        "        second_last_generation = len(generation_dict) - 1\n",
        "\n",
        "        for this_generation in range(second_last_generation, 0, -1): \n",
        "            for young_parrent in generation_dict[this_generation]:\n",
        "                its_decendents = graph.get_far_descendants(young_parrent)\n",
        "                if its_decendents:\n",
        "                    area_of_decendents = sum([ndict[this_decendent].area for this_decendent in its_decendents])\n",
        "                    ndict[young_parrent].area -= area_of_decendents\n",
        "    poly_name, poly_area, poly_overlaps = [], [], []\n",
        "    for this_poly_code, this_poly_node in graph.get_nodes().items():\n",
        "        if this_poly_node.area > 0:\n",
        "            poly_items = \"\"\n",
        "            for chari, char in enumerate(this_poly_code):\n",
        "                if chari == 0:\n",
        "                    poly_items = poly_items + code_to_item[char]\n",
        "                else:\n",
        "                    poly_items = poly_items + \"_\" + code_to_item[char]\n",
        "            poly_name.append(poly_items)\n",
        "            poly_area.append(this_poly_node.area)\n",
        "            poly_overlaps.append(len(this_poly_code))\n",
        "\n",
        "    temp = {'poly_name': poly_name, 'poly_area': poly_area, 'Number_of_overlaps': poly_overlaps} \n",
        "    occlusion_dict[occ_fn] = temp\n",
        "    # If you want to create a DataFrame for each image, you can uncomment the following line\n",
        "    # occlusion_df = pd.DataFrame(temp)\n",
        "    # If you want to display the graph and image, you can uncomment the following lines\n",
        "    # graph.display_graph()\n",
        "    # show_image(occ_pic, occ_ann)\n",
        "    # print(occlusion_df)\n",
        "\n",
        "# Now occlusion_dict contains the desired data\n",
        "# If you want to combine all data into a single DataFrame\n",
        "occlusion_list = []\n",
        "for filename, data in occlusion_dict.items():\n",
        "    df = pd.DataFrame(data)\n",
        "    df['filename'] = filename\n",
        "    occlusion_list.append(df)\n",
        "\n",
        "# Combine all DataFrames\n",
        "final_occlusion_df = pd.concat(occlusion_list, ignore_index=True)\n",
        "\n",
        "# Display the final DataFrame\n",
        "print(final_occlusion_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_occlusion_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Calculate SSQ per row\n",
        "final_occlusion_df['ssq_value'] = (final_occlusion_df['Number_of_overlaps'] ** 2) * final_occlusion_df['poly_area']\n",
        "\n",
        "# Step 2: Aggregate by filename to get the weighted sum of squares (SSQ)\n",
        "weighted_ssq_df = final_occlusion_df.groupby('filename')['ssq_value'].sum().reset_index()\n",
        "\n",
        "# Step 3: Rename column to 'weighted_ssq_occlusions'\n",
        "weighted_ssq_df.rename(columns={'ssq_value': 'weighted_ssq_occlusions'}, inplace=True)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "weighted_ssq_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBDbIakZoz8B"
      },
      "source": [
        "### Removing Unwanted Features [TRAIN SET] of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ho_MT4wDoz8B",
        "outputId": "317f6d0e-e0db-4e32-881e-df2874565b00"
      },
      "outputs": [],
      "source": [
        "unique_difficult = df_train_annotations['difficult'].unique()\n",
        "print(f\"Unique values in 'difficult': {unique_difficult}\")\n",
        "\n",
        "unique_truncated = df_train_annotations['truncated'].unique()\n",
        "print(f\"Unique values in 'difficult': {unique_truncated}\")\n",
        "\n",
        "unique_pose = df_train_annotations['pose'].unique()\n",
        "print(f\"Unique values in 'pose': {unique_pose}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "FFv-7vt6oz8B",
        "outputId": "e295140e-bbbd-4859-8456-aee0038d5eb0"
      },
      "outputs": [],
      "source": [
        "df_train_annotations_cleaned = df_train_annotations.drop(columns=['depth', 'height','width','pose', 'truncated', 'difficult'])\n",
        "df_train_annotations_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "bcd7gJ02oz8B",
        "outputId": "b690268d-4a06-4409-d826-6188341485ca"
      },
      "outputs": [],
      "source": [
        "df_train_images_cleaned = df_train_images.drop(columns=['tensor', 'height', 'width', 'mode'])\n",
        "df_train_images_cleaned.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQY05M0Goz8B"
      },
      "source": [
        "### Converting Df to Tensor Dataset [TRAIN SET] and using DataLoader for batching"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BQBJZnooz8B",
        "outputId": "a0a2fdb2-4ae7-4cb8-da91-f01793ff7804"
      },
      "outputs": [],
      "source": [
        "# Custom collate function to handle varying number of bounding boxes\n",
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []  # Store dictionaries for boxes and labels\n",
        "\n",
        "    for item in batch:\n",
        "        images.append(item[0])  # Image tensor\n",
        "\n",
        "        # Store targets as a dictionary with 'boxes' and 'labels'\n",
        "        targets.append({\n",
        "            'boxes': item[1],   # Bounding boxes\n",
        "            'labels': item[2]   # Class labels (integer encoded)\n",
        "        })\n",
        "\n",
        "    # Stack images normally, since their sizes are uniform\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Return stacked images and list of dictionaries for boxes and labels\n",
        "    return images, targets\n",
        "\n",
        "\n",
        "class XrayDataset(Dataset):\n",
        "    def __init__(self, df_images, df_annotations):\n",
        "        self.df_images = df_images\n",
        "        self.df_annotations = df_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image tensor (ensure it's a PyTorch tensor)\n",
        "        image_tensor = self.df_images.iloc[idx]['normalized_tensor']\n",
        "        if not isinstance(image_tensor, torch.Tensor):\n",
        "            image_tensor = torch.tensor(image_tensor, dtype=torch.float32)\n",
        "\n",
        "        # Get the filename for this image\n",
        "        filename = self.df_images.iloc[idx]['filename']\n",
        "\n",
        "        # Get bounding box and labels for the same image\n",
        "        annotation = self.df_annotations[self.df_annotations['filename'] == filename]\n",
        "\n",
        "        # Convert bounding box to torch tensors\n",
        "        boxes = torch.tensor(annotation[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype=torch.float32)\n",
        "\n",
        "        # Convert one-hot encoded labels back to class indices\n",
        "        one_hot_labels = annotation[['class_gun', 'class_knife', 'class_plier', 'class_scissor', 'class_wrench']].values\n",
        "        labels = torch.tensor([label.argmax() for label in one_hot_labels], dtype=torch.int64)  # Integer labels\n",
        "\n",
        "        return image_tensor, boxes, labels\n",
        "\n",
        "# Create Dataset and DataLoader with the custom collate function (EXAMPLE USAGE TO CHECK)\n",
        "train_dataset = XrayDataset(df_train_images_cleaned, df_train_annotations_cleaned)\n",
        "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True, collate_fn=custom_collate_fn)\n",
        "\n",
        "# Check the first few records in DataLoader\n",
        "for i, (image_tensor, targets) in enumerate(train_loader):\n",
        "    print(f\"Batch {i+1}:\")\n",
        "    print(f\"Image Tensor Shape: {image_tensor.shape}\")  # Image tensor shape\n",
        "    print(f\"Bounding Boxes: {len(targets)} -> {[t['boxes'].shape for t in targets]}\")  # Bounding box shapes\n",
        "    print(f\"Labels: {len(targets)} -> {[t['labels'].shape for t in targets]}\")  # Labels for each image\n",
        "\n",
        "    # Print only the first batch for checking\n",
        "    if i == 0:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgApZzz3oz8B",
        "outputId": "06a34013-c299-4ec0-f6de-2176fc22f960"
      },
      "outputs": [],
      "source": [
        "# Unpack the first batch for inspection\n",
        "for i, (image_tensor, targets) in enumerate(train_loader):\n",
        "    print(f\"\\nBatch {i+1}:\")\n",
        "\n",
        "    # Image tensor shape\n",
        "    print(f\"Image Tensor Shape: {image_tensor.shape}\")  # Should be (32, 3, 584, 688)\n",
        "\n",
        "    # Inspect the first image in this batch\n",
        "    first_image_tensor = image_tensor[0]\n",
        "    print(f\"First Image Tensor Shape: {first_image_tensor.shape}\")  # Should be (3, 584, 688)\n",
        "    print(f\"First Image Tensor Values (first 10 pixels): \\n{first_image_tensor[:, :11, :11]}\")  # Inspect the first few pixel values\n",
        "\n",
        "    # Bounding boxes for the first image\n",
        "    first_image_boxes = targets[0]['boxes']\n",
        "    print(f\"Bounding Boxes for First Image: {first_image_boxes.shape}\")\n",
        "    print(f\"First Image Bounding Box Coordinates: \\n{first_image_boxes}\")  # Should print the [xmin, ymin, xmax, ymax] values\n",
        "\n",
        "    # Labels for the first image\n",
        "    first_image_labels = targets[0]['labels']\n",
        "    print(f\"Labels for First Image (integer encoded): {first_image_labels}\")\n",
        "\n",
        "    # Interpreting labels as class indices\n",
        "    interpreted_labels = first_image_labels.tolist()  # Convert the tensor to list\n",
        "    print(f\"Interpreted Labels: {interpreted_labels}\")  # Integer encoded class labels\n",
        "\n",
        "    # Stop after the first batch\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYo8r_KIoz8B",
        "outputId": "6a32627e-3a3c-426a-e702-1badc511a6a3"
      },
      "outputs": [],
      "source": [
        "print(image_tensor[2],targets[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nygc1gARoz8B",
        "outputId": "88991467-ab68-4250-d7bf-714dd3f0c7f5"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ2Tqaanoz8B"
      },
      "source": [
        "### Export Final Tensors as pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slF2HLTVoz8B",
        "outputId": "6434ac11-8e70-4427-fe16-fef6f7859750"
      },
      "outputs": [],
      "source": [
        "# Check the number of items in the dataset\n",
        "print(f\"Total number of items in train_dataset: {len(train_dataset)}\")\n",
        "\n",
        "# Inspect the first few items (images, bounding boxes, and labels)\n",
        "for idx in range(5):  # Inspect first 5 items\n",
        "    image_tensor, boxes, labels = train_dataset[idx]\n",
        "\n",
        "    print(f\"\\nItem {idx + 1}:\")\n",
        "    print(f\"Image Tensor Shape: {image_tensor.shape}\")  # Shape of the image tensor\n",
        "    print(f\"Bounding Boxes: {boxes}\")  # Bounding boxes as a tensor\n",
        "    print(f\"Labels: {labels}\")  # Labels (class indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "xibaT33moz8B"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "# import gzip\n",
        "\n",
        "# # Specify the path where you want to save the compressed pkl file\n",
        "# output_file = 'train_dataset_full.pkl.gz'\n",
        "\n",
        "# # Prepare an empty list to collect all batches\n",
        "# full_dataset = []\n",
        "\n",
        "# batch_size = 100  # Process dataset in batches of 100\n",
        "# total_items = len(train_dataset)\n",
        "\n",
        "# # Collect the dataset in chunks (batches)\n",
        "# for start_idx in range(0, total_items, batch_size):\n",
        "#     end_idx = min(start_idx + batch_size, total_items)\n",
        "#     batch = [train_dataset[i] for i in range(start_idx, end_idx)]\n",
        "\n",
        "#     # Append the current batch to the full dataset\n",
        "#     full_dataset.extend(batch)\n",
        "#     print(f\"Processed items {start_idx + 1} to {end_idx}\")\n",
        "\n",
        "# # Now, save and compress the entire dataset to a gzip file\n",
        "# with gzip.open(output_file, 'wb') as f:\n",
        "#     pickle.dump(full_dataset, f)\n",
        "\n",
        "# print(f\"\\ntrain_dataset successfully exported and compressed as {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FncSbqUDoz8B",
        "outputId": "b1913be2-c4af-4adf-b18a-7bb3e53a9fbc"
      },
      "outputs": [],
      "source": [
        "len(train_dataset[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "2nolziZ3oz8B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8FBgamgoz8B"
      },
      "source": [
        "### Deletion of Any Augmented File (Reset Purpose)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UMTzNp5Woz8B"
      },
      "outputs": [],
      "source": [
        "# Use this function to delete any type of augmented data (e.g., ColourJitter or Flip)\n",
        "def delete_augmented_data(df_train_images, df_train_annotations, augmentation_type):\n",
        "    \"\"\"\n",
        "    Deletes all augmented images with filenames containing the specified augmentation type (e.g., '_jit', '_rot'),\n",
        "    and removes corresponding entries from df_train_images and df_train_annotations DataFrames.\n",
        "    It checks images and annotations separately and performs actions based on which one has augmented entries.\n",
        "\n",
        "    Parameters:\n",
        "    - df_train_images: DataFrame containing image metadata.\n",
        "    - df_train_annotations: DataFrame containing annotation metadata.\n",
        "    - augmentation_type: The type of augmentation to delete (e.g., '_jit', '_rot').\n",
        "\n",
        "    Returns:\n",
        "    - Updated df_train_images and df_train_annotations DataFrames.\n",
        "    \"\"\"\n",
        "    # Validate input\n",
        "    if not isinstance(augmentation_type, str) or not augmentation_type.startswith('_'):\n",
        "        raise ValueError(\"Invalid augmentation_type. It must be a string that starts with an underscore (e.g., '_jit', '_hvflip').\")\n",
        "\n",
        "    # Part 1: Handle augmented entries in df_train_images\n",
        "    augmented_df_images = df_train_images[df_train_images['filename'].str.contains(augmentation_type)]\n",
        "    if not augmented_df_images.empty:\n",
        "        initial_images_count = len(df_train_images)\n",
        "        df_train_images = df_train_images[~df_train_images['filename'].str.contains(augmentation_type)]\n",
        "        images_removed = initial_images_count - len(df_train_images)\n",
        "        print(f\"Removed {images_removed} '{augmentation_type}' entries from df_train_images.\")\n",
        "    else:\n",
        "        print(f\"No '{augmentation_type}' entries found in df_train_images.\")\n",
        "\n",
        "    # Part 2: Handle augmented entries in df_train_annotations\n",
        "    augmented_df_annotations = df_train_annotations[df_train_annotations['filename'].str.contains(augmentation_type)]\n",
        "    if not augmented_df_annotations.empty:\n",
        "        initial_annotations_count = len(df_train_annotations)\n",
        "        df_train_annotations = df_train_annotations[~df_train_annotations['filename'].str.contains(augmentation_type)]\n",
        "        annotations_removed = initial_annotations_count - len(df_train_annotations)\n",
        "        print(f\"Removed {annotations_removed} '{augmentation_type}' entries from df_train_annotations.\")\n",
        "    else:\n",
        "        print(f\"No '{augmentation_type}' entries found in df_train_annotations.\")\n",
        "\n",
        "    return df_train_images, df_train_annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aEPyNijFoz8B"
      },
      "outputs": [],
      "source": [
        "# Use this to delete all Rotated Images and its corresponding annotations.\n",
        "# df_train_images, df_train_annotations = delete_augmented_data(\n",
        "#     df_train_images, df_train_annotations, '_block'\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "b-CArcezoz8B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doHfVYtyoz8B"
      },
      "source": [
        "## Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLue2Kc3oz8B"
      },
      "source": [
        "### Faster R-CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "p_z68QF1oz8B"
      },
      "outputs": [],
      "source": [
        "# Custom collate function to handle varying number of bounding boxes\n",
        "# Before using the bounding boxes in training\n",
        "def filter_invalid_images_and_boxes(images, targets):\n",
        "    \"\"\"\n",
        "    Filters out invalid bounding boxes with zero or negative width/height.\n",
        "    Also removes images that have no valid bounding boxes.\n",
        "\n",
        "    Parameters:\n",
        "    - images (list of Tensor): List of image tensors.\n",
        "    - targets (list of dict): List of target dictionaries containing 'boxes' and 'labels'.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_images: List of images with valid targets.\n",
        "    - filtered_targets: Corresponding list of targets with valid boxes.\n",
        "    \"\"\"\n",
        "    filtered_images = []\n",
        "    filtered_targets = []\n",
        "\n",
        "    for img, target in zip(images, targets):\n",
        "        valid_boxes = []\n",
        "        valid_labels = []\n",
        "\n",
        "        for i, box in enumerate(target['boxes']):\n",
        "            xmin, ymin, xmax, ymax = box.tolist()\n",
        "            # Check if the box has positive width and height\n",
        "            if xmax > xmin and ymax > ymin:\n",
        "                valid_boxes.append(box)\n",
        "                valid_labels.append(target['labels'][i])\n",
        "            else:\n",
        "                # Output the invalid bounding box\n",
        "                print(f\"Invalid bounding box found: {box}\")\n",
        "                print(f\"Corresponding label: {target['labels'][i]}\")\n",
        "\n",
        "        # If there are valid boxes, keep the image and the target; otherwise, skip the image\n",
        "        if valid_boxes:\n",
        "            filtered_target = {\n",
        "                'boxes': torch.stack(valid_boxes),  # Stack valid boxes\n",
        "                'labels': torch.tensor(valid_labels)\n",
        "            }\n",
        "            filtered_images.append(img)  # Keep the image\n",
        "            filtered_targets.append(filtered_target)  # Keep the filtered target\n",
        "        else:\n",
        "            print(\"No valid boxes, skipping image\")\n",
        "\n",
        "    return filtered_images, filtered_targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "u6z6mZACoz8B"
      },
      "outputs": [],
      "source": [
        "# Function to create the Faster R-CNN model\n",
        "def create_model_frcnn(num_classes, anchor_sizes, aspect_ratios, rpn_nms_thresh, backbone_name='resnet50', pretrained=True):\n",
        "    \"\"\"\n",
        "    Creates a Faster R-CNN model with the specified backbone.\n",
        "\n",
        "    Parameters:\n",
        "    - num_classes (int): Number of classes for detection (including background).\n",
        "    - backbone_name (str): Name of the backbone model.\n",
        "    - pretrained (bool): Whether to use ImageNet pretrained weights.\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): Faster R-CNN model.\n",
        "    \"\"\"\n",
        "    # Load the backbone with pre-trained weights\n",
        "    if backbone_name == 'resnet50':\n",
        "        weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V1 if pretrained else None\n",
        "        backbone = torchvision.models.resnet50(weights=weights)\n",
        "        backbone = nn.Sequential(*list(backbone.children())[:-2])  # Remove classification head\n",
        "        backbone.out_channels = 2048\n",
        "    else:\n",
        "        raise ValueError(f\"Backbone '{backbone_name}' is not supported.\")\n",
        "\n",
        "    # Define anchor generator with sizes and aspect ratios\n",
        "    anchor_generator = AnchorGenerator(\n",
        "        sizes=anchor_sizes,\n",
        "        aspect_ratios=aspect_ratios\n",
        "    )\n",
        "\n",
        "    # Define RoIAlign\n",
        "    roi_pooler = MultiScaleRoIAlign(\n",
        "        featmap_names=['0'],  # Names of the feature maps\n",
        "        output_size=7,\n",
        "        sampling_ratio=2\n",
        "    )\n",
        "\n",
        "    # Put the pieces together inside a FasterRCNN model\n",
        "    model = FasterRCNN (\n",
        "        backbone,\n",
        "        num_classes=num_classes,\n",
        "        rpn_anchor_generator=anchor_generator,\n",
        "        box_roi_pool=roi_pooler,\n",
        "        rpn_pre_nms_top_n_train=2000,  # Pre-NMS Top K\n",
        "        rpn_post_nms_top_n_train=1000,  # Post-NMS Top K\n",
        "        rpn_nms_thresh=rpn_nms_thresh  # NMS Threshold\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define custom fastrcnn_loss function with class weights\n",
        "def custom_fastrcnn_loss(class_logits, box_regression, labels, regression_targets, class_weights_tensor, device):\n",
        "    \"\"\"\n",
        "    Computes the loss for Faster R-CNN with class weights.\n",
        "\n",
        "    Arguments:\n",
        "        class_logits (Tensor): the predicted class logits\n",
        "        box_regression (Tensor): the predicted box regression\n",
        "        labels (list[Tensor]): the ground truth class labels\n",
        "        regression_targets (Tensor): the ground truth box regression targets\n",
        "\n",
        "    Returns:\n",
        "        classification_loss (Tensor)\n",
        "        box_loss (Tensor)\n",
        "    \"\"\"\n",
        "    labels = torch.cat(labels, dim=0)\n",
        "    # Use class_weights_tensor in cross_entropy loss\n",
        "    classification_loss = F.cross_entropy(class_logits, labels, weight=class_weights_tensor)\n",
        "\n",
        "    # Compute box regression loss\n",
        "    sampled_pos_inds_subset = torch.where(labels > 0)[0]\n",
        "    labels_pos = labels[sampled_pos_inds_subset]\n",
        "    if sampled_pos_inds_subset.numel() > 0:\n",
        "        box_regression = box_regression[sampled_pos_inds_subset]\n",
        "        regression_targets = regression_targets[sampled_pos_inds_subset]\n",
        "        box_loss = F.smooth_l1_loss(\n",
        "            box_regression,\n",
        "            regression_targets,\n",
        "            beta=1 / 9,\n",
        "            reduction=\"sum\",\n",
        "        ) / labels.numel()\n",
        "    else:\n",
        "        box_loss = torch.tensor(0).to(device)\n",
        "\n",
        "    return classification_loss, box_loss\n",
        "\n",
        "# Function to train the model\n",
        "def train_model_frcnn(model, dataloader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor, fold_idx=0 , device=None, save_path='best_model_fold_{fold_idx}_epoch_{epoch}.pth'):\n",
        "    \"\"\"\n",
        "    Trains the Faster R-CNN model.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): The Faster R-CNN model.\n",
        "    - dataloader (DataLoader): DataLoader for training data.\n",
        "    - optimizer (torch.optim.Optimizer): Optimizer.\n",
        "    - scheduler (torch.optim.lr_scheduler): Learning rate scheduler.\n",
        "    - num_epochs (int): Number of epochs to train.\n",
        "    - fold_idx (int): Current fold index during 20 fold cross-validation.\n",
        "    - save_path (str): Path to save the best model.\n",
        "    - device (torch.device): Device to train on.\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): Trained model.\n",
        "    - best_fold_val_loss (float): Best validation loss for the fold.\n",
        "    - best_epoch (int): Epoch at which the best validation loss occurred.\n",
        "    \"\"\"\n",
        "    # Initialize TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    # Monkey-patch the model's fastrcnn_loss function\n",
        "    model.roi_heads.fastrcnn_loss = partial(custom_fastrcnn_loss, class_weights_tensor=class_weights_tensor, device=device)\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_fold_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    patience = 3  # Patience for early stopping\n",
        "    best_epoch = 0  # Track the epoch where the best loss occurred\n",
        "    best_model_state = None\n",
        "\n",
        "    total_training_time = 0.0  # Initialize total training time\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        epoch_loss = 0.0\n",
        "        epoch_loss_classifier = 0.0\n",
        "        epoch_loss_box_reg = 0.0\n",
        "        epoch_loss_objectness = 0.0\n",
        "        epoch_loss_rpn_box_reg = 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress bar\n",
        "        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\", dynamic_ncols=True)\n",
        "\n",
        "        for i, (images, targets) in pbar:\n",
        "            # Filter out invalid boxes and their corresponding images\n",
        "            images, targets = filter_invalid_images_and_boxes(images, targets)\n",
        "            if not images:  # Skip if no valid images in this batch\n",
        "                continue\n",
        "\n",
        "            images = [image.to(device) for image in images]\n",
        "            target_list = [{'boxes': t['boxes'].to(device), 'labels': t['labels'].to(device)} for t in targets]\n",
        "            for t in targets:\n",
        "                d = {}\n",
        "                d['boxes'] = t['boxes'].to(device)\n",
        "                d['labels'] = t['labels'].to(device)\n",
        "                target_list.append(d)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            try:\n",
        "                loss_dict = model(images, target_list)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                loss_value = losses.item()\n",
        "\n",
        "                # Backward pass\n",
        "                losses.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Accumulate losses\n",
        "                epoch_loss += loss_value\n",
        "                epoch_loss_classifier += loss_dict['loss_classifier'].item()\n",
        "                epoch_loss_box_reg += loss_dict['loss_box_reg'].item()\n",
        "                epoch_loss_objectness += loss_dict['loss_objectness'].item()\n",
        "                epoch_loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\n",
        "                    'Total Loss': f'{loss_value:.4f}',\n",
        "                    'Cls': f\"{loss_dict['loss_classifier'].item():.4f}\",\n",
        "                    'BoxReg': f\"{loss_dict['loss_box_reg'].item():.4f}\",\n",
        "                    'Obj': f\"{loss_dict['loss_objectness'].item():.4f}\",\n",
        "                    'RPNBoxReg': f\"{loss_dict['loss_rpn_box_reg'].item():.4f}\"\n",
        "                })\n",
        "\n",
        "                # Log to TensorBoard\n",
        "                global_step = epoch * len(dataloader) + i\n",
        "                writer.add_scalar('Loss/Total', loss_value, global_step)\n",
        "                writer.add_scalar('Loss/Classifier', loss_dict['loss_classifier'].item(), global_step)\n",
        "                writer.add_scalar('Loss/BoxReg', loss_dict['loss_box_reg'].item(), global_step)\n",
        "                writer.add_scalar('Loss/Objectness', loss_dict['loss_objectness'].item(), global_step)\n",
        "                writer.add_scalar('Loss/RPNBoxReg', loss_dict['loss_rpn_box_reg'].item(), global_step)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during training: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "        # Calculate average losses\n",
        "        num_batches = len(dataloader)\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        avg_loss_classifier = epoch_loss_classifier / num_batches\n",
        "        avg_loss_box_reg = epoch_loss_box_reg / num_batches\n",
        "        avg_loss_objectness = epoch_loss_objectness / num_batches\n",
        "        avg_loss_rpn_box_reg = epoch_loss_rpn_box_reg / num_batches\n",
        "\n",
        "        # Log average losses to TensorBoard\n",
        "        writer.add_scalar('Loss_epoch/Total', avg_loss, epoch)\n",
        "        writer.add_scalar('Loss_epoch/Classifier', avg_loss_classifier, epoch)\n",
        "        writer.add_scalar('Loss_epoch/BoxReg', avg_loss_box_reg, epoch)\n",
        "        writer.add_scalar('Loss_epoch/Objectness', avg_loss_objectness, epoch)\n",
        "        writer.add_scalar('Loss_epoch/RPNBoxReg', avg_loss_rpn_box_reg, epoch)\n",
        "\n",
        "        # Calculate elapsed time for the epoch\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_training_time += elapsed_time  # Accumulate the total training time\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Total Loss: {avg_loss:.4f}, \"\n",
        "              f\"Cls Loss: {avg_loss_classifier:.4f}, \"\n",
        "              f\"Box Reg Loss: {avg_loss_box_reg:.4f}, \"\n",
        "              f\"Obj Loss: {avg_loss_objectness:.4f}, \"\n",
        "              f\"RPN Box Reg Loss: {avg_loss_rpn_box_reg:.4f}, \"\n",
        "              f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "\n",
        "        print(\"Calculating Validation loss...\")\n",
        "\n",
        "        # Validation step\n",
        "        model.train()  # Keep model in training mode during validation to get losses\n",
        "\n",
        "        # Optionally set BatchNorm layers to eval mode if needed\n",
        "        # for module in model.modules():\n",
        "        #     if isinstance(module, torch.nn.modules.BatchNorm2d):\n",
        "        #         module.eval()\n",
        "\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for images, targets in val_loader:\n",
        "                # Filter out invalid boxes and their corresponding images\n",
        "                images, targets = filter_invalid_images_and_boxes(images, targets)\n",
        "                if not images:  # Skip if no valid images in this batch\n",
        "                    continue\n",
        "\n",
        "                images = [img.to(device) for img in images]\n",
        "\n",
        "                # Prepare target list\n",
        "                target_list = [{'boxes': t['boxes'].to(device), 'labels': t['labels'].to(device)} for t in targets]\n",
        "\n",
        "                # Forward pass\n",
        "                try:\n",
        "                    loss_dict = model(images, target_list)  # Now returns losses\n",
        "                    losses = sum(loss for loss in loss_dict.values())  # loss_dict is a dict now\n",
        "                    val_loss += losses.item()\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during validation: {e}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Update the best model if validation loss improves\n",
        "        if avg_val_loss < best_fold_val_loss:\n",
        "            print(f\"Validation loss improved from {best_fold_val_loss:.4f} to {avg_val_loss:.4f}.\")\n",
        "            best_fold_val_loss = avg_val_loss\n",
        "            best_epoch = epoch + 1  # Track the epoch at which the best loss occurred\n",
        "            best_model_state = copy.deepcopy(model.state_dict())  # Save the best model state\n",
        "            epochs_without_improvement = 0  # Reset counter for early stopping\n",
        "        else:\n",
        "            print(f\"Validation loss did not improve. Val loss: {avg_val_loss}\")\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        # Check early stopping condition\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
        "            break\n",
        "\n",
        "    # After all epochs, save the best model for this fold\n",
        "    if best_model_state is not None:\n",
        "        model_save_path = save_path.format(fold_idx=fold_idx+1, epoch=best_epoch)\n",
        "        torch.save(best_model_state, model_save_path)\n",
        "        print(f\"Best model for Fold {fold_idx + 1} saved at {model_save_path}.\")\n",
        "\n",
        "    torch.cuda.empty_cache()  # Clear cache to prevent memory issues\n",
        "    gc.collect() # Trigger garbage collection to free CPU memory\n",
        "\n",
        "    # Close TensorBoard writer\n",
        "    writer.close()\n",
        "\n",
        "    return model, best_fold_val_loss, best_epoch, total_training_time, best_model_state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4LonMn3oz8C"
      },
      "source": [
        "### STN+FasterRCNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "fzO4nx9xoz8C"
      },
      "outputs": [],
      "source": [
        "#helper function\n",
        "def compute_projective_grid(transform_params, input_image):#we first create the grid coordinates here specifically for projective stn type - it is similar to affin but can in output change the symmetry of the image\n",
        "    batch_size, _, _ = transform_params.size()\n",
        "\n",
        "    height, width = input_image.size(2), input_image.size(3)\n",
        "    grid_coords = torch.stack(torch.meshgrid(\n",
        "        torch.linspace(-1, 1, height),\n",
        "        torch.linspace(-1, 1, width)\n",
        "    ), dim=-1).unsqueeze(0).expand(batch_size, -1, -1, -1).to(input_image.device)\n",
        "\n",
        "\n",
        "    grid_coords = grid_coords.view(batch_size, -1, 2)\n",
        "    grid_coords = torch.cat([grid_coords, torch.ones_like(grid_coords[:, :, :1])], dim=-1)  # Add ones for homogenous coordinates\n",
        "\n",
        "    grid_transformed = torch.bmm(grid_coords, transform_params.transpose(1, 2))\n",
        "\n",
        "\n",
        "    grid_transformed = grid_transformed[:, :, :2] / grid_transformed[:, :, 2:].clamp(min=1e-6)\n",
        "\n",
        "    #once we apply the the grid coordinates to the tensor - it is transformed\n",
        "    grid_transformed = grid_transformed.view(batch_size, height, width, 2)\n",
        "    return grid_transformed\n",
        "\n",
        "def limit_affine_params(transform_params, scale_limit=(0.8, 1.1), translate_limit=(-0.2, 0.2)):\n",
        "    theta_clamped = transform_params.clone()\n",
        "    theta_clamped[:, 0, 0] = transform_params[:, 0, 0].clamp(*scale_limit)\n",
        "    theta_clamped[:, 1, 1] = transform_params[:, 1, 1].clamp(*scale_limit)\n",
        "    theta_clamped[:, 0, 2] = transform_params[:, 0, 2].clamp(*translate_limit)\n",
        "    theta_clamped[:, 1, 2] = transform_params[:, 1, 2].clamp(*translate_limit)\n",
        "    return theta_clamped\n",
        "\n",
        "# helper function for tps (Thin Plate Spline) for radial basis - https://math.stackexchange.com/questions/4378825/how-to-compute-weights-in-thin-plate-spline-radial-basis-function\n",
        "def radial_basis_fn(input_pts, base_pts):\n",
        "    num_inputs = input_pts.size(0)\n",
        "    num_base = base_pts.size(0)\n",
        "    diffs = input_pts.view(num_inputs, 1, 2) - base_pts.view(1, num_base, 2) #we calculate the radial basis for the x and y coordinates here\n",
        "    dist_squared = (diffs ** 2).sum(dim=2) #then get the square\n",
        "    rbf_output = 0.5 * dist_squared * torch.log(dist_squared + 1e-6)  # small value added to avoid calculation complexity\n",
        "    return rbf_output.masked_fill(rbf_output != rbf_output, 0) #no nas\n",
        "\n",
        "\n",
        "\n",
        "# grid generator for tps stn type : inspiration taken from grid gen created here https://github.com/liuzhuang1024/stn_tps/blob/master/tps_grid_gen.py\n",
        "class FlexibleTPSGrid(nn.Module):\n",
        "  #tps transformation being slightly more complicated than affine and projective requires a dynamic grid generated\n",
        "    def __init__(self, output_height, output_width, control_points):\n",
        "        super(FlexibleTPSGrid, self).__init__()\n",
        "        num_points = control_points.size(0) #here we first set the num of points - in our case 10 at random\n",
        "        # kernel matrix\n",
        "        kernel_matrix = torch.zeros(num_points + 3, num_points + 3) #then we create a matrix which will be square and we initiate this as our grid\n",
        "        rbf_representation = radial_basis_fn(control_points, control_points) #we calculate the radial basis from above for the points\n",
        "        kernel_matrix[:num_points, :num_points].copy_(rbf_representation) #we populate using the the radial basis for those positions\n",
        "        kernel_matrix[:num_points, -3].fill_(1) #rest we fill with one for easier compution\n",
        "        kernel_matrix[-3, :num_points].fill_(1)\n",
        "        kernel_matrix[:num_points, -2:].copy_(control_points)\n",
        "        kernel_matrix[-2:, :num_points].copy_(control_points.transpose(0, 1))\n",
        "\n",
        "        # we take the inverse if this martix which we will use for further computation\n",
        "        self.inv_kernel_matrix = torch.inverse(kernel_matrix)\n",
        "\n",
        "        # grid coordinates\n",
        "        num_pixels = output_height * output_width\n",
        "        grid_coords = torch.Tensor(list(itertools.product(range(output_height), range(output_width))))\n",
        "        y_coords, x_coords = grid_coords.split(1, dim=1)\n",
        "        y_coords = y_coords * 2 / (output_height - 1) - 1\n",
        "        x_coords = x_coords * 2 / (output_width - 1) - 1\n",
        "        final_grid_coords = torch.cat([x_coords, y_coords], dim=1)\n",
        "        rbf_target = radial_basis_fn(final_grid_coords, control_points)\n",
        "        self.final_representation = torch.cat([rbf_target, torch.ones(num_pixels, 1), final_grid_coords], dim=1)\n",
        "\n",
        "        # we had to fix padding matrix to 3 rows to match concat\n",
        "        self.register_buffer('padding_matrix', torch.zeros(3, 2))  #3,2\n",
        "\n",
        "    def forward(self, src_control_points):\n",
        "        # in order to maintain consistency, we first get batch size\n",
        "        batch_size = src_control_points.size(0)\n",
        "\n",
        "        # for computiation here we need to add padding 3x2\n",
        "        padding_matrix_expanded = self.padding_matrix.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "\n",
        "        # this helps further concatenation\n",
        "        extended_control_pts = torch.cat([src_control_points, padding_matrix_expanded], dim=1)\n",
        "\n",
        "        # then we match the inverse matrox for 12x12\n",
        "        batch_inv_kernel_matrix = self.inv_kernel_matrix.expand(batch_size, -1, -1)\n",
        "\n",
        "        # we get the transformation matrix by multiplying the points and the inverse matrix\n",
        "        transformation_matrix = torch.matmul(batch_inv_kernel_matrix, extended_control_pts)\n",
        "\n",
        "        # we get target gridby using the final representation - radial basis calculation and kernel\n",
        "        target_grid = torch.matmul(self.final_representation, transformation_matrix)\n",
        "        return target_grid\n",
        "\n",
        "\n",
        "\n",
        "# spatial transformer network - affine , projective, tps  - we use only one during training\n",
        "class DynamicSTN(nn.Module):\n",
        "    def __init__(self, transformation_type='affine', control_pts=10, out_height=584, out_width=688):#default type set to afiine\n",
        "        super(DynamicSTN, self).__init__()\n",
        "        self.transformation_type = transformation_type\n",
        "        self.control_pts = control_pts\n",
        "        self.out_height = out_height\n",
        "        self.out_width = out_width\n",
        "\n",
        "        # localization network cretaion - inital feature input map for stn , predicts params for affine transformation matrix for tn tutorial in pytorch\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=7, stride=1, padding=0),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
        "            nn.MaxPool2d(2, stride=2), #we max pool to\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        # affine matrix formation\n",
        "        if self.transformation_type == 'affine':\n",
        "            self.fc_transform = nn.Sequential(\n",
        "                nn.Linear(32 * 142 * 168, 128),\n",
        "                nn.ReLU(True),\n",
        "                nn.Linear(128, 6)  #2x3 (6 params)\n",
        "            )\n",
        "            self.fc_transform[2].weight.data.zero_()\n",
        "            self.fc_transform[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float, device=self.fc_transform[2].bias.data.device))\n",
        "\n",
        "        # projective transformation\n",
        "        elif self.transformation_type == 'projective':\n",
        "            self.fc_transform = nn.Sequential(\n",
        "                nn.Linear(32 * 142 * 168, 128),\n",
        "                nn.ReLU(True),\n",
        "                nn.Linear(128, 9)  # 3x3 matrix\n",
        "            )\n",
        "            self.fc_transform[2].weight.data.zero_()\n",
        "            self.fc_transform[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0, 0, 0, 1], dtype=torch.float))\n",
        "\n",
        "        # tps ransformation\n",
        "        elif self.transformation_type == 'tps':\n",
        "\n",
        "            control_pts = self.adjust_control_pts(control_pts) #we need a perfect square for computation\n",
        "\n",
        "\n",
        "            self.fc_transform = nn.Sequential(\n",
        "                nn.Linear(32 * 142 * 168, 128),\n",
        "                nn.ReLU(True),\n",
        "                nn.Linear(128, control_pts * 2)  # to get tps control points\n",
        "            )\n",
        "\n",
        "            # initil wieghts for transformation\n",
        "            self.fc_transform[2].weight.data.zero_()\n",
        "            self.fc_transform[2].bias.data.copy_(self._initialize_control_pts(control_pts).view(-1))\n",
        "\n",
        "            # inititalize grid generator\n",
        "            self.tps_grid_gen = FlexibleTPSGrid(self.out_height, self.out_width, self._initialize_control_pts(control_pts))\n",
        "\n",
        "    def adjust_control_pts(self, control_pts):\n",
        "        #helper fn to get perfect square\n",
        "        grid_size = int(math.sqrt(control_pts))\n",
        "        if grid_size ** 2 != control_pts:\n",
        "            control_pts = grid_size ** 2\n",
        "            print(f\"Adjusted control_pts to {control_pts} (nearest perfect square).\")\n",
        "        return control_pts\n",
        "\n",
        "    def _initialize_control_pts(self, control_pts): #helper function\n",
        "        grid_size = int(math.sqrt(control_pts))\n",
        "        control_grid = torch.meshgrid(\n",
        "            torch.linspace(-1, 1, grid_size),\n",
        "            torch.linspace(-1, 1, grid_size),\n",
        "            indexing='ij'\n",
        "        )\n",
        "        control_x = control_grid[0].reshape(-1, 1)\n",
        "        control_y = control_grid[1].reshape(-1, 1)\n",
        "        return torch.cat([control_x, control_y], dim=1)\n",
        "\n",
        "\n",
        "    def forward(self, input_image, targets=None):\n",
        "        # extract features\n",
        "        features = self.localization(input_image)\n",
        "        features = features.view(-1, 32 * 142 * 168)\n",
        "        transform_params = self.fc_transform(features)\n",
        "\n",
        "        #transformation based on type\n",
        "        if self.transformation_type == 'affine':\n",
        "            transform_params = transform_params.view(-1, 2, 3)\n",
        "            transform_params = limit_affine_params(transform_params)\n",
        "            grid = F.affine_grid(transform_params, input_image.size(), align_corners=False)\n",
        "            print(\"theta\",transform_params)\n",
        "\n",
        "        elif self.transformation_type == 'projective':\n",
        "            transform_params = transform_params.view(-1, 3, 3)\n",
        "            grid = compute_projective_grid(transform_params, input_image)\n",
        "\n",
        "        elif self.transformation_type == 'tps':\n",
        "\n",
        "            batch_size = input_image.size(0)\n",
        "            control_points = transform_params.view(batch_size, 9, 2)\n",
        "            # print(f\"Control Points Shape: {control_points.size()}\")\n",
        "\n",
        "            # use initiated grid gen\n",
        "            grid = self.tps_grid_gen(control_points)\n",
        "\n",
        "            # reshape the generated grid to match the image dimensions: [batch_size, height, width, 2]\n",
        "            height, width = input_image.shape[2], input_image.shape[3]\n",
        "            grid = grid.view(batch_size, height, width, 2)\n",
        "            # print(f\"Reshaped Grid Shape: {grid.size()}\")\n",
        "\n",
        "        output_image = F.grid_sample(input_image, grid, align_corners=False)\n",
        "\n",
        "        return output_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "AaHaPuafoz8C"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet50\n",
        "\n",
        "class FasterRCNNWithParallelSTN(nn.Module):\n",
        "\n",
        "    ##we create a custom mpsule to faster rcnn which runs stn together -\n",
        "    ##we leverage the existing faster rcnn library but we first call stn on the images to transform them\n",
        "\n",
        "    def __init__(self, num_classes, anchor_sizes, aspect_ratios, rpn_nms_thresh, backbone_name='resnet50', pretrained=True, stn_type='affine'):\n",
        "        super(FasterRCNNWithParallelSTN, self).__init__()\n",
        "\n",
        "        # for pre-trained ResNet backbone\n",
        "        if backbone_name == 'resnet50': #resnet50 backbone chosen because we want to fins occlusions and it is the most efficient pretrained model (Faster R-CNN tutorial by torchvision)\n",
        "            weights = resnet50(weights='IMAGENET1K_V1') if pretrained else None\n",
        "            backbone = nn.Sequential(*list(weights.children())[:-2])  # Remove final classification head\n",
        "            backbone.out_channels = 2048\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported backbone: {backbone_name}\")\n",
        "\n",
        "        # DynamicSTN with the specified transformation type (affine/projective/tps)\n",
        "        self.stn = DynamicSTN(transformation_type='affine').to(device)\n",
        "\n",
        "        # anchor generator for the Region Proposal Network (RPN)\n",
        "        anchor_generator = AnchorGenerator(\n",
        "            sizes=anchor_sizes,\n",
        "            aspect_ratios=aspect_ratios\n",
        "        )\n",
        "\n",
        "        # RoIAlign layer for pooling\n",
        "        roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "            featmap_names=['0'], output_size=7, sampling_ratio=2\n",
        "        )\n",
        "\n",
        "\n",
        "        # faster R-CNN initialization\n",
        "        self.faster_rcnn = FasterRCNN(\n",
        "            backbone=backbone,\n",
        "            num_classes=num_classes,\n",
        "            rpn_anchor_generator=anchor_generator,\n",
        "            box_roi_pool=roi_pooler,\n",
        "            rpn_pre_nms_top_n_train=2000,\n",
        "            rpn_post_nms_top_n_train=1000,\n",
        "            rpn_nms_thresh=rpn_nms_thresh\n",
        "        ) #we declare the regular faster rcnn here\n",
        "\n",
        "    def forward(self, images, targets=None):\n",
        "        # apply the chosen STN transformation to each image first\n",
        "        transformed_images = images\n",
        "        if targets is not None:\n",
        "            images_tensor = torch.stack(images, dim=0).to(device)\n",
        "            print(f\"Stacked Images Tensor Shape: {images_tensor.shape}\")\n",
        "            # print(images_tensor)\n",
        "\n",
        "            transformed_images_tensor = self.stn(images_tensor).to(device)\n",
        "            theta = self.stn.fc_transform[-1].weight.data\n",
        "            print(f\"Transformed Images Tensor Shape: {transformed_images_tensor.shape}\")\n",
        "            print(transformed_images_tensor.shape)\n",
        "            if (theta.abs() > 1e5).any() or (theta.abs() < 1e-2).all():\n",
        "                print(\"Freezing transformation parameters\")\n",
        "                for param in self.stn.fc_transform.parameters():\n",
        "                    param.requires_grad = False\n",
        "            else:\n",
        "                for param in self.stn.fc_transform.parameters():\n",
        "                    param.requires_grad = True\n",
        "            transformed_images = [img for img in transformed_images_tensor]\n",
        "            print(f\"Number of Transformed Images: {len(transformed_images)}\")\n",
        "            print(f\"First Transformed Image Tensor min: {transformed_images[0].min()}, max: {transformed_images[0].max()}\")\n",
        "            return self.faster_rcnn(transformed_images, targets)\n",
        "        else:\n",
        "            return self.faster_rcnn(transformed_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "_tlEnGcEoz8C"
      },
      "outputs": [],
      "source": [
        "#to view stns\n",
        "def save_transformed_tensors(tensors, transformation_names, save_dir, batch_idx):\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    for idx, (tensor, transform_name) in enumerate(zip(tensors, transformation_names)):\n",
        "        tensor_filename = f\"{batch_idx}_tensor_{idx}_{transform_name}.pt\"\n",
        "        save_path = os.path.join(save_dir, tensor_filename)\n",
        "        torch.save(tensor.cpu(), save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "v6DZ7nX9oz8C"
      },
      "outputs": [],
      "source": [
        "def create_model(num_classes, anchor_sizes, aspect_ratios, rpn_nms_thresh, backbone_name='resnet50', pretrained=True, stn_type='affine'):\n",
        "    \"\"\"\n",
        "    Creates a Faster R-CNN model with parallel STNs and dynamic hyperparameters.\n",
        "\n",
        "    Parameters:\n",
        "    - num_classes (int): Number of classes for detection (including background).\n",
        "    - anchor_sizes (tuple): Sizes for anchors.\n",
        "    - aspect_ratios (tuple): Aspect ratios for anchors.\n",
        "    - rpn_nms_thresh (float): NMS threshold for Region Proposal Network.\n",
        "    - backbone_name (str): Name of the backbone model.\n",
        "    - pretrained (bool): Whether to use ImageNet pretrained weights.\n",
        "\n",
        "    Returns:\n",
        "    - model (torch.nn.Module): Faster R-CNN model with parallel STNs.\n",
        "    \"\"\"\n",
        "    return FasterRCNNWithParallelSTN(\n",
        "        num_classes=num_classes,\n",
        "        anchor_sizes=anchor_sizes,\n",
        "        aspect_ratios=aspect_ratios,\n",
        "        rpn_nms_thresh=rpn_nms_thresh,\n",
        "        backbone_name=backbone_name,\n",
        "        pretrained=pretrained,\n",
        "        stn_type = stn_type\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "WE8nd-_Zoz8C"
      },
      "outputs": [],
      "source": [
        "def train_model(model, dataloader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor, fold_idx=0, device=None, save_path='best_model_fold_{fold_idx}_epoch_{epoch}.pth', save_dir=\"transformed_tensors\"):\n",
        "    \"\"\"\n",
        "    Trains the Faster R-CNN model with parallel STNs and saves transformed tensors.\n",
        "    \"\"\"\n",
        "    # Initialize TensorBoard writer\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    model.faster_rcnn.roi_heads.fastrcnn_loss = partial(custom_fastrcnn_loss, class_weights_tensor=class_weights_tensor, device=device)\n",
        "\n",
        "    # Early stopping variables and other training setup\n",
        "    best_fold_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    patience = 3  # Early stopping patience\n",
        "    best_epoch = 0\n",
        "    total_training_time = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        epoch_loss = 0.0\n",
        "        epoch_loss_classifier = 0.0\n",
        "        epoch_loss_box_reg = 0.0\n",
        "        epoch_loss_objectness = 0.0\n",
        "        epoch_loss_rpn_box_reg = 0.0\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress bar\n",
        "        pbar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{num_epochs}\", dynamic_ncols=True)\n",
        "\n",
        "        for i, (images, targets) in pbar:\n",
        "            images, targets = filter_invalid_images_and_boxes(images, targets)\n",
        "            if not images:\n",
        "                continue\n",
        "\n",
        "            images = [image.to(device) for image in images]\n",
        "            target_list = [{'boxes': t['boxes'].to(device), 'labels': t['labels'].to(device)} for t in targets]\n",
        "\n",
        "            for t in targets:\n",
        "                d = {}\n",
        "                d['boxes'] = t['boxes'].to(device)\n",
        "                d['labels'] = t['labels'].to(device)\n",
        "                target_list.append(d)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            try:\n",
        "                loss_dict = model(images, target_list)\n",
        "                losses = sum(loss for loss in loss_dict.values())\n",
        "                loss_value = losses.item()\n",
        "                # Debugging: Check the loss dictionary\n",
        "                print(f\"Loss dict: {loss_dict}\")\n",
        "                print(f\"Total loss: {loss_value}\")\n",
        "                # Backward pass\n",
        "                losses.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Accumulate losses\n",
        "                epoch_loss += loss_value\n",
        "                epoch_loss_classifier += loss_dict['loss_classifier'].item()\n",
        "                epoch_loss_box_reg += loss_dict['loss_box_reg'].item()\n",
        "                epoch_loss_objectness += loss_dict['loss_objectness'].item()\n",
        "                epoch_loss_rpn_box_reg += loss_dict['loss_rpn_box_reg'].item()\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix({\n",
        "                    'Total Loss': f'{loss_value:.4f}',\n",
        "                })\n",
        "\n",
        "                # Log to TensorBoard\n",
        "                global_step = epoch * len(dataloader) + i\n",
        "                writer.add_scalar('Loss/Total', loss_value, global_step)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during training: {e}\")\n",
        "                traceback.print_exc()\n",
        "                continue\n",
        "\n",
        "        # Scheduler step\n",
        "        scheduler.step()\n",
        "\n",
        "       # Calculate average losses\n",
        "        num_batches = len(dataloader)\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        avg_loss_classifier = epoch_loss_classifier / num_batches\n",
        "        avg_loss_box_reg = epoch_loss_box_reg / num_batches\n",
        "        avg_loss_objectness = epoch_loss_objectness / num_batches\n",
        "        avg_loss_rpn_box_reg = epoch_loss_rpn_box_reg / num_batches\n",
        "\n",
        "        # Log average losses to TensorBoard\n",
        "        writer.add_scalar('Loss_epoch/Total', avg_loss, epoch)\n",
        "        writer.add_scalar('Loss_epoch/Classifier', avg_loss_classifier, epoch)\n",
        "        writer.add_scalar('Loss_epoch/BoxReg', avg_loss_box_reg, epoch)\n",
        "        writer.add_scalar('Loss_epoch/Objectness', avg_loss_objectness, epoch)\n",
        "        writer.add_scalar('Loss_epoch/RPNBoxReg', avg_loss_rpn_box_reg, epoch)\n",
        "\n",
        "        # Calculate elapsed time for the epoch\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_training_time += elapsed_time  # Accumulate the total training time\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
        "              f\"Total Loss: {avg_loss:.4f}, \"\n",
        "              f\"Cls Loss: {avg_loss_classifier:.4f}, \"\n",
        "              f\"Box Reg Loss: {avg_loss_box_reg:.4f}, \"\n",
        "              f\"Obj Loss: {avg_loss_objectness:.4f}, \"\n",
        "              f\"RPN Box Reg Loss: {avg_loss_rpn_box_reg:.4f}, \"\n",
        "              f\"Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Validation step\n",
        "        model.train()    #to get losses\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "              for i, (images, targets) in enumerate(val_loader):\n",
        "                  images, targets = filter_invalid_images_and_boxes(images, targets)\n",
        "                  if not images:\n",
        "                      continue\n",
        "                  images = [image.to(device) for image in images]\n",
        "                  target_list = [{'boxes': t['boxes'].to(device), 'labels': t['labels'].to(device)} for t in targets]\n",
        "                  for t in targets:\n",
        "                    d = {}\n",
        "                    d['boxes'] = t['boxes'].to(device)\n",
        "                    d['labels'] = t['labels'].to(device)\n",
        "                    target_list.append(d)\n",
        "                  try:\n",
        "                    loss_dict = model(images, target_list)  # Now returns losses\n",
        "                    losses = sum(loss for loss in loss_dict.values())  # loss_dict is a dict now\n",
        "                    val_loss += losses.item()\n",
        "                  except Exception as e:\n",
        "                      print(f\"Error during validation: {e}\")\n",
        "                      continue\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "\n",
        "        # Save the model if validation loss improves\n",
        "        if avg_val_loss < best_fold_val_loss:\n",
        "            print(f\"Validation loss improved from {best_fold_val_loss:.4f} to {avg_val_loss:.4f}. Saving model.\")\n",
        "            best_fold_val_loss = avg_val_loss\n",
        "            best_epoch = epoch + 1  # Track the epoch at which the best loss occurred\n",
        "            torch.save(model.state_dict(), save_path.format(fold_idx=fold_idx, epoch=epoch+1))\n",
        "            epochs_without_improvement = 0  # Reset counter for early stopping\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        # Check early stopping condition\n",
        "        if epochs_without_improvement >= patience:\n",
        "            print(f\"Early stopping triggered at epoch {epoch+1}.\")\n",
        "            break\n",
        "\n",
        "    writer.close()\n",
        "    return model, best_fold_val_loss, best_epoch, total_training_time, model.state_dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Az7qE8Ckoz8C"
      },
      "source": [
        "### RFBNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "lAtdvupDoz8C"
      },
      "outputs": [],
      "source": [
        "# Define the image resizing function\n",
        "def image_resizer(img, new_dim):\n",
        "    new_h, new_w = new_dim\n",
        "    resize_transform = transforms.Resize((new_h, new_w))\n",
        "    image_resized = resize_transform(transforms.ToPILImage()(img))  # Convert to PIL image and resize\n",
        "    return transforms.ToTensor()(image_resized)\n",
        "\n",
        "# Define the object location resizing function\n",
        "def object_location_resizer(objs, old_dim=(584, 688), new_dim=(512, 512)):\n",
        "    old_height, old_width = old_dim\n",
        "    new_height, new_width = new_dim\n",
        "    height_ratio = new_height / old_height\n",
        "    width_ratio = new_width / old_width\n",
        "\n",
        "    ratioLst = [width_ratio, height_ratio, width_ratio, height_ratio]\n",
        "    new_boxes = []\n",
        "    for box in objs:\n",
        "        box = box.tolist()\n",
        "        box = [abs(int(a * b)) for a, b in zip(box, ratioLst)]\n",
        "        new_boxes.append(box)\n",
        "    return torch.tensor(new_boxes)\n",
        "\n",
        "# Define a custom Dataset that processes the images on-the-fly\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset, old_dim=(584, 688), new_dim=(512, 512)):\n",
        "        self.dataset = dataset\n",
        "        self.old_dim = old_dim\n",
        "        self.new_dim = new_dim\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image, bounding boxes, and labels\n",
        "        img, boxes, labels = self.dataset[idx]\n",
        "\n",
        "        # Resize image and bounding boxes on-the-fly\n",
        "        img_resized = image_resizer(img, self.new_dim)\n",
        "        boxes_resized = object_location_resizer(boxes, old_dim=self.old_dim, new_dim=self.new_dim)\n",
        "\n",
        "        return img_resized, boxes_resized, labels\n",
        "\n",
        "# Instantiate the dataset\n",
        "train_dataset_rfb = CustomDataset(train_dataset, old_dim=(584, 688), new_dim=(512, 512))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "GTvhctVpoz8C"
      },
      "outputs": [],
      "source": [
        "class BasicConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        self.out_channels = out_planes\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
        "        self.relu = nn.ReLU(inplace=True) if relu else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        if self.bn is not None:\n",
        "            x = self.bn(x)\n",
        "        if self.relu is not None:\n",
        "            x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicRFB(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1,map_reduce=8):\n",
        "        super(BasicRFB, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.out_channels = out_planes\n",
        "        inter_planes = in_planes // map_reduce\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),\n",
        "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n",
        "                )\n",
        "        self.branch1 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n",
        "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
        "                )\n",
        "        self.branch2 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n",
        "                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=3, stride=stride, padding=1),\n",
        "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n",
        "                )\n",
        "        self.branch3 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
        "                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=(7,1), stride=stride, padding=(3,0)),\n",
        "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
        "                )\n",
        "\n",
        "        self.ConvLinear = BasicConv(8*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n",
        "        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "\n",
        "        out = torch.cat((x0,x1,x2,x3),1)\n",
        "        out = self.ConvLinear(out)\n",
        "        short = self.shortcut(x)\n",
        "        out = out*self.scale + short\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BasicRFB_c(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1,map_reduce=8):\n",
        "        super(BasicRFB_c, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.out_channels = out_planes\n",
        "        inter_planes = in_planes // map_reduce\n",
        "        self.branch0 = nn.Sequential(\n",
        "                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),\n",
        "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n",
        "                )\n",
        "        self.branch1 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n",
        "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
        "                )\n",
        "        self.branch2 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
        "                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=(7,1), stride=stride, padding=(3,0)),\n",
        "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
        "                )\n",
        "\n",
        "        self.ConvLinear = BasicConv(6*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n",
        "        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "\n",
        "        out = torch.cat((x0,x1,x2),1)\n",
        "        out = self.ConvLinear(out)\n",
        "        short = self.shortcut(x)\n",
        "        out = out*self.scale + short\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class BasicRFB_a(nn.Module):\n",
        "\n",
        "    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n",
        "        super(BasicRFB_a, self).__init__()\n",
        "        self.scale = scale\n",
        "        self.out_channels = out_planes\n",
        "        inter_planes = in_planes //8\n",
        "\n",
        "\n",
        "        self.branch0 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n",
        "                )\n",
        "        self.branch1 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
        "                )\n",
        "        self.branch2 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
        "                )\n",
        "        self.branch3 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n",
        "                )\n",
        "        self.branch4 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n",
        "                )\n",
        "        self.branch5 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n",
        "                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
        "                )\n",
        "\n",
        "        self.branch6 = nn.Sequential(\n",
        "                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n",
        "                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(3,1), stride=1, padding=(1,0)),\n",
        "                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n",
        "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
        "                )\n",
        "        self.ConvLinear = BasicConv(7*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n",
        "        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n",
        "        self.relu = nn.ReLU(inplace=False)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x0 = self.branch0(x)\n",
        "        x1 = self.branch1(x)\n",
        "        x2 = self.branch2(x)\n",
        "        x3 = self.branch3(x)\n",
        "        x4 = self.branch4(x)\n",
        "        x5 = self.branch5(x)\n",
        "        x6 = self.branch6(x)\n",
        "\n",
        "        out = torch.cat((x0,x1,x2,x3,x4,x5,x6),1)\n",
        "        out = self.ConvLinear(out)\n",
        "        short = self.shortcut(x)\n",
        "        out = out*self.scale + short\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class RFBNet(nn.Module):\n",
        "\n",
        "    def __init__(self, img_size, base, extras, head, num_classes):\n",
        "        print(\"Entered RFB========\")\n",
        "        super(RFBNet, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        if img_size == 300:\n",
        "            self.indicator = 3\n",
        "        elif img_size == 512:\n",
        "            self.indicator = 5\n",
        "        else:\n",
        "            self.indicator = 5\n",
        "            print(f\"GAK PASS RFBNet__init__ : indicator {self.indicator} Error: Sorry only RFB300 and RFB512 are supported!\")\n",
        "            # return\n",
        "        self.base = nn.ModuleList(base)\n",
        "        # Layer learns to scale the l2 normalized features from conv4_3\n",
        "\n",
        "        self.reduce = BasicConv(512,256,kernel_size=1,stride=1)\n",
        "        self.up_reduce = BasicConv(1024,256,kernel_size=1,stride=1)\n",
        "\n",
        "        self.rfb = BasicRFB_a(512, 512, stride=1, scale=1.0)\n",
        "        self.extras = nn.ModuleList(extras)\n",
        "\n",
        "        self.loc = nn.ModuleList(head[0])\n",
        "        self.conf = nn.ModuleList(head[1])\n",
        "\n",
        "\n",
        "    def forward(self, x, phase=''):\n",
        "        \"\"\"Applies network layers and ops on input image(s) x.\n",
        "        Args:\n",
        "            x: input image or batch of images. Shape: [batch,3*batch,300,300].\n",
        "        Return:\n",
        "            Depending on phase:\n",
        "            test:\n",
        "                Variable(tensor) of output class label predictions,\n",
        "                confidence score, and corresponding location predictions for\n",
        "                each object detected. Shape: [batch,topk,7]\n",
        "            train:\n",
        "                list of concat outputs from:\n",
        "                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n",
        "                    2: localization layers, Shape: [batch,num_priors*4]\n",
        "                    3: priorbox layers, Shape: [2,num_priors*4]\n",
        "        \"\"\"\n",
        "        sources = list()\n",
        "        loc = list()\n",
        "        conf = list()\n",
        "\n",
        "        # apply vgg up to conv4_3 relu\n",
        "        for k in range(23):\n",
        "            x = self.base[k](x)\n",
        "\n",
        "        s1 = self.reduce(x)\n",
        "\n",
        "        # apply vgg up to fc7\n",
        "        for k in range(23, len(self.base)):\n",
        "            x = self.base[k](x)\n",
        "        s2 = self.up_reduce(x)\n",
        "        s2 = F.upsample(s2, scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        s = torch.cat((s1,s2),1) # ERROR HERE\n",
        "\n",
        "        ss = self.rfb(s)\n",
        "        sources.append(ss)\n",
        "\n",
        "        # apply extra layers and cache source layer outputs\n",
        "        for k, v in enumerate(self.extras):\n",
        "            x = v(x)\n",
        "            if k < self.indicator or k%2 ==0:\n",
        "                sources.append(x)\n",
        "\n",
        "        # apply multibox head to source layers\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "\n",
        "        #print([o.size() for o in loc])\n",
        "\n",
        "\n",
        "        # location predictions\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
        "        loc = loc.view(loc.size(0), -1, 4)\n",
        "\n",
        "        # class confidence scores\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
        "\n",
        "        if phase == 'eval' or phase == 'test':\n",
        "            conf = F.softmax(conf.view(-1, self.num_classes), dim=-1)\n",
        "        else:\n",
        "            conf = conf.view(conf.size(0), -1, self.num_classes)\n",
        "\n",
        "        return (loc, conf)\n",
        "\n",
        "    def load_model(self, model_file):\n",
        "        print('Loading model weights from', model_file)\n",
        "        self.load_state_dict(torch.load(model_file))\n",
        "        print('Done.')\n",
        "\n",
        "    def load_backbone(self, model_file):\n",
        "        print('Loading backbone/base model weights from', model_file)\n",
        "        self.base.load_state_dict(torch.load(model_file))\n",
        "        print('Done.')\n",
        "\n",
        "    def init_weights(self, base=False, rfb=True, extras=True, loc=True, conf=True, reduce=True, up_reduce=True):\n",
        "\n",
        "        def weights_init(m):\n",
        "            for key in m.state_dict():\n",
        "                if key.split('.')[-1] == 'weight':\n",
        "                    if 'conv' in key:\n",
        "                        init.kaiming_normal_(m.state_dict()[key], mode='fan_out')\n",
        "                    if 'bn' in key:\n",
        "                        m.state_dict()[key][...] = 1\n",
        "                elif key.split('.')[-1] == 'bias':\n",
        "                    m.state_dict()[key][...] = 0\n",
        "\n",
        "        print('Initializing weights...')\n",
        "        print('Init: base: {}, rfb: {}, extras: {}, loc: {}, conf: {}'.format(base, rfb, extras, loc, conf))\n",
        "        if base: self.base.apply(weights_init)\n",
        "        if reduce: self.reduce.apply(weights_init)\n",
        "        if up_reduce: self.up_reduce.apply(weights_init)\n",
        "        if rfb: self.rfb.apply(weights_init)\n",
        "        if extras: self.extras.apply(weights_init)\n",
        "        if loc: self.loc.apply(weights_init)\n",
        "        if conf: self.conf.apply(weights_init)\n",
        "        print('Done.')\n",
        "\n",
        "# This function is derived from torchvision VGG make_layers()\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
        "def vgg(cfg, i, batch_norm=False):\n",
        "    layers = []\n",
        "    in_channels = i\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif v == 'C':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            if batch_norm:\n",
        "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
        "            else:\n",
        "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "    layers += [pool5, conv6,\n",
        "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
        "    return layers\n",
        "\n",
        "base = {\n",
        "    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
        "            512, 512, 512],\n",
        "    '512': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
        "            512, 512, 512],\n",
        "}\n",
        "\n",
        "\n",
        "def add_extras(size, cfg, i, batch_norm=False):\n",
        "    # Extra layers added to VGG for feature scaling\n",
        "    layers = []\n",
        "    in_channels = i\n",
        "    flag = False\n",
        "    for k, v in enumerate(cfg):\n",
        "        if in_channels != 'S':\n",
        "            if v == 'S':\n",
        "                if in_channels == 256:\n",
        "                    layers += [BasicRFB_c(in_channels, cfg[k+1], stride=2, scale = 1.0)]\n",
        "                else:\n",
        "                    layers += [BasicRFB(in_channels, cfg[k+1], stride=2, scale = 1.0)]\n",
        "            else:\n",
        "                layers += [BasicRFB(in_channels, v, scale = 1.0)]\n",
        "        in_channels = v\n",
        "    if size == 512:\n",
        "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
        "        layers += [BasicConv(128,256,kernel_size=4,stride=1,padding=1)]\n",
        "    elif size ==300:\n",
        "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
        "        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n",
        "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
        "        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n",
        "    else:\n",
        "        print(f\"GAK PASS : size = {size} add_extras: Error: Sorry only RFB300 and RFB512 are supported!\")\n",
        "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
        "        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n",
        "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
        "    return layers\n",
        "\n",
        "extras = {\n",
        "    '300': [1024, 'S', 512, 'S', 256],\n",
        "    '512': [1024, 'S', 512, 'S', 256, 'S', 256,'S',256],\n",
        "}\n",
        "\n",
        "\n",
        "def multibox(size, vgg, extra_layers, cfg, num_classes):\n",
        "    loc_layers = []\n",
        "    conf_layers = []\n",
        "    vgg_source = [-2]\n",
        "    for k, v in enumerate(vgg_source):\n",
        "        if k == 0:\n",
        "            loc_layers += [nn.Conv2d(512,\n",
        "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
        "            conf_layers +=[nn.Conv2d(512,\n",
        "                                 cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
        "        else:\n",
        "            loc_layers += [nn.Conv2d(vgg[v].out_channels,\n",
        "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
        "            conf_layers += [nn.Conv2d(vgg[v].out_channels,\n",
        "                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
        "    i = 1\n",
        "    indicator = 0\n",
        "    if size == 300:\n",
        "        indicator = 3\n",
        "    elif size == 512:\n",
        "        indicator = 5\n",
        "    else:\n",
        "        indicator = 5\n",
        "        print(f\"GAK PASS indicator {indicator}: multibox: Error: Sorry only RFB300 and RFB512 are supported!\")\n",
        "        # return\n",
        "\n",
        "    for k, v in enumerate(extra_layers):\n",
        "        if k < indicator or k%2== 0:\n",
        "            loc_layers += [nn.Conv2d(v.out_channels, cfg[i]\n",
        "                                 * 4, kernel_size=3, padding=1)]\n",
        "            conf_layers += [nn.Conv2d(v.out_channels, cfg[i]\n",
        "                                  * num_classes, kernel_size=3, padding=1)]\n",
        "            i +=1\n",
        "    return vgg, extra_layers, (loc_layers, conf_layers)\n",
        "\n",
        "mbox = {\n",
        "    '300': [6, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n",
        "    '512': [6, 6, 6, 6, 6, 4, 4],\n",
        "}\n",
        "\n",
        "\n",
        "def build_net(img_size=512, num_classes=2):\n",
        "    if img_size not in [300, 512]:\n",
        "        print(f\"GAK PASS : size = {img_size} Error: only RFBE300 and RFBE512 are supported!\")\n",
        "        # return\n",
        "\n",
        "    if img_size not in base.keys():\n",
        "      base[str(img_size)] = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
        "            512, 512, 512]\n",
        "\n",
        "    if img_size not in extras.keys():\n",
        "      extras[str(img_size)] = [1024, 'S', 512, 'S', 256, 'S', 256,'S',256]\n",
        "\n",
        "    if img_size not in mbox.keys():\n",
        "      mbox[str(img_size)] = [6, 6, 6, 6, 6, 4, 4]\n",
        "\n",
        "    # print(f\"base = {base}\")\n",
        "    # print()\n",
        "    # print(f\"extras = {extras}\")\n",
        "    # print('\\nNetwork: RFBE_VGG. Image size: %d. Number of classes: %d' % (img_size, num_classes))\n",
        "\n",
        "    return RFBNet(img_size, *multibox(img_size, vgg(base[str(img_size)], 3),\n",
        "                                add_extras(img_size, extras[str(img_size)], 1024),\n",
        "                                mbox[str(img_size)], num_classes), num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "qISU8AImoz8C"
      },
      "outputs": [],
      "source": [
        "def jaccard(box_a, box_b):\n",
        "    \"\"\"Compute the jaccard overlap (IoU) between two sets of boxes.\n",
        "    The jaccard overlap is simply the intersection over union of two boxes.\n",
        "\n",
        "    Args:\n",
        "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects, 4].\n",
        "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors, 4].\n",
        "\n",
        "    Return:\n",
        "        IoU: (tensor) Shape: [num_objects, num_priors]\n",
        "    \"\"\"\n",
        "    # Intersection box: find the max of the min coords and the min of the max coords\n",
        "    inter_xmin = torch.max(box_a[:, 0].unsqueeze(1), box_b[:, 0])\n",
        "    inter_ymin = torch.max(box_a[:, 1].unsqueeze(1), box_b[:, 1])\n",
        "    inter_xmax = torch.min(box_a[:, 2].unsqueeze(1), box_b[:, 2])\n",
        "    inter_ymax = torch.min(box_a[:, 3].unsqueeze(1), box_b[:, 3])\n",
        "\n",
        "    # Compute the width and height of the intersection box\n",
        "    inter_w = torch.clamp(inter_xmax - inter_xmin, min=0)\n",
        "    inter_h = torch.clamp(inter_ymax - inter_ymin, min=0)\n",
        "\n",
        "    # Intersection area\n",
        "    inter_area = inter_w * inter_h\n",
        "\n",
        "    # Area of both the boxes\n",
        "    area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])  # [num_objects]\n",
        "    area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])  # [num_priors]\n",
        "\n",
        "    # Union area\n",
        "    union_area = area_a.unsqueeze(1) + area_b - inter_area\n",
        "\n",
        "    # IoU calculation\n",
        "    return inter_area / union_area  # [num_objects, num_priors]\n",
        "\n",
        "def point_form(boxes):\n",
        "    \"\"\" Convert prior boxes from (cx, cy, w, h) to (xmin, ymin, xmax, ymax).\n",
        "    Args:\n",
        "        boxes: (tensor) prior boxes in center-offset form (cx, cy, w, h).\n",
        "                Shape: [num_priors, 4]\n",
        "    Return:\n",
        "        boxes: (tensor) Converted boxes in point (corner) form (xmin, ymin, xmax, ymax).\n",
        "                Shape: [num_priors, 4]\n",
        "    \"\"\"\n",
        "    return torch.cat((boxes[:, :2] - boxes[:, 2:] / 2,  # xmin, ymin\n",
        "                      boxes[:, :2] + boxes[:, 2:] / 2), 1)  # xmax, ymax\n",
        "\n",
        "\n",
        "############################################\n",
        "# from lib.utils.box_utils import match, log_sum_exp\n",
        "def log_sum_exp(x):\n",
        "    \"\"\"Utility function for computing log_sum_exp while determining\n",
        "    This will be used to determine unaveraged confidence loss across\n",
        "    all examples in a batch.\n",
        "    Args:\n",
        "        x (Variable(tensor)): conf_preds from conf layers\n",
        "    \"\"\"\n",
        "    x_max = x.data.max()\n",
        "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
        "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
        "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
        "    overlap, encode the bounding boxes, then return the matched indices\n",
        "    corresponding to both confidence and location preds.\n",
        "    Args:\n",
        "        threshold: (float) The overlap threshold used when mathing boxes.\n",
        "        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n",
        "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
        "        variances: (tensor) Variances corresponding to each prior coord,\n",
        "            Shape: [num_priors, 4].\n",
        "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
        "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
        "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
        "        idx: (int) current batch index\n",
        "    Return:\n",
        "        The matched indices corresponding to 1)location and 2)confidence preds.\n",
        "    \"\"\"\n",
        "    # jaccard index\n",
        "    overlaps = jaccard(\n",
        "        truths,\n",
        "        point_form(priors)\n",
        "    )\n",
        "    # (Bipartite Matching)\n",
        "    # [1,num_objects] best prior for each ground truth\n",
        "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
        "    # [1,num_priors] best ground truth for each prior\n",
        "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
        "    best_truth_idx.squeeze_(0)\n",
        "    best_truth_overlap.squeeze_(0)\n",
        "    best_prior_idx.squeeze_(1)\n",
        "    best_prior_overlap.squeeze_(1)\n",
        "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
        "    # TODO refactor: index  best_prior_idx with long tensor\n",
        "    # ensure every gt matches with its prior of max overlap\n",
        "    for j in range(best_prior_idx.size(0)):\n",
        "        best_truth_idx[best_prior_idx[j]] = j\n",
        "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
        "    conf = labels[best_truth_idx]          # Shape: [num_priors]\n",
        "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
        "    loc = encode(matches, priors, variances)\n",
        "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
        "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
        "\n",
        "def encode(matched, priors, variances):\n",
        "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
        "    we have matched (based on jaccard overlap) with the prior boxes.\n",
        "    Args:\n",
        "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
        "            Shape: [num_priors, 4].\n",
        "        priors: (tensor) Prior boxes in center-offset form\n",
        "            Shape: [num_priors,4].\n",
        "        variances: (list[float]) Variances of priorboxes\n",
        "    Return:\n",
        "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
        "    \"\"\"\n",
        "\n",
        "    # dist b/t match center and prior's center\n",
        "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
        "    # encode variance\n",
        "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
        "    # match wh / prior wh\n",
        "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
        "    g_wh = torch.log(g_wh) / variances[1]\n",
        "\n",
        "    # return target for smooth_l1_loss\n",
        "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
        "\n",
        "\n",
        "# Updated voc_eval function\n",
        "def voc_eval(detections, annotations, cid=None, ovthresh=0.5):\n",
        "    \"\"\"\n",
        "    Top-level function that does the PASCAL VOC evaluation.\n",
        "    Args:\n",
        "        detections: dict of detections for a specific class (keyed on image IDs), where\n",
        "                    each entry is a list of detection bounding boxes and confidences.\n",
        "        annotations: dict of ground truth annotations keyed on image IDs, where each entry\n",
        "                    is a list of ground truth boxes with their respective class IDs.\n",
        "        cid: Class ID to evaluate.\n",
        "        ovthresh: Overlap threshold for considering a detection as True Positive.\n",
        "    Returns:\n",
        "        rec: Recall for the class\n",
        "        prec: Precision for the class\n",
        "        ap: Average Precision (AP) for the class\n",
        "    \"\"\"\n",
        "    use_07_metric = False  # Use 11-point interpolation for PASCAL VOC 2007\n",
        "\n",
        "    # Extract gt objects for this class\n",
        "    class_recs = {}\n",
        "    npos = 0\n",
        "    for imagename in annotations:\n",
        "        # For each image, list of objects: [[x1, y1, x2, y2, class_id], ...]\n",
        "        R = [obj[:4] for obj in annotations[imagename] if int(obj[-1]) == cid]\n",
        "        bbox = np.array(R)\n",
        "        difficult = np.array([0] * len(R)).astype(np.bool)  # No difficult examples\n",
        "        det = [False] * len(R)\n",
        "        npos += sum(~difficult)\n",
        "        class_recs[imagename] = {'bbox': bbox, 'difficult': difficult, 'det': det}\n",
        "\n",
        "    # Detections for this class\n",
        "    image_ids = []\n",
        "    confidence = []\n",
        "    BB = []\n",
        "    class_dict = detections.get(cid, {})\n",
        "    for image_file in class_dict:\n",
        "        dets = class_dict[image_file]\n",
        "        for k in range(dets.shape[0]):\n",
        "            image_ids.append(image_file)\n",
        "            BB.append(dets[k, 0:4])\n",
        "            confidence.append(dets[k, -1])\n",
        "\n",
        "    BB = np.array(BB)\n",
        "    confidence = np.array(confidence)\n",
        "\n",
        "    nd = len(image_ids)\n",
        "    tp = np.zeros(nd)\n",
        "    fp = np.zeros(nd)\n",
        "    if npos == 0:  # Handle case where there are no ground truths for this class\n",
        "        return np.zeros(0), np.zeros(0), 0.0\n",
        "    if BB.shape[0] > 0:\n",
        "        # Sort by confidence\n",
        "        sorted_ind = np.argsort(-confidence)\n",
        "        BB = BB[sorted_ind, :]\n",
        "        image_ids = [image_ids[x] for x in sorted_ind]\n",
        "\n",
        "        # Go through detections and mark TPs and FPs\n",
        "        for d in range(nd):\n",
        "            # Check if the image ID exists in the ground truth for this class\n",
        "            if image_ids[d] not in class_recs:\n",
        "                fp[d] = 1.  # Count it as a false positive since no ground truth exists for this image\n",
        "                continue\n",
        "\n",
        "            R = class_recs[image_ids[d]]\n",
        "            bb = BB[d, :].astype(float)\n",
        "            ovmax = -np.inf\n",
        "            BBGT = R['bbox'].astype(float)\n",
        "\n",
        "            if BBGT.size > 0:\n",
        "                # Compute IoU overlaps\n",
        "                ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
        "                iymin = np.maximum(BBGT[:, 1], bb[1])\n",
        "                ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
        "                iymax = np.minimum(BBGT[:, 3], bb[3])\n",
        "                iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
        "                ih = np.maximum(iymax - iymin + 1., 0.)\n",
        "                inters = iw * ih\n",
        "                uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n",
        "                       (BBGT[:, 2] - BBGT[:, 0] + 1.) * (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
        "                overlaps = inters / uni\n",
        "                ovmax = np.max(overlaps)\n",
        "                jmax = np.argmax(overlaps)\n",
        "\n",
        "            if ovmax > ovthresh:\n",
        "                if not R['difficult'][jmax]:\n",
        "                    if not R['det'][jmax]:\n",
        "                        tp[d] = 1.\n",
        "                        R['det'][jmax] = 1\n",
        "                    else:\n",
        "                        fp[d] = 1.\n",
        "            else:\n",
        "                fp[d] = 1.\n",
        "\n",
        "    # Compute precision-recall\n",
        "    fp = np.cumsum(fp)\n",
        "    tp = np.cumsum(tp)\n",
        "    rec = tp / float(npos)\n",
        "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
        "    ap = voc_ap(rec, prec, use_07_metric)\n",
        "\n",
        "    return rec, prec, ap\n",
        "\n",
        "\n",
        "# Helper function to compute AP\n",
        "def voc_ap(rec, prec, use_07_metric=False):\n",
        "    \"\"\" Compute the average precision (AP) using either the 07 or 12/10 metric \"\"\"\n",
        "    if use_07_metric:\n",
        "        ap = 0.\n",
        "        for t in np.arange(0., 1.1, 0.1):\n",
        "            if np.sum(rec >= t) == 0:\n",
        "                p = 0\n",
        "            else:\n",
        "                p = np.max(prec[rec >= t])\n",
        "            ap += p / 11.\n",
        "    else:\n",
        "        mrec = np.concatenate(([0.], rec, [1.]))\n",
        "        mpre = np.concatenate(([0.], prec, [0.]))\n",
        "\n",
        "        for i in range(mpre.size - 1, 0, -1):\n",
        "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
        "\n",
        "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
        "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
        "\n",
        "    return ap\n",
        "\n",
        "# Evaluation function for all classes\n",
        "def evaluate_detections(detections_dict, annotations, classes=None, iou_thresh=0.5):\n",
        "    aps = []\n",
        "    print('--------------------------------')\n",
        "    print('Class : \\t AP (100)')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    for cid, classname in enumerate(classes):\n",
        "        if classname == '__background__':\n",
        "            continue\n",
        "        # print(f\"$$$$ voc_eval detections_dict= {detections_dict} <> annotations = {annotations} <>  cid = {cid} <> iou_thresh = {iou_thresh}\" )\n",
        "        rec, prec, ap = voc_eval(detections_dict, annotations, cid, iou_thresh)\n",
        "        # print(f\"@@@@ voc_eval\")\n",
        "        aps.append(ap)\n",
        "        print(f'{classname} : \\t{100 * ap:.2f}')\n",
        "\n",
        "    mAP = 100 * np.mean(aps)\n",
        "    print('--------------------------------')\n",
        "    print(f'mAP: \\t{mAP:.2f}')\n",
        "    print('--------------------------------')\n",
        "\n",
        "    return aps, mAP\n",
        "\n",
        "############################################\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSD Weighted Loss Function\n",
        "    Compute Targets:\n",
        "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
        "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
        "           (default threshold: 0.5).\n",
        "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
        "           truth boxes and their matched  'priorboxes'.\n",
        "        3) Hard negative mining to filter the excessive number of negative examples\n",
        "           that comes with using a large number of default bounding boxes.\n",
        "           (default negative:positive ratio 3:1)\n",
        "    Objective Loss:\n",
        "        L(x,c,l,g) = (Lconf(x, c) + Î±Lloc(x,l,g)) / N\n",
        "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
        "        weighted by Î± which is set to 1 by cross val.\n",
        "        Args:\n",
        "            c: class confidences,\n",
        "            l: predicted boxes,\n",
        "            g: ground truth boxes\n",
        "            N: number of matched default boxes\n",
        "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(self, num_classes, overlap_thresh, negpos_ratio):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.threshold = overlap_thresh\n",
        "        self.negpos_ratio = negpos_ratio\n",
        "        self.variance = [0.1, 0.2]\n",
        "\n",
        "    # TODO: update losses: https://pytorch.org/docs/stable/nn.html?highlight=f%20cross_entropy#torch.nn.functional.cross_entropy\n",
        "    def forward(self, predictions, priors, targets):\n",
        "        \"\"\"Multibox Loss\n",
        "        Args:\n",
        "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
        "            and prior boxes from SSD net.\n",
        "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
        "                loc shape: torch.size(batch_size,num_priors,4)\n",
        "                priors shape: torch.size(num_priors,4)\n",
        "\n",
        "            ground_truth (tensor): Ground truth boxes and labels for a batch,\n",
        "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
        "        \"\"\"\n",
        "\n",
        "        # print(\"INSIDE MultiBoxLoss forward()\")\n",
        "        # print(f\"type(predictions) = {type(predictions)}\")       #  <class 'tuple'>\n",
        "        # print(f\"len(predictions) = {len(predictions)}\")         # 2\n",
        "        # print(f\"predictions[0].shape = {predictions[0].shape}\") # torch.Size([1, 40376, 4])\n",
        "        # # print(f\"predictions = {predictions}\")\n",
        "        # print()\n",
        "        # print(f\"type(priors) = {type(priors)}\")                # <class 'torch.Tensor'>\n",
        "        # print(f\"priors.shape = {priors.shape}\")                # torch.Size([32756, 4])\n",
        "        # # print(f\"priors = {priors}\")\n",
        "        # print()\n",
        "        # print(f\"type(targets) = {type(targets)}\")              # <class 'list'>\n",
        "        # print(f\"len(targets) = {len(targets)}\")                # 1\n",
        "        # print(f\"targets[0].shape = {targets[0].shape}\")        # torch.Size([4, 5])\n",
        "        # print(f\"targets = {targets}\")\n",
        "\n",
        "        loc_data, conf_data = predictions\n",
        "        priors = priors\n",
        "        num = loc_data.size(0)\n",
        "        num_priors = (priors.size(0))\n",
        "\n",
        "        # match priors (default boxes) and ground truth boxes\n",
        "        loc_t = torch.Tensor(num, num_priors, 4)\n",
        "        conf_t = torch.LongTensor(num, num_priors)\n",
        "        for idx in range(num):\n",
        "            truths = targets[idx][:,:-1].data\n",
        "            labels = targets[idx][:,-1].data\n",
        "            defaults = priors.data\n",
        "            match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx)\n",
        "\n",
        "\n",
        "        ## GAK WAS HERE\n",
        "        ## 01\n",
        "        # loc_t = loc_t.cuda()\n",
        "        # conf_t = conf_t.cuda()\n",
        "        ## / 01\n",
        "        ## 02\n",
        "        if torch.cuda.is_available():\n",
        "            device = torch.device(\"cuda\")\n",
        "        else:\n",
        "            device = torch.device(\"cpu\")\n",
        "        loc_t = loc_t.to(device)\n",
        "        conf_t = conf_t.to(device)\n",
        "        ## / 02\n",
        "\n",
        "        pos = conf_t > 0\n",
        "\n",
        "        # Localization Loss (Smooth L1)\n",
        "        # Shape: [batch,num_priors,4]\n",
        "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
        "        loc_p = loc_data[pos_idx].view(-1,4)\n",
        "        loc_t = loc_t[pos_idx].view(-1,4)\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "\n",
        "        # Compute max conf across batch for hard negative mining\n",
        "        batch_conf = conf_data.view(-1, self.num_classes)\n",
        "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1,1))\n",
        "\n",
        "        # Hard Negative Mining\n",
        "        loss_c[pos.view(-1,1)] = 0 # filter out pos boxes for now\n",
        "        loss_c = loss_c.view(num, -1)\n",
        "        _,loss_idx = loss_c.sort(1, descending=True)\n",
        "        _,idx_rank = loss_idx.sort(1)\n",
        "        num_pos = pos.long().sum(1,keepdim=True)\n",
        "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
        "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
        "\n",
        "        # Confidence Loss Including Positive and Negative Examples\n",
        "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
        "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n",
        "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
        "        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')\n",
        "\n",
        "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + Î±Lloc(x,l,g)) / N\n",
        "\n",
        "        N = max(num_pos.sum().float(), 1.)\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "\n",
        "        return loss_l, loss_c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "bTnTwCa3oz8D"
      },
      "outputs": [],
      "source": [
        "class Anchors_W(object):\n",
        "    \"\"\"Compute anchor box coordinates in center-offset form for each source feature map.\n",
        "    \"\"\"\n",
        "    def __init__(self, cfg):\n",
        "        self.image_size = cfg['min_dim']                     # 512\n",
        "        self.variance = cfg['variance'] or [0.1, 0.2]        # [0.1, 0.2]\n",
        "        self.feature_maps = cfg['feature_maps']              # [64, 32, 16, 8, 4, 2, 1]\n",
        "        self.min_sizes = cfg['min_sizes']                    # [20.48, 51.2, 133.12, 215.04, 296.96, 378.88, 460.8]\n",
        "        self.max_sizes = cfg['max_sizes']                    # [51.2, 133.12, 215.04, 296.96, 378.88, 460.8, 542.72]\n",
        "        self.steps = cfg['steps']                            # [8, 16, 32, 64, 128, 256, 512]\n",
        "        self.aspect_ratios = cfg['aspect_ratios']            # [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
        "        self.clip = cfg['clip']                              # True\n",
        "\n",
        "        for v in self.variance:\n",
        "            if v <= 0:\n",
        "                raise ValueError('Variances must be greater than 0')\n",
        "\n",
        "    def forward(self):\n",
        "        \"\"\"Generate the anchor boxes for each feature map.\"\"\"\n",
        "        anchor_boxes = []\n",
        "\n",
        "        for k, f in enumerate(self.feature_maps):  # Iterate through feature maps\n",
        "            for i, j in product(range(f), repeat=2):  # Iterate through grid positions\n",
        "                f_k = self.image_size / self.steps[k]  # e.g., 512 / [8, 16, 32, ...] -> [64.0, 32.0, ...]\n",
        "                cx = (j + 0.5) / f_k  # Center x\n",
        "                cy = (i + 0.5) / f_k  # Center y\n",
        "\n",
        "                # Create anchor for aspect ratio 1\n",
        "                s_k = self.min_sizes[k] / self.image_size  # Scale relative to image size\n",
        "                anchor_boxes += [cx, cy, s_k, s_k]\n",
        "\n",
        "                # Create an additional anchor box for aspect ratio 1 with larger size\n",
        "                s_k_prime = sqrt(s_k * (self.max_sizes[k] / self.image_size))\n",
        "                anchor_boxes += [cx, cy, s_k_prime, s_k_prime]\n",
        "\n",
        "                # Create anchors for other aspect ratios\n",
        "                for ar in self.aspect_ratios[k]:\n",
        "                    anchor_boxes += [cx, cy, s_k * sqrt(ar), s_k / sqrt(ar)]\n",
        "                    anchor_boxes += [cx, cy, s_k / sqrt(ar), s_k * sqrt(ar)]\n",
        "\n",
        "        # Convert the list of anchor boxes into a tensor\n",
        "        output = torch.Tensor(anchor_boxes).view(-1, 4)\n",
        "\n",
        "        # Clip the boxes to ensure coordinates are within [0, 1] range\n",
        "        if self.clip:\n",
        "            output.clamp_(max=1., min=0.)\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "13pf0nDooz8D"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_data, batch_size, optimizer, multibox_loss, priors, device):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The RFBNet model.\n",
        "    - train_data: Training dataset.\n",
        "    - batch_size: Batch size for training.\n",
        "    - optimizer: Optimizer.\n",
        "    - multibox_loss: Loss function for object detection.\n",
        "    - priors: Priors from anchor boxes.\n",
        "    - device: Device to train on (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "    - Total training loss (localization and confidence).\n",
        "    \"\"\"\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loc_loss = 0.0\n",
        "    total_conf_loss = 0.0\n",
        "\n",
        "    for i in range(0, len(train_data), batch_size):\n",
        "        # Gather batch of images, bounding boxes, and class labels from training data\n",
        "        batch_images = [train_data[j][0] for j in range(i, min(i + batch_size, len(train_data)))]\n",
        "        batch_boxes = [train_data[j][1] for j in range(i, min(i + batch_size, len(train_data)))]\n",
        "        batch_labels = [train_data[j][2] for j in range(i, min(i + batch_size, len(train_data)))]\n",
        "\n",
        "        # Stack the batch of images into a single tensor\n",
        "        batch_images = torch.stack(batch_images).to(device)\n",
        "\n",
        "        # Forward pass through the model to get predictions (loc_preds, conf_preds)\n",
        "        loc_preds, conf_preds = model(batch_images)\n",
        "\n",
        "        # Prepare ground truth targets (bounding boxes + labels)\n",
        "        targets = [torch.cat([boxes, labels.unsqueeze(1)], dim=1).to(device) for boxes, labels in zip(batch_boxes, batch_labels)]\n",
        "\n",
        "        # Compute the loss\n",
        "        loss_l, loss_c = multibox_loss((loc_preds, conf_preds), priors, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = loss_l + loss_c\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loc_loss += loss_l.item()\n",
        "        total_conf_loss += loss_c.item()\n",
        "\n",
        "    return total_loc_loss / len(train_data), total_conf_loss / len(train_data)\n",
        "\n",
        "\n",
        "def validate_epoch(model, val_data, batch_size, multibox_loss, priors, device, num_classes):\n",
        "    \"\"\"\n",
        "    Validates the model for one epoch and calculates mAP.\n",
        "\n",
        "    Parameters:\n",
        "    - model: The RFBNet model.\n",
        "    - val_data: Validation dataset.\n",
        "    - batch_size: Batch size for validation.\n",
        "    - multibox_loss: Loss function for object detection.\n",
        "    - priors: Priors from anchor boxes.\n",
        "    - device: Device to validate on (CPU or GPU).\n",
        "    - num_classes: Number of classes.\n",
        "\n",
        "    Returns:\n",
        "    - Average validation loss (localization and confidence).\n",
        "    - Detections and annotations for calculating mAP.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    val_loc_loss = 0.0\n",
        "    val_conf_loss = 0.0\n",
        "\n",
        "    detections_dict = {cid: {} for cid in range(num_classes)}  # Dictionary to store detections\n",
        "    annotations = {}\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation during validation\n",
        "        for i in range(0, len(val_data), batch_size):\n",
        "            # Gather batch of images, bounding boxes, and class labels from validation data\n",
        "            val_images = [val_data[j][0] for j in range(i, min(i + batch_size, len(val_data)))]\n",
        "            val_boxes = [val_data[j][1] for j in range(i, min(i + batch_size, len(val_data)))]\n",
        "            val_labels = [val_data[j][2] for j in range(i, min(i + batch_size, len(val_data)))]\n",
        "\n",
        "            # Stack the batch of validation images into a single tensor\n",
        "            val_images = torch.stack(val_images).to(device)\n",
        "\n",
        "            # Forward pass through the model to get predictions\n",
        "            val_loc_preds, val_conf_preds = model(val_images)\n",
        "\n",
        "            # Prepare ground truth targets for validation\n",
        "            val_targets = [torch.cat([boxes, labels.unsqueeze(1)], dim=1).to(device) for boxes, labels in zip(val_boxes, val_labels)]\n",
        "\n",
        "            # Compute validation loss\n",
        "            val_loss_l, val_loss_c = multibox_loss((val_loc_preds, val_conf_preds), priors, val_targets)\n",
        "            val_loc_loss += val_loss_l.item()\n",
        "            val_conf_loss += val_loss_c.item()\n",
        "\n",
        "            # Store predictions for evaluation later\n",
        "            for idx, img_id in enumerate(range(i, min(i + batch_size, len(val_data)))):\n",
        "                decoded_boxes = val_loc_preds[idx].cpu().detach().numpy()\n",
        "                confidences = val_conf_preds[idx].cpu().detach().numpy()\n",
        "                for cid in range(1, num_classes):  # Skipping background class (cid=0)\n",
        "                    class_mask = confidences[:, cid] > 0.5  # Example threshold\n",
        "                    if class_mask.sum() > 0:\n",
        "                        detections_dict[cid][img_id] = np.hstack([decoded_boxes[class_mask], confidences[class_mask, cid].reshape(-1, 1)])\n",
        "\n",
        "            # Store ground truth annotations for evaluation\n",
        "            annotations[img_id] = [val_data[j][1].cpu().numpy().tolist() for j in range(i, min(i + batch_size, len(val_data)))]\n",
        "\n",
        "    avg_val_loc_loss = val_loc_loss / len(val_data)\n",
        "    avg_val_conf_loss = val_conf_loss / len(val_data)\n",
        "\n",
        "    return avg_val_loc_loss, avg_val_conf_loss, detections_dict, annotations\n",
        "\n",
        "\n",
        "def train_and_validate_rfbnet(train_data, val_data, num_epochs, batch_size, anchorbox_params, priors, device, fold_idx=0):\n",
        "    \"\"\"\n",
        "    Train and validate the RFBNet model for object detection.\n",
        "\n",
        "    Parameters:\n",
        "    - train_data: The training dataset.\n",
        "    - val_data: The validation dataset.\n",
        "    - num_epochs: Number of epochs to train.\n",
        "    - batch_size: The batch size.\n",
        "    - priors: The anchor boxes used in RFBNet.\n",
        "    - device: Device to run the training/validation (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "    - None (Prints out losses and mAP after each epoch)\n",
        "    \"\"\"\n",
        "    img_size = 512\n",
        "    num_classes = 5\n",
        "    anchor_cfg = anchorbox_params[\"RFB_512\"]\n",
        "    anchors_w = Anchors_W(anchor_cfg)\n",
        "    priors = anchors_w.forward()\n",
        "\n",
        "    # Instantiate the RFBNet model\n",
        "    this_rfb = RFBNet(img_size, *multibox(img_size, vgg(base[str(img_size)], 3),\n",
        "                                  add_extras(img_size, extras[str(img_size)], 1024),\n",
        "                                  mbox[str(img_size)], num_classes), num_classes)\n",
        "\n",
        "    # Set model to training mode\n",
        "    this_rfb.train()\n",
        "\n",
        "    # Instantiate the MultiBoxLoss\n",
        "    multibox_loss = MultiBoxLoss(num_classes=num_classes, overlap_thresh=0.5, negpos_ratio=3)\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adam(this_rfb.parameters(), lr=0.001)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "\n",
        "    # Move model and priors to the correct device\n",
        "    this_rfb.to(device)\n",
        "    priors = priors.to(device)\n",
        "\n",
        "    # Initialize variables to track the best validation loss and model\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        avg_train_loc_loss, avg_train_conf_loss = train_epoch(this_rfb, train_data, batch_size, optimizer, multibox_loss, priors, device)\n",
        "        print(f\"Epoch {epoch+1}: Train Loc Loss: {avg_train_loc_loss}, Train Conf Loss: {avg_train_conf_loss}\")\n",
        "\n",
        "        # Validation\n",
        "        avg_val_loc_loss, avg_val_conf_loss, detections, annotations = validate_epoch(this_rfb, val_data, batch_size, multibox_loss, priors, device, num_classes)\n",
        "        print(f\"Epoch {epoch+1}: Val Loc Loss: {avg_val_loc_loss}, Val Conf Loss: {avg_val_conf_loss}\")\n",
        "\n",
        "        # Evaluate mAP\n",
        "        aps, mAP = evaluate_detections(detections, annotations, classes=['gun', 'knife', 'plier', 'scissor', 'wrench'])\n",
        "        print(f\"Epoch {epoch+1}: mAP: {mAP}\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # Total validation loss\n",
        "        total_val_loss = avg_val_loc_loss + avg_val_conf_loss\n",
        "\n",
        "        # Check if current validation loss is the best so far\n",
        "        if total_val_loss < best_val_loss:\n",
        "            best_val_loss = total_val_loss\n",
        "            best_epoch = epoch + 1  # Epochs are zero-indexed in code, so add 1\n",
        "            # Save the model state\n",
        "            best_model_state = copy.deepcopy(this_rfb.state_dict())\n",
        "            print(f\"New best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")\n",
        "\n",
        "        # Step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "    # At the end of training, save the best model for this fold\n",
        "    if best_model_state is not None:\n",
        "        model_save_path = f'best_model_fold_{fold_idx+1}.pth'\n",
        "        torch.save(best_model_state, model_save_path)\n",
        "        print(f\"Best model for Fold {fold_idx+1} saved at {model_save_path} with validation loss: {best_val_loss:.4f} from Epoch {best_epoch}\")\n",
        "\n",
        "\n",
        "    # Clean up memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ampqkX26oz8D"
      },
      "source": [
        "## 20 K-fold Cross Validatation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {
        "id": "NfFP9MfZoz8D"
      },
      "outputs": [],
      "source": [
        "# Collection of Ground Truth and Predictions during validation\n",
        "def collect_predictions_and_ground_truths_with_speed(model, val_loader, device, num_images_to_print=3):\n",
        "    \"\"\"\n",
        "    Collect predictions, ground truths, and calculate inference speed during validation.\n",
        "\n",
        "    Parameters:\n",
        "    - model (torch.nn.Module): Trained model.\n",
        "    - val_loader (DataLoader): DataLoader for validation data.\n",
        "    - device (torch.device): Device to run inference on.\n",
        "    - num_images_to_print (int): Number of images to print outputs for (after NMS).\n",
        "\n",
        "    Returns:\n",
        "    - all_predictions: List of dictionaries containing optimized predictions.\n",
        "    - all_ground_truths: List of dictionaries containing optimized ground truth labels and bounding boxes.\n",
        "    - avg_inference_time: Average inference time per image (in seconds).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "    total_inference_time = 0.0\n",
        "    num_images = 0  # Track the total number of images processed\n",
        "    printed_images = 0  # Counter for printed images\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, targets) in enumerate(val_loader):\n",
        "            # Filter out invalid boxes and their corresponding images\n",
        "            images, targets = filter_invalid_images_and_boxes(images, targets)\n",
        "            if not images:  # Skip if no valid images\n",
        "                continue\n",
        "\n",
        "            images = [img.to(device) for img in images]\n",
        "\n",
        "            # Start timing the inference\n",
        "            start_time = time.time()\n",
        "            try:\n",
        "                outputs = model(images)  # Get predictions from the model\n",
        "            except Exception as e:\n",
        "                print(f\"Error during inference: {e}\")\n",
        "                continue\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Calculate time taken for this batch and accumulate\n",
        "            batch_inference_time = end_time - start_time\n",
        "            total_inference_time += batch_inference_time\n",
        "            num_images += len(images)  # Increment by the number of images in the batch\n",
        "\n",
        "            # Store predictions and ground truths, and print the first few images' predictions\n",
        "            for i in range(len(images)):\n",
        "                # Convert predictions to numpy arrays with optimized data types\n",
        "                pred_boxes = outputs[i]['boxes'].cpu().numpy().astype('float32')\n",
        "                pred_scores = outputs[i]['scores'].cpu().numpy().astype('float32')\n",
        "                pred_labels = outputs[i]['labels'].cpu().numpy().astype('int16')\n",
        "\n",
        "                # Store predictions: 'boxes', 'scores', 'labels'\n",
        "                pred_dict = {\n",
        "                    'boxes': pred_boxes,\n",
        "                    'scores': pred_scores,\n",
        "                    'labels': pred_labels,\n",
        "                }\n",
        "                all_predictions.append(pred_dict)\n",
        "\n",
        "                # Convert ground truths to numpy arrays with optimized data types\n",
        "                gt_boxes = targets[i]['boxes'].cpu().numpy().astype('float32')\n",
        "                gt_labels = targets[i]['labels'].cpu().numpy().astype('int16')\n",
        "\n",
        "                # Store ground truths: 'boxes', 'labels'\n",
        "                gt_dict = {\n",
        "                    'boxes': gt_boxes,\n",
        "                    'labels': gt_labels,\n",
        "                }\n",
        "                all_ground_truths.append(gt_dict)\n",
        "\n",
        "                # Print the first `num_images_to_print` images' predictions after NMS\n",
        "                if printed_images < num_images_to_print:\n",
        "                    print(f\"\\nImage {printed_images + 1} (after NMS):\")\n",
        "                    if len(pred_boxes) == 0:\n",
        "                        print(\"No predictions for this image.\")\n",
        "                    else:\n",
        "                        for j in range(len(pred_boxes)):\n",
        "                            print(f\"  Prediction {j + 1}:\")\n",
        "                            print(f\"    Box: {pred_boxes[j]}\")\n",
        "                            print(f\"    Score: {pred_scores[j]:.4f}\")\n",
        "                            print(f\"    Label: {pred_labels[j]}\")\n",
        "                    printed_images += 1\n",
        "\n",
        "    # Calculate the average inference time per image\n",
        "    avg_inference_time = total_inference_time / num_images\n",
        "\n",
        "    return all_predictions, all_ground_truths, avg_inference_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "lEPy58idoz8D"
      },
      "outputs": [],
      "source": [
        "# Function to perform K-Fold cross-validation\n",
        "def cross_validate_model(model_name, dataset, num_folds, num_epochs, **kwargs):\n",
        "    \"\"\"\n",
        "    Performs K-Fold cross-validation on the given model and dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Name of the model ('faster_rcnn', 'stn_cnn', 'stn_rfbnet').\n",
        "    - dataset (torch.utils.data.Dataset): The dataset to perform cross-validation on.\n",
        "    - num_folds (int): Number of folds for cross-validation.\n",
        "    - num_epochs (int): Number of epochs for training in each fold.\n",
        "    - kwargs: Additional parameters for model creation and training functions.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Initialize variables to store metrics\n",
        "    overall_best_loss = float('inf')  # Track the best validation loss across all folds\n",
        "    overall_best_model_state = None   # To store the best model's state_dict\n",
        "    overall_best_fold = None          # Track the fold number for the best model\n",
        "    overall_best_epoch = None         # Track the epoch for the best model\n",
        "    # Initialize a dictionary to store all the fold-specific data\n",
        "    all_fold_data = {}\n",
        "\n",
        "    # Extract the parameters from kwargs\n",
        "    device = kwargs.get('device')\n",
        "    anchor_sizes = kwargs.get('anchor_sizes')\n",
        "    aspect_ratios = kwargs.get('aspect_ratios')\n",
        "    rpn_nms_thresh = kwargs.get('rpn_nms_thresh')\n",
        "    anchorbox_params = kwargs.get('anchorbox_params')\n",
        "    stn_type = kwargs.get('stn_type')\n",
        "    \n",
        "    print(f\"Number of epochs that will run: {num_epochs}\")\n",
        "    print(f\"Number of folds that will run: {num_folds}\")\n",
        "\n",
        "    # Shuffle indices\n",
        "    num_samples = len(dataset)\n",
        "    indices = list(range(num_samples))\n",
        "    random.seed(42)\n",
        "    random.shuffle(indices)\n",
        "\n",
        "    fold_size = num_samples // num_folds\n",
        "    folds = []\n",
        "\n",
        "    # Mapping class indices to class names\n",
        "    class_mapping = {0: 'gun', 1: 'knife', 2: 'plier', 3: 'scissor', 4: 'wrench'}\n",
        "\n",
        "    # Prepare class weights tensor\n",
        "    class_weights = {'gun': 0.065648621975652,\n",
        "                     'knife': 0.35833206161710135,\n",
        "                     'plier': 0.04574451850431081,\n",
        "                     'scissor': 0.3739117164700188,\n",
        "                     'wrench': 0.15636308143291694}\n",
        "\n",
        "    for i in range(num_folds):\n",
        "        start = i * fold_size\n",
        "        if i == num_folds - 1:\n",
        "            end = num_samples  # Last fold takes the remaining samples\n",
        "        else:\n",
        "            end = start + fold_size\n",
        "        fold = indices[start:end]\n",
        "        folds.append(fold)\n",
        "\n",
        "    for fold_idx in range(num_folds):\n",
        "        print(f\"\\nFold {fold_idx + 1}/{num_folds}\")\n",
        "\n",
        "        # Validation indices for this fold\n",
        "        val_indices = folds[fold_idx]\n",
        "\n",
        "        # Training indices are all other indices\n",
        "        train_indices = [idx for i, fold in enumerate(folds) if i != fold_idx for idx in fold]\n",
        "\n",
        "        # Create training and validation datasets\n",
        "        train_subset = Subset(dataset, train_indices)\n",
        "        val_subset = Subset(dataset, val_indices)\n",
        "\n",
        "        # Create DataLoaders\n",
        "        train_loader = DataLoader(train_subset, batch_size=kwargs.get('batch_size', 8), shuffle=True, collate_fn=custom_collate_fn)\n",
        "        val_loader = DataLoader(val_subset, batch_size=kwargs.get('batch_size', 8), shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "\n",
        "        num_classes = len(class_mapping) + 1  # +1 for background\n",
        "        class_weights_tensor = torch.ones(num_classes)  # Initialize with ones for all classes including background\n",
        "\n",
        "        # Map class indices to weights\n",
        "        for class_index, class_name in class_mapping.items():\n",
        "            class_weight = class_weights[class_name]\n",
        "            class_weights_tensor[class_index] = class_weight\n",
        "\n",
        "        class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "        # Create and Train model\n",
        "        if model_name == 'faster_rcnn':\n",
        "\n",
        "            model = create_model_frcnn(num_classes=num_classes, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, rpn_nms_thresh=rpn_nms_thresh)\n",
        "\n",
        "            model.to(device)\n",
        "\n",
        "            # Define optimizer and scheduler\n",
        "            params = [p for p in model.parameters() if p.requires_grad]\n",
        "            optimizer = optim.SGD(\n",
        "                params,\n",
        "                lr = 0.005, # Default value if not passed\n",
        "                momentum=0.9,\n",
        "                weight_decay=0.0005\n",
        "            )\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "\n",
        "            # Train model and get the best model state for the current fold\n",
        "            trained_model, best_fold_val_loss, best_epoch, total_training_time, best_model_state = train_model_frcnn(\n",
        "                      model, train_loader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor,\n",
        "                      fold_idx, device\n",
        "                  )\n",
        "\n",
        "            # Update the overall best model if necessary\n",
        "            if best_fold_val_loss < overall_best_loss:\n",
        "                print(f\"Validation loss in {fold_idx + 1} improved from {overall_best_loss:.4f} to {best_fold_val_loss:.4f} \"\n",
        "                      f\"by Epoch {best_epoch}.\")\n",
        "                overall_best_loss = best_fold_val_loss\n",
        "                overall_best_model_state = best_model_state  # Update overall best model state\n",
        "                overall_best_fold = fold_idx + 1  # Save fold index (1-based)\n",
        "                overall_best_epoch = best_epoch  # Save the best epoch\n",
        "\n",
        "            # Load the best model state for the current fold\n",
        "            trained_model.load_state_dict(best_model_state)\n",
        "\n",
        "            print(f\"Calculating and Saving Predictions for best model in fold {fold_idx + 1}...\")\n",
        "\n",
        "            # Collect predictions, ground truths, and inference speed\n",
        "            fold_predictions, fold_ground_truths, avg_inference_time = collect_predictions_and_ground_truths_with_speed(\n",
        "                trained_model, val_loader, device\n",
        "            )\n",
        "\n",
        "            # Create a dictionary for this fold's data\n",
        "            fold_data = {\n",
        "                'fold_index': fold_idx + 1,\n",
        "                'best_fold_val_loss': best_fold_val_loss,\n",
        "                'best_epoch': best_epoch,\n",
        "                'total_training_time': total_training_time,\n",
        "                'avg_inference_time': avg_inference_time,\n",
        "                'fold_predictions': fold_predictions,\n",
        "                'fold_ground_truths': fold_ground_truths\n",
        "            }\n",
        "\n",
        "            all_fold_data[f\"fold_{fold_idx+1}\"] = fold_data\n",
        "\n",
        "            # Clean up\n",
        "            del model, trained_model, optimizer, scheduler\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(\"Done. Ready for the next fold...\")\n",
        "\n",
        "        elif model_name == 'stn_frcnn':\n",
        "            # Implement model creation for STN-CNN\n",
        "\n",
        "            model = create_model(num_classes=num_classes, anchor_sizes=anchor_sizes, aspect_ratios=aspect_ratios, rpn_nms_thresh=rpn_nms_thresh, backbone_name='resnet50', pretrained=True, stn_type=stn_type)\n",
        "            model.to(device)\n",
        "\n",
        "            # Define optimizer and scheduler\n",
        "            params = [p for p in model.parameters() if p.requires_grad]\n",
        "            optimizer = optim.SGD(\n",
        "                params,\n",
        "                lr = 0.005, # Default value if not passed\n",
        "                momentum=0.9,\n",
        "                weight_decay=0.0005\n",
        "            )\n",
        "            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "            # Train model and get the best model state for the current fold\n",
        "            trained_model, best_fold_val_loss, best_epoch, total_training_time, best_model_state = train_model(\n",
        "                      model, train_loader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor,\n",
        "                      fold_idx, device\n",
        "                  )\n",
        "\n",
        "            # Update the overall best model if necessary\n",
        "            if best_fold_val_loss < overall_best_loss:\n",
        "                print(f\"Validation loss in {fold_idx + 1} improved from {overall_best_loss:.4f} to {best_fold_val_loss:.4f} \"\n",
        "                      f\"by Epoch {best_epoch}.\")\n",
        "                overall_best_loss = best_fold_val_loss\n",
        "                overall_best_model_state = best_model_state  # Update overall best model state\n",
        "                overall_best_fold = fold_idx + 1  # Save fold index (1-based)\n",
        "                overall_best_epoch = best_epoch  # Save the best epoch\n",
        "\n",
        "            # Load the best model state for the current fold\n",
        "            trained_model.load_state_dict(best_model_state)\n",
        "\n",
        "            print(f\"Calculating and Saving Predictions for best model in fold {fold_idx + 1}...\")\n",
        "\n",
        "            # Collect predictions, ground truths, and inference speed\n",
        "            fold_predictions, fold_ground_truths, avg_inference_time = collect_predictions_and_ground_truths_with_speed(\n",
        "                trained_model, val_loader, device\n",
        "            )\n",
        "\n",
        "            # Create a dictionary for this fold's data\n",
        "            fold_data = {\n",
        "                'fold_index': fold_idx + 1,\n",
        "                'best_fold_val_loss': best_fold_val_loss,\n",
        "                'best_epoch': best_epoch,\n",
        "                'total_training_time': total_training_time,\n",
        "                'avg_inference_time': avg_inference_time,\n",
        "                'fold_predictions': fold_predictions,\n",
        "                'fold_ground_truths': fold_ground_truths\n",
        "            }\n",
        "\n",
        "            all_fold_data[f\"fold_{fold_idx+1}\"] = fold_data\n",
        "\n",
        "            # Clean up\n",
        "            del model, trained_model, optimizer, scheduler\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(\"Done. Ready for the next fold...\")\n",
        "\n",
        "        elif model_name == 'rfbnet':\n",
        "            # Implement model creation for STN-RFBNet\n",
        "\n",
        "            # Call the train_and_validate_rfbnet function for each fold\n",
        "            train_and_validate_rfbnet(\n",
        "                train_data=train_subset,\n",
        "                val_data=val_subset,\n",
        "                anchorbox_params=anchorbox_params,\n",
        "                num_epochs=num_epochs,\n",
        "                batch_size=kwargs.get('batch_size', 10),\n",
        "                priors=None,  # Priors will be generated within the train_and_validate_rfbnet function\n",
        "                device=device,\n",
        "                fold_idx=fold_idx)  # Pass the fold index\n",
        "\n",
        "            # Clean up (if variables are defined)\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            print(\"Done. Ready for the next fold...\")\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported model: {model_name}\")\n",
        "\n",
        "    # After all folds are processed, save everything in a single pkl file\n",
        "    if all_fold_data is not None:\n",
        "        with open(f'all_fold_data_{model_name}.pkl', 'wb') as f:\n",
        "            pickle.dump(all_fold_data, f)\n",
        "\n",
        "    # After all folds are done, save the best model across all folds\n",
        "    if overall_best_model_state is not None:\n",
        "        model_filename = f'overall_best_model_fold_{overall_best_fold}_epoch_{overall_best_epoch}.pth'\n",
        "        torch.save(overall_best_model_state, model_filename)\n",
        "        print(f\"\\nBest model across all folds saved with validation loss: {overall_best_loss:.4f} from Fold {overall_best_fold}, Epoch {overall_best_epoch}.\")\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1guI865oz8D"
      },
      "source": [
        "### Call the Training Models (KFold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp-daiKtoz8D",
        "outputId": "08b1d224-2eba-4aaf-c3bc-b503141f6786"
      },
      "outputs": [],
      "source": [
        "# 20-fold Cross-validation parameters\n",
        "num_folds = 20\n",
        "num_epochs = 5\n",
        "batch_size = 3\n",
        "\n",
        "# 20-fold Cross-validation parameters (Faster RCNN+STN)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA GPU available, using the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU available, using the CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "fc0d92dee8ef49578dc593d27ce7aa7f",
            "87c002e4e16342f5858b648577a368d6",
            "c80108bae03b460db1361b0bb282ad2a",
            "d8832cd09ab949b282f7bbd982978c35",
            "17f6edc3809f41fe87141145920dd0dd",
            "28f0c893e5e74ac68a91b2754105123b",
            "ef2e7f082c454b78a48aa2d4cc017cbb",
            "d06147ec99814929909d6d6de6dddec2",
            "3217364beb2c4d68b43cf5562eba96d6",
            "322426750dc94ffa838dd2b4bca31607",
            "e51ece58127c4a4cafc40f3c379a2987"
          ]
        },
        "id": "1I_Uj00Zoz8D",
        "outputId": "5fdf64d1-c9e0-43ab-8122-2669ad5da701"
      },
      "outputs": [],
      "source": [
        "## TRAIN FASTER RCNN\n",
        "\n",
        "# Output the device being used\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters\n",
        "anchor_sizes = ((32, 64, 128),)  # Anchor sizes\n",
        "aspect_ratios = ((0.5, 1.0, 2.0),)  # Aspect ratios\n",
        "rpn_nms_thresh = 0.7\n",
        "\n",
        "# Perform cross-validation Faster-RCNN\n",
        "cross_validate_model(\n",
        "    model_name='faster_rcnn',\n",
        "    dataset=train_dataset,\n",
        "    num_folds=num_folds,\n",
        "    num_epochs=num_epochs,\n",
        "    device = device,\n",
        "    anchor_sizes=anchor_sizes,\n",
        "    aspect_ratios=aspect_ratios,\n",
        "    rpn_nms_thresh = rpn_nms_thresh,\n",
        "    batch_size=batch_size\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "1344622f364849879213d96fc5de7115",
            "284f1e73a23f46229baf8957730b369e",
            "dca86c5fa37d4876acd08f8cb48b5cc4",
            "cff013a727974e22b3bb22f3cd5a510b",
            "563e936c5b5442b599a0a11214fd9dba",
            "32936de581544ea7aa701c77406784c1",
            "aed054e4b0b8441fb50ea523242b9fc2",
            "cb2df77cd8354a2d87faa99cf86ddb3e",
            "8b616f1933794926b4f38c08061c48ca",
            "8f7aace8b98742fd83555192cf8b4717",
            "cb36545c775f4a8cbbdca3d6424779f2"
          ]
        },
        "id": "71_r0RPEoz8D",
        "outputId": "3f2cc2d9-d842-41a8-9dfb-5e06aeaa081b"
      },
      "outputs": [],
      "source": [
        "## TRAIN STN+FASTER RCNN\n",
        "import traceback\n",
        "\n",
        "# Output the device being used\n",
        "print(f\"Using device: {device}\")\n",
        "num_folds = 20\n",
        "num_epochs = 7\n",
        "batch_size = 1\n",
        "# Hyperparameters\n",
        "anchor_sizes = ((32, 64, 128),)  # Anchor sizes\n",
        "aspect_ratios = ((0.5, 1.0, 2.0),)  # Aspect ratios\n",
        "rpn_nms_thresh = 0.7\n",
        "stn_type=\"affine\"\n",
        "\n",
        "# Perform cross-validation Faster-RCNN\n",
        "with torch.autograd.set_detect_anomaly(True): #we need to avoid incoorect transformatiosns --> manually applied transformations\n",
        "  cross_validate_model(\n",
        "      model_name='stn_frcnn',\n",
        "      dataset=train_dataset,\n",
        "      num_folds=num_folds,\n",
        "      num_epochs=num_epochs,\n",
        "      stn_type=stn_type,\n",
        "      anchor_sizes=anchor_sizes,\n",
        "      aspect_ratios=aspect_ratios,\n",
        "      rpn_nms_thresh = rpn_nms_thresh,\n",
        "      batch_size=batch_size,\n",
        "      device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jAUH7HIoz8D"
      },
      "outputs": [],
      "source": [
        "## TRAIN RFBNet\n",
        "\n",
        "# Output the device being used\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "#Hyperparameter\n",
        "\n",
        "anchorbox_params={\n",
        "    \"RFB_512\" : {\n",
        "        'feature_maps' : [64, 32, 16, 8, 4, 2, 1],\n",
        "\n",
        "        'min_dim' : 512,\n",
        "\n",
        "        'steps' : [8, 16, 32, 64, 128, 256, 512],\n",
        "\n",
        "        'min_sizes' : [20.48, 51.2, 133.12, 215.04, 296.96, 378.88, 460.8],\n",
        "\n",
        "        'max_sizes' : [51.2, 133.12, 215.04, 296.96, 378.88, 460.8, 542.72],\n",
        "\n",
        "        'aspect_ratios' : [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "\n",
        "        'variance' : [0.1, 0.2],\n",
        "\n",
        "        'clip' : True,\n",
        "    }\n",
        "}\n",
        "\n",
        "# Call cross-validation\n",
        "cross_validate_model(\n",
        "    model_name='rfbnet',\n",
        "    dataset=train_dataset_rfb,  # Your preprocessed dataset\n",
        "    num_folds=num_folds,  # Number of folds\n",
        "    num_epochs=num_epochs,  # Number of epochs per fold\n",
        "    batch_size=batch_size,  # Batch size\n",
        "    device=device,  # Device (GPU or CPU)\n",
        "    anchorbox_params=anchorbox_params  # Anchor box configuration\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvz8rodRoz8D"
      },
      "source": [
        "## Unpacking Predictions & Calculating Performance Metrics (and Error Bars for each all 3 models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfwxjmSuoz8D"
      },
      "source": [
        "### Unpacking from pkl file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Hji4dXhoz8D"
      },
      "outputs": [],
      "source": [
        "### FASTER R-CNN\n",
        "\n",
        "# 5 epoch - > best model , val loss Fold 1\n",
        "\n",
        "file_path = 'all_fold_data_faster_rcnn.pkl'\n",
        "\n",
        "pickle_file_path = os.path.join(base_path, file_path)\n",
        "\n",
        "    # Open the pickle file and load the data\n",
        "with open(pickle_file_path, 'rb') as file:\n",
        "    faster_r_cnn_predictions = pickle.load(file)\n",
        "\n",
        "# Loop over the first 5 folds and extract the first 2 records of predictions and ground truths\n",
        "num_folds_to_view = 5\n",
        "num_records_to_view = 6\n",
        "\n",
        "# Dictionary to store extracted data\n",
        "extracted_data = {}\n",
        "\n",
        "for fold_idx in range(1, num_folds_to_view + 1):\n",
        "    fold_key = f'fold_{fold_idx}'\n",
        "\n",
        "    fold_data = faster_r_cnn_predictions[fold_key]\n",
        "\n",
        "    # Get first 2 records of predictions and ground truths\n",
        "    first_predictions = fold_data['fold_predictions'][:num_records_to_view]\n",
        "    first_ground_truths = fold_data['fold_ground_truths'][:num_records_to_view]\n",
        "\n",
        "    # Add all other values from the fold's dictionary\n",
        "    extracted_data[fold_key] = {\n",
        "        'fold_index': fold_data.get('fold_index'),\n",
        "        'best_fold_val_loss': fold_data.get('best_fold_val_loss'),\n",
        "        'best_epoch': fold_data.get('best_epoch'),\n",
        "        'total_training_time': fold_data.get('total_training_time'),\n",
        "        'avg_inference_time': fold_data.get('avg_inference_time'),\n",
        "        'predictions': first_predictions,\n",
        "        'ground_truths': first_ground_truths\n",
        "    }\n",
        "\n",
        "# Print the extracted data directly\n",
        "print(extracted_data)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N5YttdCqoz8E"
      },
      "outputs": [],
      "source": [
        "### FASTER R-CNN +STN\n",
        "\n",
        "#Best model overall --> fold 7 epoch 5 \n",
        "\n",
        "\n",
        "file_path = '/content/all_fold_data_stn_frcnn.pkl'\n",
        "\n",
        "pickle_file_path = '/content/all_fold_data_stn_frcnn.pkl'\n",
        "\n",
        "    # Open the pickle file and load the data\n",
        "with open(pickle_file_path, 'rb') as file:\n",
        "    faster_r_cnn_predictions = pickle.load(file)\n",
        "\n",
        "# Loop over the first 5 folds and extract the first 2 records of predictions and ground truths\n",
        "num_folds_to_view = 5\n",
        "num_records_to_view = 2\n",
        "\n",
        "# Dictionary to store extracted data\n",
        "extracted_data = {}\n",
        "\n",
        "for fold_idx in range(1, num_folds_to_view + 1):\n",
        "    fold_key = f'fold_{fold_idx}'\n",
        "\n",
        "    fold_data = faster_r_cnn_predictions[fold_key]\n",
        "\n",
        "    # Get first 2 records of predictions and ground truths\n",
        "    first_predictions = fold_data['fold_predictions'][:num_records_to_view]\n",
        "    first_ground_truths = fold_data['fold_ground_truths'][:num_records_to_view]\n",
        "\n",
        "    # Add all other values from the fold's dictionary\n",
        "    extracted_data[fold_key] = {\n",
        "        'fold_index': fold_data.get('fold_index'),\n",
        "        'best_fold_val_loss': fold_data.get('best_fold_val_loss'),\n",
        "        'best_epoch': fold_data.get('best_epoch'),\n",
        "        'total_training_time': fold_data.get('total_training_time'),\n",
        "        'avg_inference_time': fold_data.get('avg_inference_time'),\n",
        "        'predictions': first_predictions,\n",
        "        'ground_truths': first_ground_truths\n",
        "    }\n",
        "\n",
        "# Print the extracted data directly\n",
        "print(extracted_data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OT4yqiTfoz8E"
      },
      "outputs": [],
      "source": [
        "### RFBNet\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVXDx_wwoz8E"
      },
      "source": [
        "### Performance Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpnBymunoz8E"
      },
      "source": [
        "#### IoU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "h6x-ZlbRoz8E"
      },
      "outputs": [],
      "source": [
        "def calculate_iou(box1, box2):\n",
        "    \"\"\"\n",
        "    Calculates IoU between two boxes.\n",
        "    Each box is represented by [xmin, ymin, xmax, ymax].\n",
        "\n",
        "    Parameters:\n",
        "    - box1: ndarray of shape (4,)\n",
        "    - box2: ndarray of shape (4,)\n",
        "\n",
        "    Returns:\n",
        "    - iou: float\n",
        "    \"\"\"\n",
        "    # Compute intersection\n",
        "    x_min = max(box1[0], box2[0])\n",
        "    y_min = max(box1[1], box2[1])\n",
        "    x_max = min(box1[2], box2[2])\n",
        "    y_max = min(box1[3], box2[3])\n",
        "\n",
        "    intersection = max(0, x_max - x_min) * max(0, y_max - y_min)\n",
        "\n",
        "    # Compute union\n",
        "    area_box1 = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "    union = area_box1 + area_box2 - intersection\n",
        "\n",
        "    if union == 0:\n",
        "        return 0.0\n",
        "\n",
        "    iou = intersection / union\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TskB9YX3oz8E"
      },
      "source": [
        "#### Average Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "PCxaVKLyoz8E"
      },
      "outputs": [],
      "source": [
        "def compute_ap(recalls, precisions):\n",
        "    \"\"\"\n",
        "    Compute Average Precision given precision and recall.\n",
        "\n",
        "    Parameters:\n",
        "    - recalls: Array of recall values.\n",
        "    - precisions: Array of precision values.\n",
        "\n",
        "    Returns:\n",
        "    - ap: Average Precision.\n",
        "    \"\"\"\n",
        "    # Append sentinel values at the end\n",
        "    recalls = np.concatenate(([0.0], recalls, [1.0]))\n",
        "    precisions = np.concatenate(([0.0], precisions, [0.0]))\n",
        "\n",
        "    # Compute the precision envelope\n",
        "    for i in range(len(precisions) - 1, 0, -1):\n",
        "        precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n",
        "\n",
        "    # Integrate area under curve\n",
        "    indices = np.where(recalls[1:] != recalls[:-1])[0]\n",
        "    ap = np.sum((recalls[indices + 1] - recalls[indices]) * precisions[indices + 1])\n",
        "    return ap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuStNUqcoz8E"
      },
      "source": [
        "#### Mean Average Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "HSR-HMNuoz8E"
      },
      "outputs": [],
      "source": [
        "def calculate_map(predictions, ground_truths, iou_threshold=0.5, num_classes=5):\n",
        "    \"\"\"\n",
        "    Calculates mean Average Precision (mAP) at the specified IoU threshold.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: List of dicts with keys 'boxes', 'scores', 'labels'.\n",
        "    - ground_truths: List of dicts with keys 'boxes', 'labels'.\n",
        "    - iou_threshold: IoU threshold to consider a detection as True Positive.\n",
        "    - num_classes: Number of classes (excluding background).\n",
        "\n",
        "    Returns:\n",
        "    - mean_ap: Mean Average Precision over all classes.\n",
        "    \"\"\"\n",
        "    aps = []\n",
        "    for cls in range(1, num_classes + 1):  # Classes are 1-indexed\n",
        "        true_positives = []\n",
        "        scores = []\n",
        "        num_gt = 0\n",
        "\n",
        "        # Collect detections and ground truths for this class\n",
        "        for pred, gt in zip(predictions, ground_truths):\n",
        "            pred_boxes = pred['boxes'][pred['labels'] == cls]\n",
        "            pred_scores = pred['scores'][pred['labels'] == cls]\n",
        "            gt_boxes = gt['boxes'][gt['labels'] == cls]\n",
        "\n",
        "            num_gt += len(gt_boxes)\n",
        "            detected = []\n",
        "\n",
        "            # Sort predictions by confidence\n",
        "            sorted_indices = np.argsort(-pred_scores)\n",
        "            pred_boxes = pred_boxes[sorted_indices]\n",
        "            pred_scores = pred_scores[sorted_indices]\n",
        "\n",
        "            for pred_box in pred_boxes:\n",
        "                ious = [calculate_iou(pred_box, gt_box) for gt_box in gt_boxes]\n",
        "                max_iou = max(ious) if ious else 0\n",
        "                if max_iou >= iou_threshold and gt_boxes.size > 0:\n",
        "                    if ious.index(max_iou) not in detected:\n",
        "                        true_positives.append(1)\n",
        "                        detected.append(ious.index(max_iou))\n",
        "                    else:\n",
        "                        true_positives.append(0)\n",
        "                else:\n",
        "                    true_positives.append(0)\n",
        "                scores.append(pred_scores[0])\n",
        "\n",
        "        if num_gt == 0:\n",
        "            continue\n",
        "\n",
        "        # Compute precision-recall curve\n",
        "        true_positives = np.array(true_positives)\n",
        "        scores = np.array(scores)\n",
        "        indices = np.argsort(-scores)\n",
        "        true_positives = true_positives[indices]\n",
        "        false_positives = 1 - true_positives\n",
        "\n",
        "        cum_tp = np.cumsum(true_positives)\n",
        "        cum_fp = np.cumsum(false_positives)\n",
        "        recalls = cum_tp / num_gt\n",
        "        precisions = cum_tp / (cum_tp + cum_fp)\n",
        "\n",
        "        # Compute AP\n",
        "        ap = compute_ap(recalls, precisions)\n",
        "        aps.append(ap)\n",
        "\n",
        "    if len(aps) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    mean_ap = np.mean(aps)\n",
        "    return mean_ap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7zrXwCpoz8E"
      },
      "source": [
        "#### Recall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "7wbCMDyBoz8E"
      },
      "outputs": [],
      "source": [
        "def calculate_recall(predictions, ground_truths, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculates Recall@IoU for the given predictions and ground truths.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: List of dicts with keys 'boxes', 'scores', 'labels'.\n",
        "    - ground_truths: List of dicts with keys 'boxes', 'labels'.\n",
        "    - iou_threshold: IoU threshold to consider a detection as True Positive.\n",
        "\n",
        "    Returns:\n",
        "    - recall: Recall at the specified IoU threshold.\n",
        "    \"\"\"\n",
        "    true_positives = 0\n",
        "    total_gt_boxes = 0\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        pred_boxes = pred['boxes']\n",
        "        pred_labels = pred['labels']\n",
        "        gt_boxes = gt['boxes']\n",
        "        gt_labels = gt['labels']\n",
        "        total_gt_boxes += len(gt_boxes)\n",
        "        detected = []\n",
        "\n",
        "        for gt_idx, gt_box in enumerate(gt_boxes):\n",
        "            ious = [calculate_iou(gt_box, pred_box) for pred_box in pred_boxes]\n",
        "            max_iou = max(ious) if ious else 0\n",
        "            if max_iou >= iou_threshold:\n",
        "                pred_idx = ious.index(max_iou)\n",
        "                if pred_idx not in detected and pred_labels[pred_idx] == gt_labels[gt_idx]:\n",
        "                    true_positives += 1\n",
        "                    detected.append(pred_idx)\n",
        "\n",
        "    if total_gt_boxes == 0:\n",
        "        return 0.0\n",
        "\n",
        "    recall = true_positives / total_gt_boxes\n",
        "    return recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk39aiAooz8E"
      },
      "source": [
        "#### Precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KtC1-NsPoz8E"
      },
      "outputs": [],
      "source": [
        "def calculate_precision(predictions, ground_truths, iou_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Calculates Precision@IoU for the given predictions and ground truths.\n",
        "\n",
        "    Parameters:\n",
        "    - predictions: List of dicts with keys 'boxes', 'scores', 'labels'.\n",
        "    - ground_truths: List of dicts with keys 'boxes', 'labels'.\n",
        "    - iou_threshold: IoU threshold to consider a detection as True Positive.\n",
        "\n",
        "    Returns:\n",
        "    - precision: Precision at the specified IoU threshold.\n",
        "    \"\"\"\n",
        "    true_positives = 0\n",
        "    total_predicted_boxes = 0\n",
        "\n",
        "    for pred, gt in zip(predictions, ground_truths):\n",
        "        pred_boxes = pred['boxes']\n",
        "        pred_labels = pred['labels']\n",
        "        gt_boxes = gt['boxes']\n",
        "        gt_labels = gt['labels']\n",
        "\n",
        "        total_predicted_boxes += len(pred_boxes)\n",
        "        matched_gt_indices = []\n",
        "\n",
        "        for pred_idx, pred_box in enumerate(pred_boxes):\n",
        "            ious = [calculate_iou(pred_box, gt_box) for gt_box in gt_boxes]\n",
        "            max_iou = max(ious) if ious else 0\n",
        "            if max_iou >= iou_threshold:\n",
        "                gt_idx = ious.index(max_iou)\n",
        "                if gt_idx not in matched_gt_indices and pred_labels[pred_idx] == gt_labels[gt_idx]:\n",
        "                    true_positives += 1\n",
        "                    matched_gt_indices.append(gt_idx)\n",
        "\n",
        "    if total_predicted_boxes == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = true_positives / total_predicted_boxes\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPSrF0A1oz8E"
      },
      "source": [
        "#### F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mzf1AXmAoz8E"
      },
      "outputs": [],
      "source": [
        "def calculate_f1_score(precision, recall):\n",
        "    \"\"\"\n",
        "    Calculates the F1 Score given precision and recall.\n",
        "\n",
        "    Parameters:\n",
        "    - precision (float): Precision value.\n",
        "    - recall (float): Recall value.\n",
        "\n",
        "    Returns:\n",
        "    - f1_score (float): F1 Score.\n",
        "    \"\"\"\n",
        "    if (precision + recall) == 0:\n",
        "        return 0.0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "    return f1_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFINoOPUoz8E"
      },
      "source": [
        "#### Total training time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "CT5I_nn-oz8E"
      },
      "outputs": [],
      "source": [
        "def calculate_total_training_time(all_fold_data):\n",
        "    \"\"\"\n",
        "    Calculates the total training time across all folds.\n",
        "\n",
        "    Parameters:\n",
        "    - all_fold_data: Dictionary containing data for all folds.\n",
        "\n",
        "    Returns:\n",
        "    - total_training_time: Total training time across all folds.\n",
        "    - avg_training_time: Average training time per fold.\n",
        "    - var_training_time: Variance of training time across all folds.\n",
        "    \"\"\"\n",
        "    training_times = [fold_data['total_training_time'] for fold_data in all_fold_data.values()]\n",
        "    total_training_time = np.sum(training_times)\n",
        "    avg_training_time = np.mean(training_times)\n",
        "    var_training_time = np.var(training_times)\n",
        "    return total_training_time, avg_training_time, var_training_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZs73BEsoz8E"
      },
      "source": [
        "#### Total Inference Time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KYIDSuPnoz8E"
      },
      "outputs": [],
      "source": [
        "def calculate_average_inference_time(all_fold_data):\n",
        "    \"\"\"\n",
        "    Calculates the average inference time across all folds.\n",
        "\n",
        "    Parameters:\n",
        "    - all_fold_data: Dictionary containing data for all folds.\n",
        "\n",
        "    Returns:\n",
        "    - avg_inference_time: Average inference time across all folds.\n",
        "    - var_inference_time: Variance of inference time across all folds.\n",
        "    \"\"\"\n",
        "    inference_times = [fold_data['avg_inference_time'] for fold_data in all_fold_data.values()]\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "    var_inference_time = np.var(inference_times)\n",
        "    return avg_inference_time, var_inference_time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XK4qqwlZoz8E"
      },
      "source": [
        "### Processing for All folds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LpKqQu-roz8E"
      },
      "outputs": [],
      "source": [
        "def process_all_folds(all_fold_data):\n",
        "    \"\"\"\n",
        "    Processes all folds to compute performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - all_fold_data: Dictionary containing data for all folds.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_per_fold: List of dictionaries containing metrics for each fold.\n",
        "    \"\"\"\n",
        "    metrics_per_fold = []\n",
        "\n",
        "    for fold_name, fold_data in all_fold_data.items():\n",
        "        fold_index = fold_data['fold_index']\n",
        "        predictions = fold_data['fold_predictions']\n",
        "        ground_truths = fold_data['fold_ground_truths']\n",
        "        total_training_time = fold_data['total_training_time']\n",
        "        avg_inference_time = fold_data['avg_inference_time']\n",
        "        best_fold_val_loss = fold_data['best_fold_val_loss']\n",
        "        best_epoch = fold_data['best_epoch']\n",
        "\n",
        "        # Calculate mAP\n",
        "        mAP = calculate_map(predictions, ground_truths, iou_threshold=0.5, num_classes=5)\n",
        "\n",
        "        # Calculate Recall\n",
        "        recall = calculate_recall(predictions, ground_truths, iou_threshold=0.5)\n",
        "\n",
        "        # Calculate Precision\n",
        "        precision = calculate_precision(predictions, ground_truths, iou_threshold=0.5)\n",
        "\n",
        "        # Calculate F1 Score\n",
        "        f1_score = calculate_f1_score(precision, recall)\n",
        "\n",
        "        # Store metrics\n",
        "        metrics_per_fold.append({\n",
        "            'fold_index': fold_index,\n",
        "            'mAP': mAP,\n",
        "            'recall': recall,\n",
        "            'precision': precision,\n",
        "            'f1_score': f1_score,\n",
        "            'total_training_time': total_training_time,\n",
        "            'avg_inference_time': avg_inference_time,\n",
        "            'best_fold_val_loss': best_fold_val_loss,\n",
        "            'best_epoch': best_epoch\n",
        "        })\n",
        "\n",
        "    return metrics_per_fold\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "snV1qYzLoz8E"
      },
      "outputs": [],
      "source": [
        "def compute_mean_and_variance(metrics_per_fold):\n",
        "    \"\"\"\n",
        "    Computes the mean and variance of performance metrics across folds.\n",
        "\n",
        "    Parameters:\n",
        "    - metrics_per_fold: List of dictionaries containing metrics for each fold.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_summary: Dictionary containing mean and variance for each metric.\n",
        "    \"\"\"\n",
        "    mAPs = [m['mAP'] for m in metrics_per_fold]\n",
        "    recalls = [m['recall'] for m in metrics_per_fold]\n",
        "    precisions = [m['precision'] for m in metrics_per_fold]\n",
        "    f1_scores = [m['f1_score'] for m in metrics_per_fold]\n",
        "    training_times = [m['total_training_time'] for m in metrics_per_fold]\n",
        "    inference_times = [m['avg_inference_time'] for m in metrics_per_fold]\n",
        "\n",
        "    metrics_summary = {\n",
        "        'mAP_mean': np.mean(mAPs),\n",
        "        'mAP_variance': np.var(mAPs),\n",
        "        'recall_mean': np.mean(recalls),\n",
        "        'recall_variance': np.var(recalls),\n",
        "        'precision_mean': np.mean(precisions),\n",
        "        'precision_variance': np.var(precisions),\n",
        "        'f1_score_mean': np.mean(f1_scores),\n",
        "        'f1_score_variance': np.var(f1_scores),\n",
        "        'training_time_total': np.sum(training_times),\n",
        "        'training_time_mean': np.mean(training_times),\n",
        "        'training_time_variance': np.var(training_times),\n",
        "        'inference_time_mean': np.mean(inference_times),\n",
        "        'inference_time_variance': np.var(inference_times)\n",
        "    }\n",
        "\n",
        "    return metrics_summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "YAo0TDheoz8F"
      },
      "outputs": [],
      "source": [
        "def plot_metric_with_error_bars(metrics_per_fold, metric_name):\n",
        "    \"\"\"\n",
        "    Plots the specified metric across folds with error bars representing the variance.\n",
        "\n",
        "    Parameters:\n",
        "    - metrics_per_fold: List of dictionaries containing metrics for each fold.\n",
        "    - metric_name: Name of the metric to plot ('mAP', 'recall', 'precision', etc.).\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    fold_indices = [m['fold_index'] for m in metrics_per_fold]\n",
        "    metric_values = [m[metric_name] for m in metrics_per_fold]\n",
        "\n",
        "    mean_value = np.mean(metric_values)\n",
        "    std_value = np.std(metric_values)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.errorbar(fold_indices, metric_values, yerr=std_value, fmt='o', capsize=5, label=f'{metric_name} per fold')\n",
        "    plt.hlines(mean_value, xmin=min(fold_indices), xmax=max(fold_indices), colors='r', label=f'Mean {metric_name}')\n",
        "    plt.fill_between(fold_indices, mean_value - std_value, mean_value + std_value, color='r', alpha=0.2, label='Std Dev')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel(metric_name)\n",
        "    plt.title(f'{metric_name.upper()} per Fold with Mean and Std Dev')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "W0eK7vo3oz8F"
      },
      "outputs": [],
      "source": [
        "def plot_time_metric_with_error_bars(metrics_per_fold, metric_name):\n",
        "    \"\"\"\n",
        "    Plots the specified time metric across folds with error bars representing the variance.\n",
        "\n",
        "    Parameters:\n",
        "    - metrics_per_fold: List of dictionaries containing metrics for each fold.\n",
        "    - metric_name: Name of the metric to plot ('total_training_time', 'avg_inference_time').\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    fold_indices = [m['fold_index'] for m in metrics_per_fold]\n",
        "    metric_values = [m[metric_name] for m in metrics_per_fold]\n",
        "\n",
        "    mean_value = np.mean(metric_values)\n",
        "    std_value = np.std(metric_values)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.errorbar(fold_indices, metric_values, yerr=std_value, fmt='o', capsize=5, label=f'{metric_name} per fold')\n",
        "    plt.hlines(mean_value, xmin=min(fold_indices), xmax=max(fold_indices), colors='r', label=f'Mean {metric_name}')\n",
        "    plt.fill_between(fold_indices, mean_value - std_value, mean_value + std_value, color='r', alpha=0.2, label='Std Dev')\n",
        "    plt.xlabel('Fold')\n",
        "    plt.ylabel('Time (s)')\n",
        "    plt.title(f'{metric_name.replace(\"_\", \" \").title()} per Fold with Mean and Std Dev')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGUEsfGdoz8F"
      },
      "source": [
        "### Error Bars for each Performance Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usr2gSpooz8F"
      },
      "source": [
        "#### Faster RCNN Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "DLY3BuAVoz8F",
        "outputId": "829474f8-eff1-4606-9ab0-c40159e0e9d2"
      },
      "outputs": [],
      "source": [
        "metrics_per_fold = process_all_folds(faster_r_cnn_predictions)\n",
        "metrics_summary = compute_mean_and_variance(metrics_per_fold)\n",
        "\n",
        "# Output the metrics\n",
        "print(\"FASTER R CNN\")\n",
        "print('--------------------------------')\n",
        "print(\"Mean and Variance of Metrics across all folds:\\n\")\n",
        "print(f\"mAP: Mean = {metrics_summary['mAP_mean']:.4f}, Variance = {metrics_summary['mAP_variance']:.4f}\")\n",
        "print(f\"Recall: Mean = {metrics_summary['recall_mean']:.4f}, Variance = {metrics_summary['recall_variance']:.4f}\")\n",
        "print(f\"Precision: Mean = {metrics_summary['precision_mean']:.4f}, Variance = {metrics_summary['precision_variance']:.4f}\")\n",
        "print(f\"F1 Score: Mean = {metrics_summary['f1_score_mean']:.4f}, Variance = {metrics_summary['f1_score_variance']:.4f}\")\n",
        "print(f\"Total Training Time: {metrics_summary['training_time_total']:.2f} seconds\")\n",
        "print(f\"Average Training Time per Fold: Mean = {metrics_summary['training_time_mean']:.2f} s, Variance = {metrics_summary['training_time_variance']:.2f}\")\n",
        "print(f\"Average Inference Time per Fold: Mean = {metrics_summary['inference_time_mean']:.6f} s, Variance = {metrics_summary['inference_time_variance']:.6f}\")\n",
        "\n",
        "# Metrics to plot\n",
        "metrics_to_plot = ['mAP', 'recall', 'precision', 'f1_score']\n",
        "\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    plot_metric_with_error_bars(metrics_per_fold, metric)\n",
        "\n",
        "plot_time_metric_with_error_bars(metrics_per_fold, 'total_training_time')\n",
        "plot_time_metric_with_error_bars(metrics_per_fold, 'avg_inference_time')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGoay7w9oz8F"
      },
      "source": [
        "#### STN + Faster RCNN Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "file_path = 'all_fold_data_faster_rcnn.pkl'\n",
        "\n",
        "pickle_file_path = os.path.join(base_path, file_path)\n",
        "if os.path.exists(pickle_file_path):\n",
        "    with open(pickle_file_path, 'rb') as file:\n",
        "        data_dict = pickle.load(file)\n",
        "    print(f\"Loaded data type: {type(data_dict)}\")\n",
        "else:\n",
        "    print(\"Pickle file not found. Please check the file path.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "3YnkfDvtoz8F",
        "outputId": "73973def-5765-4a0f-fef2-e177a84fe5ee"
      },
      "outputs": [],
      "source": [
        "metrics_per_fold = process_all_folds(data_dict)\n",
        "metrics_summary = compute_mean_and_variance(metrics_per_fold)\n",
        "\n",
        "# Output the metrics\n",
        "print(\"STN FASTER R CNN\")\n",
        "print('--------------------------------')\n",
        "print(\"Mean and Variance of Metrics across all folds:\\n\")\n",
        "print(f\"mAP: Mean = {metrics_summary['mAP_mean']:.4f}, Variance = {metrics_summary['mAP_variance']:.4f}\")\n",
        "print(f\"Recall: Mean = {metrics_summary['recall_mean']:.4f}, Variance = {metrics_summary['recall_variance']:.4f}\")\n",
        "print(f\"Precision: Mean = {metrics_summary['precision_mean']:.4f}, Variance = {metrics_summary['precision_variance']:.4f}\")\n",
        "print(f\"F1 Score: Mean = {metrics_summary['f1_score_mean']:.4f}, Variance = {metrics_summary['f1_score_variance']:.4f}\")\n",
        "print(f\"Total Training Time: {metrics_summary['training_time_total']:.2f} seconds\")\n",
        "print(f\"Average Training Time per Fold: Mean = {metrics_summary['training_time_mean']:.2f} s, Variance = {metrics_summary['training_time_variance']:.2f}\")\n",
        "print(f\"Average Inference Time per Fold: Mean = {metrics_summary['inference_time_mean']:.6f} s, Variance = {metrics_summary['inference_time_variance']:.6f}\")\n",
        "\n",
        "# Metrics to plot\n",
        "metrics_to_plot = ['mAP', 'recall', 'precision', 'f1_score']\n",
        "\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    plot_metric_with_error_bars(metrics_per_fold, metric)\n",
        "\n",
        "plot_time_metric_with_error_bars(metrics_per_fold, 'total_training_time')\n",
        "plot_time_metric_with_error_bars(metrics_per_fold, 'avg_inference_time')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrc2M3Yroz8F"
      },
      "source": [
        "#### RFB Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-k5mhjeoz8F"
      },
      "outputs": [],
      "source": [
        "metrics_per_fold = process_all_folds()\n",
        "metrics_summary = compute_mean_and_variance(metrics_per_fold)\n",
        "\n",
        "# Output the metrics\n",
        "print(\"RFB Metrics\")\n",
        "print('--------------------------------')\n",
        "print(\"Mean and Variance of Metrics across all folds:\\n\")\n",
        "print(f\"mAP: Mean = {metrics_summary['mAP_mean']:.4f}, Variance = {metrics_summary['mAP_variance']:.4f}\")\n",
        "print(f\"Recall: Mean = {metrics_summary['recall_mean']:.4f}, Variance = {metrics_summary['recall_variance']:.4f}\")\n",
        "print(f\"Precision: Mean = {metrics_summary['precision_mean']:.4f}, Variance = {metrics_summary['precision_variance']:.4f}\")\n",
        "print(f\"F1 Score: Mean = {metrics_summary['f1_score_mean']:.4f}, Variance = {metrics_summary['f1_score_variance']:.4f}\")\n",
        "print(f\"Total Training Time: {metrics_summary['training_time_total']:.2f} seconds\")\n",
        "print(f\"Average Training Time per Fold: Mean = {metrics_summary['training_time_mean']:.2f} s, Variance = {metrics_summary['training_time_variance']:.2f}\")\n",
        "print(f\"Average Inference Time per Fold: Mean = {metrics_summary['inference_time_mean']:.6f} s, Variance = {metrics_summary['inference_time_variance']:.6f}\")\n",
        "\n",
        "# Metrics to plot\n",
        "metrics_to_plot = ['mAP', 'recall', 'precision', 'f1_score']\n",
        "\n",
        "\n",
        "for metric in metrics_to_plot:\n",
        "    plot_metric_with_error_bars(metrics_per_fold, metric)\n",
        "\n",
        "plot_time_metric_with_error_bars(metrics_per_fold, 'total_training_time')\n",
        "plot_time_metric_with_error_bars(metrics_per_fold, 'avg_inference_time')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLac59Iioz8F"
      },
      "source": [
        "## Stratified Nested K-fold for Hypertuning Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "aklvLXoOoz8F"
      },
      "outputs": [],
      "source": [
        "def stratified_k_fold_split(dataset, num_folds, seed=55):\n",
        "    \"\"\"\n",
        "    Splits the dataset into stratified folds.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: The dataset to split.\n",
        "    - num_folds: Number of folds.\n",
        "    - seed: Random seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - folds: A list of lists, where each inner list contains indices for one fold.\n",
        "    \"\"\"\n",
        "    # Get labels for all samples\n",
        "    labels_list = []\n",
        "    for idx in range(len(dataset)):\n",
        "        # Assuming dataset[idx] returns (image, boxes, labels)\n",
        "        labels = dataset[idx][2].numpy()\n",
        "        labels_list.append(labels)\n",
        "\n",
        "    # Flatten labels and count class occurrences\n",
        "    class_counts = defaultdict(int)\n",
        "    for labels in labels_list:\n",
        "        unique_labels = set(labels.tolist())  # Convert numpy array to list for set operation\n",
        "        for label in unique_labels:\n",
        "            class_counts[label] += 1\n",
        "\n",
        "    # Initialize folds\n",
        "    folds = [[] for _ in range(num_folds)]\n",
        "    label_folds = {label: [[] for _ in range(num_folds)] for label in class_counts.keys()}\n",
        "\n",
        "    # Distribute samples to folds\n",
        "    random.seed(seed)\n",
        "    for label in class_counts.keys():\n",
        "        # Get all indices with this label\n",
        "        indices_with_label = [idx for idx, labels in enumerate(labels_list) if label in labels]\n",
        "        random.shuffle(indices_with_label)\n",
        "        fold_sizes = [len(indices_with_label) // num_folds] * num_folds\n",
        "        for i in range(len(indices_with_label) % num_folds):\n",
        "            fold_sizes[i] += 1\n",
        "        start = 0\n",
        "        for fold_idx, fold_size in enumerate(fold_sizes):\n",
        "            end = start + fold_size\n",
        "            label_folds[label][fold_idx].extend(indices_with_label[start:end])\n",
        "            start = end\n",
        "\n",
        "    # Initialize a set to keep track of assigned indices\n",
        "    assigned_indices = set()\n",
        "\n",
        "    # Combine folds from all labels\n",
        "    for fold_idx in range(num_folds):\n",
        "        fold_indices = []\n",
        "        for label in class_counts.keys():\n",
        "            # Only add indices that haven't been assigned yet\n",
        "            label_indices = [idx for idx in label_folds[label][fold_idx] if idx not in assigned_indices]\n",
        "            fold_indices.extend(label_indices)\n",
        "            assigned_indices.update(label_indices)\n",
        "        random.shuffle(fold_indices)\n",
        "        folds[fold_idx] = fold_indices\n",
        "\n",
        "    return folds\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "Ivd7I_Ynoz8F"
      },
      "outputs": [],
      "source": [
        "### FASTER RCNN\n",
        "\n",
        "def nested_cross_validate_model(model_name, dataset, num_outer_folds=5, num_inner_folds=3, num_epochs=5, **kwargs):\n",
        "    \"\"\"\n",
        "    Performs Nested Stratified K-Fold Cross-Validation with hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Name of the model ('faster_rcnn').\n",
        "    - dataset (torch.utils.data.Dataset): The dataset to perform cross-validation on.\n",
        "    - num_outer_folds (int): Number of outer folds for cross-validation.\n",
        "    - num_inner_folds (int): Number of inner folds for hyperparameter tuning.\n",
        "    - num_epochs (int): Number of epochs to train in each fold.\n",
        "    - kwargs: Additional parameters.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Extract the parameters from kwargs\n",
        "    device = kwargs.get('device')\n",
        "    batch_size = kwargs.get('batch_size', 10)\n",
        "    # Mapping class indices to class names\n",
        "    class_mapping = {0: 'gun', 1: 'knife', 2: 'plier', 3: 'scissor', 4: 'wrench'}\n",
        "    num_classes = len(class_mapping) + 1  # +1 for background\n",
        "    best_overall_recall = -1\n",
        "\n",
        "    # Prepare class weights tensor\n",
        "    class_weights = {'gun': 0.065648621975652,\n",
        "                     'knife': 0.35833206161710135,\n",
        "                     'plier': 0.04574451850431081,\n",
        "                     'scissor': 0.3739117164700188,\n",
        "                     'wrench': 0.15636308143291694}\n",
        "\n",
        "    class_weights_tensor = torch.ones(num_classes)  # Initialize with ones for all classes including background\n",
        "\n",
        "    # Map class indices to weights\n",
        "    for class_index, class_name in class_mapping.items():\n",
        "        class_weight = class_weights[class_name]\n",
        "        class_weights_tensor[class_index] = class_weight\n",
        "\n",
        "    class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "    # Outer Stratified K-Fold Split\n",
        "    outer_folds = stratified_k_fold_split(dataset, num_outer_folds)\n",
        "\n",
        "    # To store metrics across folds\n",
        "    outer_fold_metrics = []\n",
        "\n",
        "    for outer_fold_idx in range(num_outer_folds):\n",
        "        print(f\"\\nOuter Fold {outer_fold_idx + 1}/{num_outer_folds}\")\n",
        "        # Split data for outer fold\n",
        "        val_indices = outer_folds[outer_fold_idx]\n",
        "        train_val_indices = [idx for i, fold in enumerate(outer_folds) if i != outer_fold_idx for idx in fold]\n",
        "        train_val_subset = Subset(dataset, train_val_indices)\n",
        "        val_subset = Subset(dataset, val_indices)\n",
        "\n",
        "        # Inner Stratified K-Fold for Hyperparameter Tuning\n",
        "        inner_folds = stratified_k_fold_split(train_val_subset, num_inner_folds)\n",
        "        best_hyperparams = None\n",
        "        best_mAP = -1\n",
        "\n",
        "        # Access the hyperparameter space from kwargs\n",
        "        hyperparameter_space = kwargs.get('hyperparameter_space')\n",
        "\n",
        "        hyperparameter_combinations = [\n",
        "            {\n",
        "                'anchor_sizes': anchor_sizes,\n",
        "                'aspect_ratios': aspect_ratios,\n",
        "                'rpn_nms_thresh': rpn_nms_thresh\n",
        "            }\n",
        "            for anchor_sizes in hyperparameter_space['anchor_sizes']\n",
        "            for aspect_ratios in hyperparameter_space['aspect_ratios']\n",
        "            for rpn_nms_thresh in hyperparameter_space['rpn_nms_thresh']\n",
        "        ]\n",
        "\n",
        "        for hyperparams in hyperparameter_combinations:\n",
        "            print(f\"\\nTesting hyperparameters: {hyperparams}\")\n",
        "            inner_fold_maps = []\n",
        "\n",
        "            for inner_fold_idx in range(num_inner_folds):\n",
        "                # Split data for inner fold\n",
        "                print(f\"\\nInner Fold {inner_fold_idx + 1}/{num_inner_folds}\")\n",
        "                inner_val_indices = inner_folds[inner_fold_idx]\n",
        "                inner_train_indices = [idx for i, fold in enumerate(inner_folds) if i != inner_fold_idx for idx in fold]\n",
        "                inner_train_subset = Subset(train_val_subset, inner_train_indices)\n",
        "                inner_val_subset = Subset(train_val_subset, inner_val_indices)\n",
        "\n",
        "                # Create DataLoaders\n",
        "                train_loader = DataLoader(inner_train_subset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "                val_loader = DataLoader(inner_val_subset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "                # Create model\n",
        "                try:\n",
        "                    model = create_model_frcnn(num_classes=num_classes, **hyperparams)\n",
        "                    model.to(device)\n",
        "\n",
        "                    # Define optimizer and scheduler\n",
        "                    params = [p for p in model.parameters() if p.requires_grad]\n",
        "                    optimizer = optim.SGD(\n",
        "                        params,\n",
        "                        lr=0.005,  # Default value if not passed\n",
        "                        momentum=0.9,\n",
        "                        weight_decay=0.0005\n",
        "                    )\n",
        "                    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "                    # Train model\n",
        "                    trained_model, _, _, total_training_time, best_model_state = train_model_frcnn(\n",
        "                        model, train_loader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor,\n",
        "                        fold_idx=inner_fold_idx, device=device\n",
        "                    )\n",
        "\n",
        "                    # Load best model state\n",
        "                    trained_model.load_state_dict(best_model_state)\n",
        "\n",
        "                    print(f\"Calculating Predictions for best model in innerfold {inner_fold_idx + 1}, hyperparameter combo {hyperparams}\")\n",
        "\n",
        "                    # Collect predictions and ground truths\n",
        "                    fold_predictions, fold_ground_truths, avg_inference_time = collect_predictions_and_ground_truths_with_speed(\n",
        "                        trained_model, val_loader, device\n",
        "                    )\n",
        "\n",
        "                    # Calculate mAP\n",
        "                    mAP = calculate_map(fold_predictions, fold_ground_truths, iou_threshold=0.5, num_classes=num_classes-1)\n",
        "                    inner_fold_maps.append(mAP)\n",
        "\n",
        "                    # Clean up\n",
        "                    del model, trained_model, optimizer, scheduler\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with hyperparameters {hyperparams}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Average mAP across inner folds\n",
        "            avg_mAP = np.mean(inner_fold_maps) if inner_fold_maps else -1\n",
        "            print(f\"Average mAP for hyperparameters {hyperparams}: {avg_mAP}\")\n",
        "\n",
        "            # Update best hyperparameters\n",
        "            if avg_mAP > best_mAP:\n",
        "                best_mAP = avg_mAP\n",
        "                best_hyperparams = hyperparams\n",
        "\n",
        "        print(f\"\\nBest hyperparameters for Outer Fold {outer_fold_idx + 1}: {best_hyperparams} with mAP: {best_mAP}\")\n",
        "\n",
        "        # Train on full training data with best hyperparameters\n",
        "        train_loader = DataLoader(train_val_subset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "        # Create model with best hyperparameters\n",
        "        model = create_model_frcnn(num_classes=num_classes, **best_hyperparams)\n",
        "        model.to(device)\n",
        "\n",
        "        # Define optimizer and scheduler\n",
        "        params = [p for p in model.parameters() if p.requires_grad]\n",
        "        optimizer = optim.SGD(\n",
        "            params,\n",
        "            lr=0.005,\n",
        "            momentum=0.9,\n",
        "            weight_decay=0.0005\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "        # Train model\n",
        "        trained_model, best_fold_val_loss, best_epoch, total_training_time, best_model_state = train_model_frcnn(\n",
        "            model, train_loader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor,\n",
        "            fold_idx=outer_fold_idx, device=device\n",
        "        )\n",
        "\n",
        "        # Load best model state\n",
        "        trained_model.load_state_dict(best_model_state)\n",
        "\n",
        "        print(f\"Calculating Predictions for best model in outerfold {outer_fold_idx + 1}\")\n",
        "\n",
        "        # Collect predictions and ground truths\n",
        "        fold_predictions, fold_ground_truths, avg_inference_time = collect_predictions_and_ground_truths_with_speed(\n",
        "            trained_model, val_loader, device\n",
        "        )\n",
        "\n",
        "        # Calculate Recall@IoU\n",
        "        recall = calculate_recall(fold_predictions, fold_ground_truths, iou_threshold=0.5)\n",
        "\n",
        "        # Store metrics\n",
        "        fold_metrics = {\n",
        "            'fold_index': outer_fold_idx + 1,\n",
        "            'best_hyperparameters': best_hyperparams,\n",
        "            'validation_loss': best_fold_val_loss,\n",
        "            'best_epoch': best_epoch,\n",
        "            'total_training_time': total_training_time,\n",
        "            'avg_inference_time': avg_inference_time,\n",
        "            'recall_at_iou': recall\n",
        "        }\n",
        "        outer_fold_metrics.append(fold_metrics)\n",
        "\n",
        "        print(f\"Fold {outer_fold_idx + 1} Recall@IoU=0.5: {recall}\")\n",
        "\n",
        "        # Check if current recall is better than the best overall recall\n",
        "        if recall > best_overall_recall:\n",
        "            best_overall_recall = recall\n",
        "            best_overall_model_state = copy.deepcopy(best_model_state)\n",
        "            best_overall_fold = outer_fold_idx + 1\n",
        "            best_overall_hyperparams = best_hyperparams\n",
        "            print(f\"New best overall model found at Fold {best_overall_fold} with Recall@IoU=0.5: {best_overall_recall}\")\n",
        "\n",
        "\n",
        "        # Clean up\n",
        "        del model, trained_model, optimizer, scheduler\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Aggregate final results\n",
        "    recalls = [metrics['recall_at_iou'] for metrics in outer_fold_metrics]\n",
        "    mean_recall = np.mean(recalls)\n",
        "    std_recall = np.std(recalls)\n",
        "\n",
        "    print(\"\\nFinal Results:\")\n",
        "    print(f\"Mean Recall@IoU=0.5 across folds: {mean_recall}\")\n",
        "    print(f\"Recall Standard Deviation: {std_recall}\")\n",
        "\n",
        "    # Save the best overall model to disk\n",
        "    if best_overall_model_state is not None:\n",
        "        model_save_path = f'best_overall_model_fold_{best_overall_fold}.pth'\n",
        "        torch.save(best_overall_model_state, model_save_path)\n",
        "        print(f\"\\nBest overall model saved at {model_save_path} with Recall@IoU=0.5: {best_overall_recall}\")\n",
        "        print(f\"Best hyperparameters: {best_overall_hyperparams}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###Â STN FASTER RCNN\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "###Â RFBNEt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x8jGdqroz8F"
      },
      "source": [
        "### Training models through Stratified Nested k-fold algorithm (HP Tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_CQpVAeoz8F",
        "outputId": "231c69a0-4b13-41ff-9a37-0474952961cc"
      },
      "outputs": [],
      "source": [
        "# Stratified k-fold Cross-validation parameters\n",
        "num_folds = 20\n",
        "num_epochs = 5\n",
        "batch_size = 10\n",
        "\n",
        "# 20-fold Cross-validation parameters (Faster RCNN+STN)\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA GPU available, using the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"No GPU available, using the CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176,
          "referenced_widgets": [
            "a6effd30eba34e668f415d89a1a18dd4",
            "4d24b08bea6e4a62a4c4ff3b40591385",
            "1e1174b30f084453830ddaab58039266",
            "d896857999d443fb9f31833044170071",
            "33bbf49dfbea4d01a58caf24c6fa3275",
            "6c942db2906840cfb844d5bd860a655d",
            "d469c49a88ca493abe0bb231828433c4",
            "6e7a932078bd447198b5c7f5ffb94bb2",
            "7ec901879f284aa199b30ed135233b35",
            "bb60c8ab3e2c4f45b414c2f84461bdc3",
            "f3217f302d5f4e5582bd8ae4a6f8806d"
          ]
        },
        "id": "nHiKXYvToz8F",
        "outputId": "ec0aa478-866c-4ed7-852c-af0cf7f01886"
      },
      "outputs": [],
      "source": [
        "## TRAIN FASTER RCNN\n",
        "\n",
        "# Define hyperparameter for Faster RCNN search space\n",
        "hyperparameter_space = {\n",
        "    'anchor_sizes': [\n",
        "        ((32, 64, 128),),  # Small-to-medium objects\n",
        "        ((64, 128, 256),)  # Medium-to-large objects\n",
        "    ],\n",
        "    'aspect_ratios': [\n",
        "        ((0.5, 1.0, 2.0),),  # Standard aspect ratios\n",
        "        ((1.0, 2.0),)        # Focused on more common aspect ratios\n",
        "    ],\n",
        "    'rpn_nms_thresh': [\n",
        "        0.7,  # Higher threshold for less aggressive suppression\n",
        "        0.5   # Lower threshold for more aggressive suppression, useful for crowded objects\n",
        "    ]\n",
        "}\n",
        "\n",
        "nested_cross_validate_model(\n",
        "    model_name='faster_rcnn',\n",
        "    dataset=train_dataset,\n",
        "    num_outer_folds=5,         # 5 outer folds\n",
        "    num_inner_folds=2,         # 2 inner folds for tuning\n",
        "    num_epochs=20,             # Up to 20 epochs with early stopping patience of 5\n",
        "    device=device,\n",
        "    batch_size=8,              # Optimal batch size for stability and efficiency\n",
        "    hyperparameter_space=hyperparameter_space  # The reduced 2x2x2 grid search\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TRAIN STN FASTER RCNN\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TRAIN RFBNet\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating performance on Testing Set using the best hyper parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preparing the TEST data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "class XrayDataset(Dataset):\n",
        "    def __init__(self, df_images, df_annotations):\n",
        "        self.df_images = df_images\n",
        "        self.df_annotations = df_annotations\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image tensor (ensure it's a PyTorch tensor)\n",
        "        image_tensor = self.df_images.iloc[idx]['normalized_tensor']\n",
        "        if not isinstance(image_tensor, torch.Tensor):\n",
        "            image_tensor = torch.tensor(image_tensor, dtype=torch.float32)\n",
        "\n",
        "        # Get the filename for this image\n",
        "        filename = self.df_images.iloc[idx]['filename']\n",
        "\n",
        "        # Get bounding box and labels for the same image\n",
        "        annotation = self.df_annotations[self.df_annotations['filename'] == filename]\n",
        "\n",
        "        # Convert bounding box to torch tensors\n",
        "        boxes = torch.tensor(annotation[['xmin', 'ymin', 'xmax', 'ymax']].values, dtype=torch.float32)\n",
        "\n",
        "        # Convert one-hot encoded labels back to class indices\n",
        "        one_hot_labels = annotation[['class_gun', 'class_knife', 'class_plier', 'class_scissor', 'class_wrench']].values\n",
        "        labels = torch.tensor([label.argmax() for label in one_hot_labels], dtype=torch.int64)  # Integer labels\n",
        "\n",
        "        # Retrieve weighted_ssq_occlusions for the image\n",
        "        weighted_ssq = self.df_images.iloc[idx]['weighted_ssq_occlusions']\n",
        "\n",
        "        return image_tensor, boxes, labels, weighted_ssq  # Include weighted_ssq in the return\n",
        "\n",
        "# Custom collate function to handle varying number of bounding boxes and include weighted_ssq\n",
        "def custom_collate_fn(batch):\n",
        "    images = []\n",
        "    targets = []  # Store dictionaries for boxes, labels, and weighted_ssq\n",
        "\n",
        "    for item in batch:\n",
        "        images.append(item[0])  # Image tensor\n",
        "\n",
        "        # Store targets as a dictionary with 'boxes', 'labels', and 'weighted_ssq'\n",
        "        targets.append({\n",
        "            'boxes': item[1],   # Bounding boxes\n",
        "            'labels': item[2],  # Class labels (integer encoded)\n",
        "            'weighted_ssq': item[3]  # Weighted sum of squares for occlusion\n",
        "        })\n",
        "\n",
        "    # Stack images normally, since their sizes are uniform\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Return stacked images and list of dictionaries for boxes, labels, and weighted_ssq\n",
        "    return images, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add weighted_ssq values to `df_test_images` based on filename matching\n",
        "# Drop the existing weighted_ssq_occlusions column if it exists in df_test_images before merging\n",
        "if 'weighted_ssq_occlusions' in df_test_images.columns:\n",
        "    df_test_images = df_test_images.drop(columns=['weighted_ssq_occlusions'])\n",
        "\n",
        "# Merge the DataFrames to add the weighted_ssq_occlusions column from weighted_ssq_df\n",
        "df_test_images = df_test_images.merge(weighted_ssq_df, on='filename', how='left')\n",
        "\n",
        "\n",
        "# Initialize normalized_tensor column in df_test_images for processing\n",
        "df_test_images['normalized_tensor'] = None\n",
        "\n",
        "# Process the tensors in mini-batches to normalize\n",
        "batch_size = 32\n",
        "process_in_mini_batches(df_test_images, batch_size)\n",
        "\n",
        "# Apply one-hot encoding to the 'class' column in df_test_annotations\n",
        "# Apply one-hot encoding to the 'class' column in df_test_annotations if it exists\n",
        "if 'class' in df_test_annotations.columns:\n",
        "    df_test_annotations = pd.get_dummies(df_test_annotations, columns=['class'])\n",
        "else:\n",
        "    print(\"'class' column not found in df_test_annotations.\")\n",
        "\n",
        "# Clean up unnecessary columns for final DataLoader creation\n",
        "df_test_images_cleaned = df_test_images.drop(columns=['tensor', 'height', 'width', 'mode'])\n",
        "df_test_annotations_cleaned = df_test_annotations.drop(columns=['depth', 'height', 'width', 'pose', 'truncated', 'difficult'])\n",
        "\n",
        "# Create a unified dataset and DataLoader for all occlusion levels\n",
        "test_dataset = XrayDataset(df_test_images_cleaned, df_test_annotations_cleaned)\n",
        "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False, collate_fn=custom_collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_annotations_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test_images_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect the full raw structure of the first 2 records from the DataLoader, including weights\n",
        "print(\"\\nInspecting the full raw structure of the first 2 records from test_loader:\")\n",
        "\n",
        "# Iterate over the test_loader and print the raw data for the first batch\n",
        "for images, targets in test_loader:\n",
        "    # Print the full structure for the first 2 images in the batch\n",
        "    for i in range(2):\n",
        "        print(f\"\\nRecord {i + 1}:\")\n",
        "        print(\"Image Tensor:\", images[i])  # Full tensor data for the image\n",
        "        print(\"Image Tensor Shape:\", images[i].shape)  # Shape of the image tensor\n",
        "        print(\"Bounding Boxes:\", targets[i]['boxes'])  # Full bounding boxes data\n",
        "        print(\"Bounding Boxes Shape:\", targets[i]['boxes'].shape)  # Shape of bounding boxes tensor\n",
        "        print(\"Labels:\", targets[i]['labels'])  # Full labels tensor\n",
        "        print(\"Labels Shape:\", targets[i]['labels'].shape)  # Shape of labels tensor\n",
        "        print(\"Weighted SSQ Occlusion:\", targets[i]['weighted_ssq'])  # Weighted occlusion value for each image\n",
        "    break  # Stop after the first batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_predictions_and_ground_truths(model, data_loader, device):\n",
        "    \"\"\"\n",
        "    Collects predictions, ground truths, and weighted_ssq_occlusions from the model.\n",
        "    \n",
        "    Parameters:\n",
        "    - model: The trained model.\n",
        "    - data_loader: DataLoader for the dataset.\n",
        "    - device: The device to run the model on.\n",
        "    \n",
        "    Returns:\n",
        "    - all_predictions: List of prediction dictionaries.\n",
        "    - all_ground_truths: List of ground truth dictionaries.\n",
        "    - weighted_ssq_list: List of weighted_ssq_occlusions values.\n",
        "    - images_list: List of images for visualization.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "    weighted_ssq_list = []\n",
        "    images_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            outputs = model(images)\n",
        "\n",
        "            for idx in range(len(images)):\n",
        "                pred_boxes = outputs[idx]['boxes'].cpu().numpy()\n",
        "                pred_scores = outputs[idx]['scores'].cpu().numpy()\n",
        "                pred_labels = outputs[idx]['labels'].cpu().numpy()\n",
        "\n",
        "                gt_boxes = targets[idx]['boxes'].cpu().numpy()\n",
        "                gt_labels = targets[idx]['labels'].cpu().numpy()\n",
        "                weighted_ssq = targets[idx]['weighted_ssq'].item()  # Assuming it's a scalar tensor\n",
        "\n",
        "                all_predictions.append({\n",
        "                    'boxes': pred_boxes,\n",
        "                    'scores': pred_scores,\n",
        "                    'labels': pred_labels\n",
        "                })\n",
        "                all_ground_truths.append({\n",
        "                    'boxes': gt_boxes,\n",
        "                    'labels': gt_labels\n",
        "                })\n",
        "                weighted_ssq_list.append(weighted_ssq)\n",
        "                images_list.append(images[idx].cpu())  # For visualization\n",
        "\n",
        "    return all_predictions, all_ground_truths, weighted_ssq_list, images_list\n",
        "\n",
        "\n",
        "def compute_per_image_metrics(predictions, ground_truths, weighted_ssq_list, num_classes):\n",
        "    \"\"\"\n",
        "    Computes per-image recall and mAP using existing functions.\n",
        "    \n",
        "    Parameters:\n",
        "    - predictions: List of prediction dictionaries.\n",
        "    - ground_truths: List of ground truth dictionaries.\n",
        "    - weighted_ssq_list: List of weighted_ssq_occlusions values.\n",
        "    - num_classes: Number of classes.\n",
        "    \n",
        "    Returns:\n",
        "    - per_image_metrics: List of dictionaries with 'recall', 'mAP', 'weighted_ssq_occlusions'.\n",
        "    \"\"\"\n",
        "    per_image_metrics = []\n",
        "\n",
        "    for i in range(len(predictions)):\n",
        "        pred = [predictions[i]]  # Single-image prediction list\n",
        "        gt = [ground_truths[i]]  # Single-image ground truth list\n",
        "\n",
        "        recall = calculate_recall(pred, gt, iou_threshold=0.5)\n",
        "        mAP = calculate_map(pred, gt, iou_threshold=0.5, num_classes=num_classes)\n",
        "\n",
        "        per_image_metrics.append({\n",
        "            'recall': recall,\n",
        "            'mAP': mAP,\n",
        "            'weighted_ssq': weighted_ssq_list[i]\n",
        "        })\n",
        "\n",
        "    return per_image_metrics\n",
        "\n",
        "def visualize_predictions_single_image(image, prediction, ground_truth, class_names, weighted_ssq):\n",
        "    \"\"\"\n",
        "    Visualizes predictions and ground truths on a single image.\n",
        "    \n",
        "    Parameters:\n",
        "    - image: Tensor image.\n",
        "    - prediction: Dictionary containing 'boxes', 'labels', 'scores'.\n",
        "    - ground_truth: Dictionary containing 'boxes', 'labels'.\n",
        "    - class_names: List of class names.\n",
        "    - weighted_ssq: Weighted SSQ occlusion value.\n",
        "    \"\"\"\n",
        "    img = image\n",
        "    img = to_pil_image(img)\n",
        "    draw = ImageDraw.Draw(img)\n",
        "\n",
        "    # Attempt to load Arial font; fall back to default if unavailable\n",
        "    try:\n",
        "        font = ImageFont.truetype(\"arial.ttf\", size=16)\n",
        "    except IOError:\n",
        "        font = ImageFont.load_default()\n",
        "\n",
        "    # Draw ground truth boxes with enhanced styling\n",
        "    gt_boxes = ground_truth['boxes']\n",
        "    gt_labels = ground_truth['labels']\n",
        "    for box, label in zip(gt_boxes, gt_labels):\n",
        "        draw.rectangle(box.tolist(), outline=\"lime\", width=3)  # Bright green for GT boxes\n",
        "        text_position = (box[0], box[1] - 20)\n",
        "        draw.rectangle([text_position, (box[0] + 80, box[1])], fill=(0, 255, 0, 120))  # Semi-transparent green background\n",
        "        draw.text(text_position, f\"GT: {class_names[label]}\", fill=\"black\", font=font)\n",
        "\n",
        "    # Draw predicted boxes with enhanced styling\n",
        "    pred_boxes = prediction['boxes']\n",
        "    pred_labels = prediction['labels']\n",
        "    pred_scores = prediction['scores']\n",
        "    for box, label, score in zip(pred_boxes, pred_labels, pred_scores):\n",
        "        draw.rectangle(box.tolist(), outline=\"red\", width=2)  # Red for predictions\n",
        "        text_position = (box[0], box[1] - 20)\n",
        "        draw.rectangle([text_position, (box[0] + 100, box[1])], fill=(255, 0, 0, 120))  # Semi-transparent red background\n",
        "        draw.text(text_position, f\"{class_names[label]}: {score:.2f}\", fill=\"white\", font=font)  # White text for contrast\n",
        "\n",
        "    # Enhance contrast and display the image\n",
        "    enhancer = ImageEnhance.Contrast(img)\n",
        "    img_enhanced = enhancer.enhance(1.2)  # Slightly boost contrast\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.title(f\"Weighted SSQ Occlusion: {weighted_ssq}\")\n",
        "    plt.imshow(img_enhanced)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model_with_weighted_ssq(model, data_loader, device, class_names, num_classes, visualization=True, num_images_to_visualize=3):\n",
        "    model.to(device)\n",
        "    predictions, ground_truths, weighted_ssq_list, images_list = collect_predictions_and_ground_truths(model, data_loader, device)\n",
        "\n",
        "    # Compute per-image metrics using existing functions\n",
        "    per_image_metrics = compute_per_image_metrics(predictions, ground_truths, weighted_ssq_list, num_classes)\n",
        "\n",
        "    # Overall metrics\n",
        "    overall_recall = np.mean([m['recall'] for m in per_image_metrics])\n",
        "    overall_mAP = np.mean([m['mAP'] for m in per_image_metrics])\n",
        "\n",
        "    print(f\"Overall Recall: {overall_recall:.4f}, Overall mAP: {overall_mAP:.4f}\")\n",
        "\n",
        "    # Optional visualization\n",
        "    if visualization:\n",
        "        # Select random images to visualize\n",
        "        total_images = len(images_list)\n",
        "        num_images = min(num_images_to_visualize, total_images)\n",
        "        random_indices = random.sample(range(total_images), num_images)\n",
        "        images_to_visualize = [images_list[i] for i in random_indices]\n",
        "        predictions_to_visualize = [predictions[i] for i in random_indices]\n",
        "        ground_truths_to_visualize = [ground_truths[i] for i in random_indices]\n",
        "        weighted_ssq_to_visualize = [weighted_ssq_list[i] for i in random_indices]\n",
        "\n",
        "        for image, pred, gt, weighted_ssq in zip(images_to_visualize, predictions_to_visualize, ground_truths_to_visualize, weighted_ssq_to_visualize):\n",
        "            # Visualize predictions and ground truths\n",
        "            visualize_predictions_single_image(image, pred, gt, class_names, weighted_ssq)\n",
        "\n",
        "    return per_image_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_metrics_vs_occlusion(per_image_metrics):\n",
        "    \"\"\"\n",
        "    Plots recall and mAP against weighted_ssq_occlusions.\n",
        "\n",
        "    Parameters:\n",
        "    - per_image_metrics: List of dictionaries with 'recall', 'mAP', 'weighted_ssq_occlusions'.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract metrics and occlusion values\n",
        "    occlusion_values = [m['weighted_ssq'] for m in per_image_metrics]\n",
        "    recall_values = [m['recall'] for m in per_image_metrics]\n",
        "    mAP_values = [m['mAP'] for m in per_image_metrics]\n",
        "\n",
        "    # Scatter plot for Recall\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(occlusion_values, recall_values, alpha=0.6, edgecolors='k')\n",
        "    plt.title('Recall vs Weighted SSQ Occlusion')\n",
        "    plt.xlabel('Weighted SSQ Occlusion')\n",
        "    plt.ylabel('Recall')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Scatter plot for mAP\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(occlusion_values, mAP_values, alpha=0.6, edgecolors='k', color='orange')\n",
        "    plt.title('mAP vs Weighted SSQ Occlusion')\n",
        "    plt.xlabel('Weighted SSQ Occlusion')\n",
        "    plt.ylabel('mAP')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calling the function for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST FASTER RCNN MODEL\n",
        "\n",
        "# Number of classes (excluding background)\n",
        "num_classes = 5  # As per your dataset\n",
        "\n",
        "# Create class names list\n",
        "class_names = ['gun', 'knife', 'plier', 'scissor', 'wrench']\n",
        "\n",
        "# Create the model with specified hyperparameters\n",
        "model = create_model_frcnn(\n",
        "    num_classes=num_classes + 1,  # +1 for background\n",
        "    anchor_sizes=((64, 128, 256),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),),\n",
        "    rpn_nms_thresh=0.7,\n",
        "    backbone_name='resnet50',\n",
        "    pretrained=True\n",
        ")\n",
        "\n",
        "# Load the best-performing model weights\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    model.load_state_dict(torch.load('overall_best_model_fold_7_epoch_5.pth'))\n",
        "    print(\"CUDA GPU available, using the GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    model.load_state_dict(torch.load('overall_best_model_fold_7_epoch_5.pth', map_location=device))\n",
        "    print(\"No GPU available, using the CPU\")\n",
        "\n",
        "# Move model to device\n",
        "model.to(device)\n",
        "\n",
        "# Test the model\n",
        "per_image_metrics = test_model_with_weighted_ssq(\n",
        "    model=model,\n",
        "    data_loader=test_loader,  # Use your test_loader here\n",
        "    device=device,\n",
        "    class_names=class_names,\n",
        "    num_classes=num_classes,\n",
        "    visualization=True\n",
        ")\n",
        "\n",
        "# Generate graphs\n",
        "plot_metrics_vs_occlusion(per_image_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST FASTER RCNN MODEL\n",
        "\n",
        "def nested_cross_validate_model_stn_frcnn(model_name, dataset, num_outer_folds=5, num_inner_folds=3, num_epochs=5, **kwargs):\n",
        "    \"\"\"\n",
        "    Performs Nested Stratified K-Fold Cross-Validation with hyperparameter tuning.\n",
        "\n",
        "    Parameters:\n",
        "    - model_name (str): Name of the model ('faster_rcnn').\n",
        "    - dataset (torch.utils.data.Dataset): The dataset to perform cross-validation on.\n",
        "    - num_outer_folds (int): Number of outer folds for cross-validation.\n",
        "    - num_inner_folds (int): Number of inner folds for hyperparameter tuning.\n",
        "    - num_epochs (int): Number of epochs to train in each fold.\n",
        "    - kwargs: Additional parameters.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    # Extract the parameters from kwargs\n",
        "    device = kwargs.get('device')\n",
        "    batch_size = kwargs.get('batch_size', 10)\n",
        "    # Mapping class indices to class names\n",
        "    class_mapping = {0: 'gun', 1: 'knife', 2: 'plier', 3: 'scissor', 4: 'wrench'}\n",
        "    num_classes = len(class_mapping) + 1  # +1 for background\n",
        "\n",
        "    # Prepare class weights tensor\n",
        "    class_weights = {'gun': 0.1190057659742028,\n",
        "                     'knife': 0.1961004103944656,\n",
        "                     'plier': 0.08725201808509703,\n",
        "                     'scissor': 0.43879561915616316,\n",
        "                     'wrench': 0.15884618639007145}\n",
        "\n",
        "    class_weights_tensor = torch.ones(num_classes)  # Initialize with ones for all classes including background\n",
        "\n",
        "    # Map class indices to weights\n",
        "    for class_index, class_name in class_mapping.items():\n",
        "        class_weight = class_weights[class_name]\n",
        "        class_weights_tensor[class_index] = class_weight\n",
        "\n",
        "    class_weights_tensor = class_weights_tensor.to(device)\n",
        "\n",
        "    # Outer Stratified K-Fold Split\n",
        "    outer_folds = stratified_k_fold_split(dataset, num_outer_folds)\n",
        "\n",
        "    # To store metrics across folds\n",
        "    outer_fold_metrics = []\n",
        "\n",
        "    for outer_fold_idx in range(num_outer_folds):\n",
        "        print(f\"\\nOuter Fold {outer_fold_idx + 1}/{num_outer_folds}\")\n",
        "        # Split data for outer fold\n",
        "        val_indices = outer_folds[outer_fold_idx]\n",
        "        train_val_indices = [idx for i, fold in enumerate(outer_folds) if i != outer_fold_idx for idx in fold]\n",
        "        train_val_subset = Subset(dataset, train_val_indices)\n",
        "        val_subset = Subset(dataset, val_indices)\n",
        "\n",
        "        # Inner Stratified K-Fold for Hyperparameter Tuning\n",
        "        inner_folds = stratified_k_fold_split(train_val_subset, num_inner_folds)\n",
        "        best_hyperparams = None\n",
        "        best_mAP = -1\n",
        "\n",
        "        # Access the hyperparameter space from kwargs\n",
        "        hyperparameter_space = kwargs.get('hyperparameter_space')\n",
        "\n",
        "        hyperparameter_combinations = [\n",
        "            {\n",
        "                'anchor_sizes': anchor_sizes,\n",
        "            }\n",
        "            for anchor_sizes in hyperparameter_space['anchor_sizes']\n",
        "        ]\n",
        "\n",
        "        for hyperparams in hyperparameter_combinations:\n",
        "            print(f\"\\nTesting hyperparameters: {hyperparams}\")\n",
        "            inner_fold_maps = []\n",
        "\n",
        "            for inner_fold_idx in range(num_inner_folds):\n",
        "                # Split data for inner fold\n",
        "                print(f\"\\nInner Fold {inner_fold_idx + 1}/{num_inner_folds}\")\n",
        "                inner_val_indices = inner_folds[inner_fold_idx]\n",
        "                inner_train_indices = [idx for i, fold in enumerate(inner_folds) if i != inner_fold_idx for idx in fold]\n",
        "                inner_train_subset = Subset(train_val_subset, inner_train_indices)\n",
        "                inner_val_subset = Subset(train_val_subset, inner_val_indices)\n",
        "\n",
        "                # Create DataLoaders\n",
        "                train_loader = DataLoader(inner_train_subset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "                val_loader = DataLoader(inner_val_subset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "                # Create model\n",
        "                try:\n",
        "                    model = create_model(num_classes=num_classes, aspect_ratios=aspect_ratios, rpn_nms_thresh=rpn_nms_thresh, backbone_name='resnet50', pretrained=True, stn_type=stn_type, **hyperparams)\n",
        "                    model.to(device)\n",
        "\n",
        "                    # Define optimizer and scheduler\n",
        "                    params = [p for p in model.parameters() if p.requires_grad]\n",
        "                    optimizer = optim.SGD(\n",
        "                        params,\n",
        "                        lr=0.005,  # Default value if not passed\n",
        "                        momentum=0.9,\n",
        "                        weight_decay=0.0005\n",
        "                    )\n",
        "                    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "                    # Train model\n",
        "                    trained_model, _, _, total_training_time, best_model_state = train_model(\n",
        "                        model, train_loader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor,\n",
        "                        fold_idx=inner_fold_idx, device=device\n",
        "                    )\n",
        "\n",
        "                    # Load best model state\n",
        "                    trained_model.load_state_dict(best_model_state)\n",
        "\n",
        "                    print(f\"Calculating Predictions for best model in innerfold {inner_fold_idx + 1}, hyperparameter combo {hyperparams}\")\n",
        "\n",
        "                    # Collect predictions and ground truths\n",
        "                    fold_predictions, fold_ground_truths, avg_inference_time = collect_predictions_and_ground_truths_with_speed(\n",
        "                        trained_model, val_loader, device\n",
        "                    )\n",
        "\n",
        "                    # Calculate mAP\n",
        "                    mAP = calculate_map(fold_predictions, fold_ground_truths, iou_threshold=0.5, num_classes=num_classes-1)\n",
        "                    inner_fold_maps.append(mAP)\n",
        "\n",
        "                    # Clean up\n",
        "                    del model, trained_model, optimizer, scheduler\n",
        "                    torch.cuda.empty_cache()\n",
        "                    gc.collect()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error with hyperparameters {hyperparams}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Average mAP across inner folds\n",
        "            avg_mAP = np.mean(inner_fold_maps) if inner_fold_maps else -1\n",
        "            print(f\"Average mAP for hyperparameters {hyperparams}: {avg_mAP}\")\n",
        "\n",
        "            # Update best hyperparameters\n",
        "            if avg_mAP > best_mAP:\n",
        "                best_mAP = avg_mAP\n",
        "                best_hyperparams = hyperparams\n",
        "\n",
        "        print(f\"\\nBest hyperparameters for Outer Fold {outer_fold_idx + 1}: {best_hyperparams} with mAP: {best_mAP}\")\n",
        "\n",
        "        # Train on full training data with best hyperparameters\n",
        "        train_loader = DataLoader(train_val_subset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate_fn)\n",
        "\n",
        "        # Create model with best hyperparameters\n",
        "        model = create_model(num_classes=num_classes, aspect_ratios=aspect_ratios, rpn_nms_thresh=rpn_nms_thresh, backbone_name='resnet50', pretrained=True, stn_type=stn_type,**best_hyperparams)\n",
        "        model.to(device)\n",
        "\n",
        "        # Define optimizer and scheduler\n",
        "        params = [p for p in model.parameters() if p.requires_grad]\n",
        "        optimizer = optim.SGD(\n",
        "            params,\n",
        "            lr=0.005,\n",
        "            momentum=0.9,\n",
        "            weight_decay=0.0005\n",
        "        )\n",
        "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "        # Train model\n",
        "        trained_model, best_fold_val_loss, best_epoch, total_training_time, best_model_state = train_model(\n",
        "            model, train_loader, val_loader, optimizer, scheduler, num_epochs, class_weights_tensor,\n",
        "            fold_idx=outer_fold_idx, device=device\n",
        "        )\n",
        "\n",
        "        # Load best model state\n",
        "        trained_model.load_state_dict(best_model_state)\n",
        "\n",
        "        print(f\"Calculating Predictions for best model in outerfold {outer_fold_idx + 1}\")\n",
        "\n",
        "        # Collect predictions and ground truths\n",
        "        fold_predictions, fold_ground_truths, avg_inference_time = collect_predictions_and_ground_truths_with_speed(\n",
        "            trained_model, val_loader, device\n",
        "        )\n",
        "\n",
        "        # Calculate Recall@IoU\n",
        "        recall = calculate_recall(fold_predictions, fold_ground_truths, iou_threshold=0.5)\n",
        "\n",
        "        # Store metrics\n",
        "        fold_metrics = {\n",
        "            'fold_index': outer_fold_idx + 1,\n",
        "            'best_hyperparameters': best_hyperparams,\n",
        "            'validation_loss': best_fold_val_loss,\n",
        "            'best_epoch': best_epoch,\n",
        "            'total_training_time': total_training_time,\n",
        "            'avg_inference_time': avg_inference_time,\n",
        "            'recall_at_iou': recall\n",
        "        }\n",
        "        outer_fold_metrics.append(fold_metrics)\n",
        "\n",
        "        print(f\"Fold {outer_fold_idx + 1} Recall@IoU=0.5: {recall}\")\n",
        "\n",
        "        # Check if current recall is better than the best overall recall\n",
        "        if recall > best_overall_recall:\n",
        "            best_overall_recall = recall\n",
        "            best_overall_model_state = copy.deepcopy(best_model_state)\n",
        "            best_overall_fold = outer_fold_idx + 1\n",
        "            best_overall_hyperparams = best_hyperparams\n",
        "            print(f\"New best overall model found at Fold {best_overall_fold} with Recall@IoU=0.5: {best_overall_recall}\")\n",
        "\n",
        "\n",
        "        # Clean up\n",
        "        del model, trained_model, optimizer, scheduler\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Aggregate final results\n",
        "    recalls = [metrics['recall_at_iou'] for metrics in outer_fold_metrics]\n",
        "    mean_recall = np.mean(recalls)\n",
        "    std_recall = np.std(recalls)\n",
        "\n",
        "    print(\"\\nFinal Results:\")\n",
        "    print(f\"Mean Recall@IoU=0.5 across folds: {mean_recall}\")\n",
        "    print(f\"Recall Standard Deviation: {std_recall}\")\n",
        "\n",
        "    # Save the best overall model to disk\n",
        "    if best_overall_model_state is not None:\n",
        "        model_save_path = f'best_overall_model_fold_{best_overall_fold}.pth'\n",
        "        torch.save(best_overall_model_state, model_save_path)\n",
        "        print(f\"\\nBest overall model saved at {model_save_path} with Recall@IoU=0.5: {best_overall_recall}\")\n",
        "        print(f\"Best hyperparameters: {best_overall_hyperparams}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#HP tuning Faster RCNN STN\n",
        "# # Define hyperparameter for Faster RCNN  STN search space\n",
        "\n",
        "hyperparameter_space = {\n",
        "    'anchor_sizes': [\n",
        "        ((32, 64, 128),), # flexibility in detecting small, medium, and somewhat large objects\n",
        "        (64, 128, 256), # focses on medium to large objects\n",
        "\n",
        "    ]\n",
        "\n",
        "}\n",
        "\n",
        "aspect_ratios = ((0.5, 1.0, 2.0),)\n",
        "rpn_nms_thresh = 0.7\n",
        "stn_type ='affine'\n",
        "\n",
        "nested_cross_validate_model_stn_frcnn(\n",
        "    model_name='stn_frcnn',\n",
        "    dataset=train_dataset,\n",
        "    num_outer_folds=5,\n",
        "    num_inner_folds=3,\n",
        "    num_epochs=10,\n",
        "    device=device,\n",
        "    batch_size=1,  # Adjust batch size based on your resources\n",
        "    hyperparameter_space = hyperparameter_space,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## TEST STN FASTER RCNN MODEL\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "sml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1344622f364849879213d96fc5de7115": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_284f1e73a23f46229baf8957730b369e",
              "IPY_MODEL_dca86c5fa37d4876acd08f8cb48b5cc4",
              "IPY_MODEL_cff013a727974e22b3bb22f3cd5a510b"
            ],
            "layout": "IPY_MODEL_563e936c5b5442b599a0a11214fd9dba"
          }
        },
        "17f6edc3809f41fe87141145920dd0dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "1e1174b30f084453830ddaab58039266": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e7a932078bd447198b5c7f5ffb94bb2",
            "max": 222,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ec901879f284aa199b30ed135233b35",
            "value": 128
          }
        },
        "284f1e73a23f46229baf8957730b369e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32936de581544ea7aa701c77406784c1",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_aed054e4b0b8441fb50ea523242b9fc2",
            "value": "Epochâ€‡1/5:â€‡â€‡â€‡0%"
          }
        },
        "28f0c893e5e74ac68a91b2754105123b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3217364beb2c4d68b43cf5562eba96d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "322426750dc94ffa838dd2b4bca31607": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32936de581544ea7aa701c77406784c1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "33bbf49dfbea4d01a58caf24c6fa3275": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "4465c0aa123f4fd195b71dfc7032ce61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85d46992ef4c4abc84f7cdd831b002ae",
              "IPY_MODEL_d005356d58e544518ce40e9d4c087d61",
              "IPY_MODEL_666fd3bf6e454266a52f6d25390b06c9"
            ],
            "layout": "IPY_MODEL_c92578d7e5734b538179f81cf6615c45"
          }
        },
        "4ba9b370a43047049ad84f756c68f70a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d24b08bea6e4a62a4c4ff3b40591385": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c942db2906840cfb844d5bd860a655d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d469c49a88ca493abe0bb231828433c4",
            "value": "Epochâ€‡1/2:â€‡â€‡58%"
          }
        },
        "563e936c5b5442b599a0a11214fd9dba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "5d183a04763c44cdbc3e3c79903a39a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62a2a4575f8e4299973a2c6a8a5a3283": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "666fd3bf6e454266a52f6d25390b06c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4169eaa0f5a479b9375abed001c6c17",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_62a2a4575f8e4299973a2c6a8a5a3283",
            "value": "â€‡13/50â€‡[01:43&lt;04:37,â€‡â€‡7.51s/it,â€‡Totalâ€‡Loss=0.0957]"
          }
        },
        "6c942db2906840cfb844d5bd860a655d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7a932078bd447198b5c7f5ffb94bb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ec901879f284aa199b30ed135233b35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f9d61aede9b42ada7052cafdd2e7bea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "85d46992ef4c4abc84f7cdd831b002ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d183a04763c44cdbc3e3c79903a39a6",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4ba9b370a43047049ad84f756c68f70a",
            "value": "Epochâ€‡1/1:â€‡â€‡26%"
          }
        },
        "87c002e4e16342f5858b648577a368d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28f0c893e5e74ac68a91b2754105123b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_ef2e7f082c454b78a48aa2d4cc017cbb",
            "value": "Epochâ€‡1/5:â€‡â€‡â€‡0%"
          }
        },
        "8b616f1933794926b4f38c08061c48ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f7aace8b98742fd83555192cf8b4717": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6effd30eba34e668f415d89a1a18dd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4d24b08bea6e4a62a4c4ff3b40591385",
              "IPY_MODEL_1e1174b30f084453830ddaab58039266",
              "IPY_MODEL_d896857999d443fb9f31833044170071"
            ],
            "layout": "IPY_MODEL_33bbf49dfbea4d01a58caf24c6fa3275"
          }
        },
        "ac5ee86c2d38466d8182203bdbc9af9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aed054e4b0b8441fb50ea523242b9fc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb60c8ab3e2c4f45b414c2f84461bdc3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80108bae03b460db1361b0bb282ad2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d06147ec99814929909d6d6de6dddec2",
            "max": 1204,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3217364beb2c4d68b43cf5562eba96d6",
            "value": 0
          }
        },
        "c92578d7e5734b538179f81cf6615c45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "cb2df77cd8354a2d87faa99cf86ddb3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb36545c775f4a8cbbdca3d6424779f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cff013a727974e22b3bb22f3cd5a510b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f7aace8b98742fd83555192cf8b4717",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_cb36545c775f4a8cbbdca3d6424779f2",
            "value": "â€‡0/1204â€‡[00:11&lt;?,â€‡?it/s]"
          }
        },
        "d005356d58e544518ce40e9d4c087d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac5ee86c2d38466d8182203bdbc9af9e",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f9d61aede9b42ada7052cafdd2e7bea",
            "value": 13
          }
        },
        "d06147ec99814929909d6d6de6dddec2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d469c49a88ca493abe0bb231828433c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8832cd09ab949b282f7bbd982978c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_322426750dc94ffa838dd2b4bca31607",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e51ece58127c4a4cafc40f3c379a2987",
            "value": "â€‡0/1204â€‡[00:11&lt;?,â€‡?it/s]"
          }
        },
        "d896857999d443fb9f31833044170071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb60c8ab3e2c4f45b414c2f84461bdc3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_f3217f302d5f4e5582bd8ae4a6f8806d",
            "value": "â€‡128/222â€‡[2:39:42&lt;1:58:33,â€‡75.67s/it,â€‡Totalâ€‡Loss=0.4379,â€‡Cls=0.1066,â€‡BoxReg=0.0485,â€‡Obj=0.1026,â€‡RPNBoxReg=0.1802]"
          }
        },
        "dca86c5fa37d4876acd08f8cb48b5cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb2df77cd8354a2d87faa99cf86ddb3e",
            "max": 1204,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b616f1933794926b4f38c08061c48ca",
            "value": 0
          }
        },
        "e4169eaa0f5a479b9375abed001c6c17": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e51ece58127c4a4cafc40f3c379a2987": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef2e7f082c454b78a48aa2d4cc017cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f3217f302d5f4e5582bd8ae4a6f8806d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc0d92dee8ef49578dc593d27ce7aa7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87c002e4e16342f5858b648577a368d6",
              "IPY_MODEL_c80108bae03b460db1361b0bb282ad2a",
              "IPY_MODEL_d8832cd09ab949b282f7bbd982978c35"
            ],
            "layout": "IPY_MODEL_17f6edc3809f41fe87141145920dd0dd"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
