{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_image\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms import ColorJitter\n",
    "from torchvision.transforms import GaussianBlur\n",
    "from matplotlib.patches import Rectangle\n",
    "import torchvision.transforms.functional as TF\n",
    "# from torchvision.transforms import functional as F\n",
    "import matplotlib.patches as patches\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import itertools\n",
    "\n",
    "from math import sqrt as sqrt\n",
    "from itertools import product as product\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import torch.nn.init as init\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torch.backends.cudnn as cudn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_path = \"C:/Users/Gaura/OneDrive/Desktop/GAURAV/Unimleb/COMP90051/gt/02/Statisitcal_Machine_Learning_Project/\"       ## Change this location\n",
    "# pickle_path = \"C:/Users/Gaura/OneDrive/Desktop/GAURAV/Unimleb/COMP90051/gt/02/Statisitcal_Machine_Learning_Project/\"       ## Change this location\n",
    "\n",
    "pickle_file_path = os.path.join(pickle_path, 'WSR_train_data.pkl')\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    wsr_data = pickle.load(file)\n",
    "\n",
    "filenames = wsr_data['file_names']  # List of filenames\n",
    "resized_images = wsr_data['images']  # Corresponding image tensors (resized)\n",
    "object_location = wsr_data['object_locations'] # Corresponding to new bb co-ordinates\n",
    "\n",
    "\n",
    "pickle_path = \"C:/Users/Gaura/OneDrive/Desktop/GAURAV/Unimleb/COMP90051/gt/02/Statisitcal_Machine_Learning_Project/\"       ## Change this location\n",
    "pickle_file_path = os.path.join(pickle_path, 'train_dataset.pkl')\n",
    "\n",
    "# Load the pickle file\n",
    "with open(pickle_file_path, 'rb') as file:\n",
    "    all_data = pickle.load(file)\n",
    "\n",
    "# filenames = data['file_names']  # List of filenames\n",
    "# resized_images = data['images']  # Corresponding image tensors (resized)\n",
    "# object_location = data['object_locations'] # Corresponding to new bb co-ordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_to_np = lambda tse : tse.permute(1, 2, 0).numpy()\n",
    "np_to_torch = lambda npa : torch.from_numpy(npa).permute(2, 0, 1)\n",
    "\n",
    "\n",
    "def image_resizer(img, new_dim):\n",
    "    new_h, new_w = new_dim\n",
    "    resize_transform = transforms.Resize((new_h, new_w))\n",
    "    image_resized = resize_transform(transforms.ToPILImage()(img))  # Convert to PIL image and resize\n",
    "    return transforms.ToTensor()(image_resized)\n",
    "\n",
    "def object_location_resizer(objs, old_dim = (584, 688),  new_dim = (512, 512)):\n",
    "    old_height, old_width = old_dim\n",
    "    new_height, new_width = new_dim\n",
    "    height_ratio = new_height/old_height\n",
    "    width_ratio = new_width/old_width\n",
    "\n",
    "    ratioLst = [width_ratio, height_ratio, width_ratio, height_ratio]\n",
    "    new_boxes = []\n",
    "    for box in objs:\n",
    "        box = box.tolist()\n",
    "        box = [abs(int(a * b)) for a, b in zip(box, ratioLst)] \n",
    "        new_boxes.append(box)\n",
    "    return torch.tensor(new_boxes)\n",
    "\n",
    "def show_image(pic, obj=False, class_code = False, convert=False, font_size=12):\n",
    "    if isinstance(pic, torch.Tensor):\n",
    "        pic = torch_to_np(pic)\n",
    "    if convert:\n",
    "        pic = image.imread(pic)\n",
    "    \n",
    "    pyplot.imshow(pic)\n",
    "    if torch.is_tensor(obj):\n",
    "        code_to_obj = dict(zip([q for q in range(5)], [\"gun\", \"knife\", \"plier\", \"scissor\", \"wrench\"]))\n",
    "        # print(code_to_obj)\n",
    "        temp = obj.tolist()\n",
    "        obj = {}\n",
    "        for i, bb in enumerate(temp):\n",
    "            # obj[i] = [int(class_code[i]), bb]\n",
    "            obj[i] = [code_to_obj[int(class_code[i])], bb]\n",
    "    print(f\"dtype : {pic.dtype}\")\n",
    "    print(f\"Shape : {pic.shape}\")\n",
    "    print(f\"Aspect Ratio (h/w) : {round(pic.shape[0] / pic.shape[1], 2)}\")\n",
    "    if obj:\n",
    "        labels = []\n",
    "        bound_box = []\n",
    "        for k, v in obj.items():\n",
    "            labels.append(v[0])\n",
    "            bound_box.append(v[1])\n",
    "        \n",
    "        # Draw bounding boxes and labels\n",
    "        for i, bbox in enumerate(bound_box):\n",
    "            x_min, y_min, x_max, y_max = bbox\n",
    "            width, height = x_max - x_min, y_max - y_min\n",
    "            \n",
    "            # Add a rectangle patch for each bounding box\n",
    "            rect = Rectangle((x_min, y_min), width, height, linewidth=3, edgecolor='red', facecolor='none')\n",
    "            pyplot.gca().add_patch(rect)\n",
    "            \n",
    "            # Add text label above the bounding box\n",
    "            pyplot.text(x_min, y_min - 5, labels[i], color='white', fontsize=font_size, \n",
    "                        bbox=dict(facecolor='red', edgecolor='red', pad=2.0))            \n",
    "\n",
    "sorted_join = lambda st : \"\".join(sorted(list(set(\"\".join(sorted(st))))))\n",
    "\n",
    "get_area = lambda arbb : (arbb[2]-arbb[0])*(arbb[3]-arbb[1])\n",
    "\n",
    "def calculate_intersection_area(box1, box2, get_intersection_box = False):\n",
    "    # Box format: [x_min, y_min, x_max, y_max]\n",
    "    x_min1, y_min1, x_max1, y_max1 = box1\n",
    "    x_min2, y_min2, x_max2, y_max2 = box2\n",
    "    \n",
    "    inter_x_min = max(x_min1, x_min2)\n",
    "    inter_y_min = max(y_min1, y_min2)\n",
    "    inter_x_max = min(x_max1, x_max2)\n",
    "    inter_y_max = min(y_max1, y_max2)\n",
    "    \n",
    "    inter_width = max(0, inter_x_max - inter_x_min)\n",
    "    inter_height = max(0, inter_y_max - inter_y_min)\n",
    "    \n",
    "    intersection_area = inter_width * inter_height\n",
    "    if (get_intersection_box):\n",
    "        return intersection_area, [inter_x_min, inter_y_min, inter_x_max, inter_y_max]\n",
    "        \n",
    "    return intersection_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_512 = [(image_resizer(tup[0], (512, 512)), object_location_resizer(tup[1], old_dim = (584, 688), new_dim = (512, 512)), tup[2]) for tup in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, dilation=1, groups=1, relu=True, bn=True, bias=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.out_channels = out_planes\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes,eps=1e-5, momentum=0.01, affine=True) if bn else None\n",
    "        self.relu = nn.ReLU(inplace=True) if relu else None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            x = self.bn(x)\n",
    "        if self.relu is not None:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicRFB(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1,map_reduce=8):\n",
    "        super(BasicRFB, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.out_channels = out_planes\n",
    "        inter_planes = in_planes // map_reduce\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),\n",
    "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n",
    "                )\n",
    "        self.branch1 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n",
    "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
    "                )\n",
    "        self.branch2 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=3, stride=1, padding=1),\n",
    "                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=3, stride=stride, padding=1),\n",
    "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n",
    "                )\n",
    "        self.branch3 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=(7,1), stride=stride, padding=(3,0)),\n",
    "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
    "                )\n",
    "\n",
    "        self.ConvLinear = BasicConv(8*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n",
    "        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "\n",
    "        out = torch.cat((x0,x1,x2,x3),1)\n",
    "        out = self.ConvLinear(out)\n",
    "        short = self.shortcut(x)\n",
    "        out = out*self.scale + short\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BasicRFB_c(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1,map_reduce=8):\n",
    "        super(BasicRFB_c, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.out_channels = out_planes\n",
    "        inter_planes = in_planes // map_reduce\n",
    "        self.branch0 = nn.Sequential(\n",
    "                BasicConv(in_planes, 2*inter_planes, kernel_size=1, stride=stride),\n",
    "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n",
    "                )\n",
    "        self.branch1 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, 2*inter_planes, kernel_size=(3,3), stride=stride, padding=(1,1)),\n",
    "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
    "                )\n",
    "        self.branch2 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, (inter_planes//2)*3, kernel_size=(1,7), stride=1, padding=(0,3)),\n",
    "                BasicConv((inter_planes//2)*3, 2*inter_planes, kernel_size=(7,1), stride=stride, padding=(3,0)),\n",
    "                BasicConv(2*inter_planes, 2*inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
    "                )\n",
    "\n",
    "        self.ConvLinear = BasicConv(6*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n",
    "        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "\n",
    "        out = torch.cat((x0,x1,x2),1)\n",
    "        out = self.ConvLinear(out)\n",
    "        short = self.shortcut(x)\n",
    "        out = out*self.scale + short\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class BasicRFB_a(nn.Module):\n",
    "\n",
    "    def __init__(self, in_planes, out_planes, stride=1, scale = 0.1):\n",
    "        super(BasicRFB_a, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.out_channels = out_planes\n",
    "        inter_planes = in_planes //8\n",
    "\n",
    "\n",
    "        self.branch0 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=1,relu=False)\n",
    "                )\n",
    "        self.branch1 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
    "                )\n",
    "        self.branch2 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=3, dilation=3, relu=False)\n",
    "                )\n",
    "        self.branch3 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=(3,1), stride=1, padding=(1,0)),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n",
    "                )\n",
    "        self.branch4 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=5, dilation=5, relu=False)\n",
    "                )\n",
    "        self.branch5 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(1,3), stride=1, padding=(0,1)),\n",
    "                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(3,1), stride=stride, padding=(1,0)),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
    "                )\n",
    "\n",
    "        self.branch6 = nn.Sequential(\n",
    "                BasicConv(in_planes, inter_planes//2, kernel_size=1, stride=1),\n",
    "                BasicConv(inter_planes//2, (inter_planes//4)*3, kernel_size=(3,1), stride=1, padding=(1,0)),\n",
    "                BasicConv((inter_planes//4)*3, inter_planes, kernel_size=(1,3), stride=stride, padding=(0,1)),\n",
    "                BasicConv(inter_planes, inter_planes, kernel_size=3, stride=1, padding=7, dilation=7, relu=False)\n",
    "                )\n",
    "        self.ConvLinear = BasicConv(7*inter_planes, out_planes, kernel_size=1, stride=1, relu=False)\n",
    "        self.shortcut = BasicConv(in_planes, out_planes, kernel_size=1, stride=stride, relu=False)\n",
    "        self.relu = nn.ReLU(inplace=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        x4 = self.branch4(x)\n",
    "        x5 = self.branch5(x)\n",
    "        x6 = self.branch6(x)\n",
    "\n",
    "        out = torch.cat((x0,x1,x2,x3,x4,x5,x6),1)\n",
    "        out = self.ConvLinear(out)\n",
    "        short = self.shortcut(x)\n",
    "        out = out*self.scale + short\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class RFBNet(nn.Module): \n",
    "\n",
    "    def __init__(self, img_size, base, extras, head, num_classes):\n",
    "        print(\"Entered RFB========\")\n",
    "        super(RFBNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        if img_size == 300:\n",
    "            self.indicator = 3\n",
    "        elif img_size == 512:\n",
    "            self.indicator = 5\n",
    "        else:\n",
    "            self.indicator = 5\n",
    "            print(f\"GAK PASS RFBNet__init__ : indicator {self.indicator} Error: Sorry only RFB300 and RFB512 are supported!\")\n",
    "            # return\n",
    "        self.base = nn.ModuleList(base)\n",
    "        # Layer learns to scale the l2 normalized features from conv4_3\n",
    "\n",
    "        self.reduce = BasicConv(512,256,kernel_size=1,stride=1)\n",
    "        self.up_reduce = BasicConv(1024,256,kernel_size=1,stride=1)\n",
    "\n",
    "        self.rfb = BasicRFB_a(512, 512, stride=1, scale=1.0)\n",
    "        self.extras = nn.ModuleList(extras)\n",
    "\n",
    "        self.loc = nn.ModuleList(head[0])\n",
    "        self.conf = nn.ModuleList(head[1])\n",
    "\n",
    "\n",
    "    def forward(self, x, phase=''):\n",
    "        \"\"\"Applies network layers and ops on input image(s) x.\n",
    "        Args:\n",
    "            x: input image or batch of images. Shape: [batch,3*batch,300,300].\n",
    "        Return:\n",
    "            Depending on phase:\n",
    "            test:\n",
    "                Variable(tensor) of output class label predictions,\n",
    "                confidence score, and corresponding location predictions for\n",
    "                each object detected. Shape: [batch,topk,7]\n",
    "            train:\n",
    "                list of concat outputs from:\n",
    "                    1: confidence layers, Shape: [batch*num_priors,num_classes]\n",
    "                    2: localization layers, Shape: [batch,num_priors*4]\n",
    "                    3: priorbox layers, Shape: [2,num_priors*4]\n",
    "        \"\"\"\n",
    "        sources = list()\n",
    "        loc = list()\n",
    "        conf = list()\n",
    "\n",
    "        # apply vgg up to conv4_3 relu\n",
    "        for k in range(23):\n",
    "            x = self.base[k](x)\n",
    "\n",
    "        s1 = self.reduce(x)\n",
    "\n",
    "        # apply vgg up to fc7\n",
    "        for k in range(23, len(self.base)):\n",
    "            x = self.base[k](x)\n",
    "        s2 = self.up_reduce(x)\n",
    "        s2 = F.upsample(s2, scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        s = torch.cat((s1,s2),1) # ERROR HERE\n",
    "\n",
    "        ss = self.rfb(s)\n",
    "        sources.append(ss)\n",
    "\n",
    "        # apply extra layers and cache source layer outputs\n",
    "        for k, v in enumerate(self.extras):\n",
    "            x = v(x)\n",
    "            if k < self.indicator or k%2 ==0:\n",
    "                sources.append(x)\n",
    "\n",
    "        # apply multibox head to source layers\n",
    "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
    "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
    "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
    "\n",
    "        #print([o.size() for o in loc])\n",
    "\n",
    "\n",
    "        # location predictions\n",
    "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
    "        loc = loc.view(loc.size(0), -1, 4)\n",
    "\n",
    "        # class confidence scores\n",
    "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
    "\n",
    "        if phase == 'eval' or phase == 'test':\n",
    "            conf = F.softmax(conf.view(-1, self.num_classes), dim=-1)\n",
    "        else:\n",
    "            conf = conf.view(conf.size(0), -1, self.num_classes)\n",
    "\n",
    "        return (loc, conf)\n",
    "\n",
    "    def load_model(self, model_file):\n",
    "        print('Loading model weights from', model_file)\n",
    "        self.load_state_dict(torch.load(model_file))\n",
    "        print('Done.')\n",
    "\n",
    "    def load_backbone(self, model_file):\n",
    "        print('Loading backbone/base model weights from', model_file)\n",
    "        self.base.load_state_dict(torch.load(model_file))\n",
    "        print('Done.')\n",
    "\n",
    "    def init_weights(self, base=False, rfb=True, extras=True, loc=True, conf=True, reduce=True, up_reduce=True):\n",
    "\n",
    "        def weights_init(m):\n",
    "            for key in m.state_dict():\n",
    "                if key.split('.')[-1] == 'weight':\n",
    "                    if 'conv' in key:\n",
    "                        init.kaiming_normal_(m.state_dict()[key], mode='fan_out')\n",
    "                    if 'bn' in key:\n",
    "                        m.state_dict()[key][...] = 1\n",
    "                elif key.split('.')[-1] == 'bias':\n",
    "                    m.state_dict()[key][...] = 0\n",
    "\n",
    "        print('Initializing weights...')\n",
    "        print('Init: base: {}, rfb: {}, extras: {}, loc: {}, conf: {}'.format(base, rfb, extras, loc, conf))\n",
    "        if base: self.base.apply(weights_init)\n",
    "        if reduce: self.reduce.apply(weights_init)\n",
    "        if up_reduce: self.up_reduce.apply(weights_init)\n",
    "        if rfb: self.rfb.apply(weights_init)\n",
    "        if extras: self.extras.apply(weights_init)\n",
    "        if loc: self.loc.apply(weights_init)\n",
    "        if conf: self.conf.apply(weights_init)\n",
    "        print('Done.')\n",
    "\n",
    "# This function is derived from torchvision VGG make_layers()\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py\n",
    "def vgg(cfg, i, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        elif v == 'C':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
    "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
    "    layers += [pool5, conv6,\n",
    "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
    "    return layers\n",
    "\n",
    "base = {\n",
    "    '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
    "            512, 512, 512],\n",
    "    '512': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
    "            512, 512, 512],\n",
    "}\n",
    "\n",
    "\n",
    "def add_extras(size, cfg, i, batch_norm=False):\n",
    "    # Extra layers added to VGG for feature scaling\n",
    "    layers = []\n",
    "    in_channels = i\n",
    "    flag = False\n",
    "    for k, v in enumerate(cfg):\n",
    "        if in_channels != 'S':\n",
    "            if v == 'S':\n",
    "                if in_channels == 256:\n",
    "                    layers += [BasicRFB_c(in_channels, cfg[k+1], stride=2, scale = 1.0)]\n",
    "                else:\n",
    "                    layers += [BasicRFB(in_channels, cfg[k+1], stride=2, scale = 1.0)]\n",
    "            else:\n",
    "                layers += [BasicRFB(in_channels, v, scale = 1.0)]\n",
    "        in_channels = v\n",
    "    if size == 512:\n",
    "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
    "        layers += [BasicConv(128,256,kernel_size=4,stride=1,padding=1)]\n",
    "    elif size ==300:\n",
    "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
    "        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n",
    "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
    "        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n",
    "    else:\n",
    "        print(f\"GAK PASS : size = {size} add_extras: Error: Sorry only RFB300 and RFB512 are supported!\")\n",
    "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
    "        layers += [BasicConv(128,256,kernel_size=3,stride=1)]\n",
    "        layers += [BasicConv(256,128,kernel_size=1,stride=1)]\n",
    "    return layers\n",
    "\n",
    "extras = {\n",
    "    '300': [1024, 'S', 512, 'S', 256],\n",
    "    '512': [1024, 'S', 512, 'S', 256, 'S', 256,'S',256],\n",
    "}\n",
    "\n",
    "\n",
    "def multibox(size, vgg, extra_layers, cfg, num_classes):\n",
    "    loc_layers = []\n",
    "    conf_layers = []\n",
    "    vgg_source = [-2]\n",
    "    for k, v in enumerate(vgg_source):\n",
    "        if k == 0:\n",
    "            loc_layers += [nn.Conv2d(512,\n",
    "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
    "            conf_layers +=[nn.Conv2d(512,\n",
    "                                 cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
    "        else:\n",
    "            loc_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                                 cfg[k] * 4, kernel_size=3, padding=1)]\n",
    "            conf_layers += [nn.Conv2d(vgg[v].out_channels,\n",
    "                        cfg[k] * num_classes, kernel_size=3, padding=1)]\n",
    "    i = 1\n",
    "    indicator = 0\n",
    "    if size == 300:\n",
    "        indicator = 3\n",
    "    elif size == 512:\n",
    "        indicator = 5\n",
    "    else:\n",
    "        indicator = 5\n",
    "        print(f\"GAK PASS indicator {indicator}: multibox: Error: Sorry only RFB300 and RFB512 are supported!\")\n",
    "        # return\n",
    "\n",
    "    for k, v in enumerate(extra_layers):\n",
    "        if k < indicator or k%2== 0:\n",
    "            loc_layers += [nn.Conv2d(v.out_channels, cfg[i]\n",
    "                                 * 4, kernel_size=3, padding=1)]\n",
    "            conf_layers += [nn.Conv2d(v.out_channels, cfg[i]\n",
    "                                  * num_classes, kernel_size=3, padding=1)]\n",
    "            i +=1\n",
    "    return vgg, extra_layers, (loc_layers, conf_layers)\n",
    "\n",
    "mbox = {\n",
    "    '300': [6, 6, 6, 6, 4, 4],  # number of boxes per feature map location\n",
    "    '512': [6, 6, 6, 6, 6, 4, 4],\n",
    "}\n",
    "\n",
    "\n",
    "def build_net(img_size=512, num_classes=2):\n",
    "    if img_size not in [300, 512]:\n",
    "        print(f\"GAK PASS : size = {img_size} Error: only RFBE300 and RFBE512 are supported!\")\n",
    "        # return\n",
    "\n",
    "    if img_size not in base.keys():\n",
    "      base[str(img_size)] = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',\n",
    "            512, 512, 512]\n",
    "\n",
    "    if img_size not in extras.keys():\n",
    "      extras[str(img_size)] = [1024, 'S', 512, 'S', 256, 'S', 256,'S',256]\n",
    "\n",
    "    if img_size not in mbox.keys():\n",
    "      mbox[str(img_size)] = [6, 6, 6, 6, 6, 4, 4]\n",
    "      \n",
    "    # print(f\"base = {base}\")\n",
    "    # print()\n",
    "    # print(f\"extras = {extras}\")\n",
    "    # print('\\nNetwork: RFBE_VGG. Image size: %d. Number of classes: %d' % (img_size, num_classes))\n",
    "\n",
    "    return RFBNet(img_size, *multibox(img_size, vgg(base[str(img_size)], 3),\n",
    "                                add_extras(img_size, extras[str(img_size)], 1024),\n",
    "                                mbox[str(img_size)], num_classes), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap (IoU) between two sets of boxes.\n",
    "    The jaccard overlap is simply the intersection over union of two boxes.\n",
    "    \n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects, 4].\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors, 4].\n",
    "    \n",
    "    Return:\n",
    "        IoU: (tensor) Shape: [num_objects, num_priors]\n",
    "    \"\"\"\n",
    "    # Intersection box: find the max of the min coords and the min of the max coords\n",
    "    inter_xmin = torch.max(box_a[:, 0].unsqueeze(1), box_b[:, 0])\n",
    "    inter_ymin = torch.max(box_a[:, 1].unsqueeze(1), box_b[:, 1])\n",
    "    inter_xmax = torch.min(box_a[:, 2].unsqueeze(1), box_b[:, 2])\n",
    "    inter_ymax = torch.min(box_a[:, 3].unsqueeze(1), box_b[:, 3])\n",
    "\n",
    "    # Compute the width and height of the intersection box\n",
    "    inter_w = torch.clamp(inter_xmax - inter_xmin, min=0)\n",
    "    inter_h = torch.clamp(inter_ymax - inter_ymin, min=0)\n",
    "\n",
    "    # Intersection area\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    # Area of both the boxes\n",
    "    area_a = (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1])  # [num_objects]\n",
    "    area_b = (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1])  # [num_priors]\n",
    "\n",
    "    # Union area\n",
    "    union_area = area_a.unsqueeze(1) + area_b - inter_area\n",
    "\n",
    "    # IoU calculation\n",
    "    return inter_area / union_area  # [num_objects, num_priors]\n",
    "\n",
    "def point_form(boxes):\n",
    "    \"\"\" Convert prior boxes from (cx, cy, w, h) to (xmin, ymin, xmax, ymax).\n",
    "    Args:\n",
    "        boxes: (tensor) prior boxes in center-offset form (cx, cy, w, h).\n",
    "                Shape: [num_priors, 4]\n",
    "    Return:\n",
    "        boxes: (tensor) Converted boxes in point (corner) form (xmin, ymin, xmax, ymax).\n",
    "                Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "    return torch.cat((boxes[:, :2] - boxes[:, 2:] / 2,  # xmin, ymin\n",
    "                      boxes[:, :2] + boxes[:, 2:] / 2), 1)  # xmax, ymax\n",
    "\n",
    "\n",
    "############################################\n",
    "# from lib.utils.box_utils import match, log_sum_exp\n",
    "def log_sum_exp(x):\n",
    "    \"\"\"Utility function for computing log_sum_exp while determining\n",
    "    This will be used to determine unaveraged confidence loss across\n",
    "    all examples in a batch.\n",
    "    Args:\n",
    "        x (Variable(tensor)): conf_preds from conf layers\n",
    "    \"\"\"\n",
    "    x_max = x.data.max()\n",
    "    return torch.log(torch.sum(torch.exp(x-x_max), 1, keepdim=True)) + x_max\n",
    "def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):\n",
    "    \"\"\"Match each prior box with the ground truth box of the highest jaccard\n",
    "    overlap, encode the bounding boxes, then return the matched indices\n",
    "    corresponding to both confidence and location preds.\n",
    "    Args:\n",
    "        threshold: (float) The overlap threshold used when mathing boxes.\n",
    "        truths: (tensor) Ground truth boxes, Shape: [num_obj, num_priors].\n",
    "        priors: (tensor) Prior boxes from priorbox layers, Shape: [n_priors,4].\n",
    "        variances: (tensor) Variances corresponding to each prior coord,\n",
    "            Shape: [num_priors, 4].\n",
    "        labels: (tensor) All the class labels for the image, Shape: [num_obj].\n",
    "        loc_t: (tensor) Tensor to be filled w/ endcoded location targets.\n",
    "        conf_t: (tensor) Tensor to be filled w/ matched indices for conf preds.\n",
    "        idx: (int) current batch index\n",
    "    Return:\n",
    "        The matched indices corresponding to 1)location and 2)confidence preds.\n",
    "    \"\"\"\n",
    "    # jaccard index\n",
    "    overlaps = jaccard(\n",
    "        truths,\n",
    "        point_form(priors)\n",
    "    )\n",
    "    # (Bipartite Matching)\n",
    "    # [1,num_objects] best prior for each ground truth\n",
    "    best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)\n",
    "    # [1,num_priors] best ground truth for each prior\n",
    "    best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)\n",
    "    best_truth_idx.squeeze_(0)\n",
    "    best_truth_overlap.squeeze_(0)\n",
    "    best_prior_idx.squeeze_(1)\n",
    "    best_prior_overlap.squeeze_(1)\n",
    "    best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior\n",
    "    # TODO refactor: index  best_prior_idx with long tensor\n",
    "    # ensure every gt matches with its prior of max overlap\n",
    "    for j in range(best_prior_idx.size(0)):\n",
    "        best_truth_idx[best_prior_idx[j]] = j\n",
    "    matches = truths[best_truth_idx]          # Shape: [num_priors,4]\n",
    "    conf = labels[best_truth_idx]          # Shape: [num_priors]\n",
    "    conf[best_truth_overlap < threshold] = 0  # label as background\n",
    "    loc = encode(matches, priors, variances)\n",
    "    loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn\n",
    "    conf_t[idx] = conf  # [num_priors] top class label for each prior\n",
    "\n",
    "def encode(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth boxes\n",
    "    we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of priorboxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:])/2 - priors[:, :2]\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = torch.log(g_wh) / variances[1]\n",
    "\n",
    "    # return target for smooth_l1_loss\n",
    "    return torch.cat([g_cxcy, g_wh], 1)  # [num_priors,4]\n",
    "############################################\n",
    "\n",
    "class MultiBoxLoss(nn.Module):\n",
    "    \"\"\"SSD Weighted Loss Function\n",
    "    Compute Targets:\n",
    "        1) Produce Confidence Target Indices by matching  ground truth boxes\n",
    "           with (default) 'priorboxes' that have jaccard index > threshold parameter\n",
    "           (default threshold: 0.5).\n",
    "        2) Produce localization target by 'encoding' variance into offsets of ground\n",
    "           truth boxes and their matched  'priorboxes'.\n",
    "        3) Hard negative mining to filter the excessive number of negative examples\n",
    "           that comes with using a large number of default bounding boxes.\n",
    "           (default negative:positive ratio 3:1)\n",
    "    Objective Loss:\n",
    "        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss\n",
    "        weighted by α which is set to 1 by cross val.\n",
    "        Args:\n",
    "            c: class confidences,\n",
    "            l: predicted boxes,\n",
    "            g: ground truth boxes\n",
    "            N: number of matched default boxes\n",
    "        See: https://arxiv.org/pdf/1512.02325.pdf for more details.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, num_classes, overlap_thresh, negpos_ratio):\n",
    "        super(MultiBoxLoss, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.threshold = overlap_thresh\n",
    "        self.negpos_ratio = negpos_ratio\n",
    "        self.variance = [0.1, 0.2]\n",
    "\n",
    "    # TODO: update losses: https://pytorch.org/docs/stable/nn.html?highlight=f%20cross_entropy#torch.nn.functional.cross_entropy\n",
    "    def forward(self, predictions, priors, targets):\n",
    "        \"\"\"Multibox Loss\n",
    "        Args:\n",
    "            predictions (tuple): A tuple containing loc preds, conf preds,\n",
    "            and prior boxes from SSD net.\n",
    "                conf shape: torch.size(batch_size,num_priors,num_classes)\n",
    "                loc shape: torch.size(batch_size,num_priors,4)\n",
    "                priors shape: torch.size(num_priors,4)\n",
    "\n",
    "            ground_truth (tensor): Ground truth boxes and labels for a batch,\n",
    "                shape: [batch_size,num_objs,5] (last idx is the label).\n",
    "        \"\"\"\n",
    "\n",
    "        # print(\"INSIDE MultiBoxLoss forward()\")\n",
    "        # print(f\"type(predictions) = {type(predictions)}\")       #  <class 'tuple'>\n",
    "        # print(f\"len(predictions) = {len(predictions)}\")         # 2\n",
    "        # print(f\"predictions[0].shape = {predictions[0].shape}\") # torch.Size([1, 40376, 4])\n",
    "        # # print(f\"predictions = {predictions}\")\n",
    "        # print()\n",
    "        # print(f\"type(priors) = {type(priors)}\")                # <class 'torch.Tensor'>\n",
    "        # print(f\"priors.shape = {priors.shape}\")                # torch.Size([32756, 4])\n",
    "        # # print(f\"priors = {priors}\")\n",
    "        # print()\n",
    "        # print(f\"type(targets) = {type(targets)}\")              # <class 'list'>\n",
    "        # print(f\"len(targets) = {len(targets)}\")                # 1\n",
    "        # print(f\"targets[0].shape = {targets[0].shape}\")        # torch.Size([4, 5])\n",
    "        # print(f\"targets = {targets}\")\n",
    "\n",
    "        loc_data, conf_data = predictions\n",
    "        priors = priors\n",
    "        num = loc_data.size(0)\n",
    "        num_priors = (priors.size(0))\n",
    "\n",
    "        # match priors (default boxes) and ground truth boxes\n",
    "        loc_t = torch.Tensor(num, num_priors, 4)\n",
    "        conf_t = torch.LongTensor(num, num_priors)\n",
    "        for idx in range(num):\n",
    "            truths = targets[idx][:,:-1].data\n",
    "            labels = targets[idx][:,-1].data\n",
    "            defaults = priors.data\n",
    "            match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, idx)\n",
    "\n",
    "\n",
    "        ## GAK WAS HERE \n",
    "        ## 01\n",
    "        # loc_t = loc_t.cuda()\n",
    "        # conf_t = conf_t.cuda()\n",
    "        ## / 01\n",
    "        ## 02\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        loc_t = loc_t.to(device)\n",
    "        conf_t = conf_t.to(device)\n",
    "        ## / 02\n",
    "\n",
    "        pos = conf_t > 0\n",
    "\n",
    "        # Localization Loss (Smooth L1)\n",
    "        # Shape: [batch,num_priors,4]\n",
    "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
    "        loc_p = loc_data[pos_idx].view(-1,4)\n",
    "        loc_t = loc_t[pos_idx].view(-1,4)\n",
    "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
    "\n",
    "        # Compute max conf across batch for hard negative mining\n",
    "        batch_conf = conf_data.view(-1, self.num_classes)\n",
    "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1,1))\n",
    "\n",
    "        # Hard Negative Mining\n",
    "        loss_c[pos.view(-1,1)] = 0 # filter out pos boxes for now\n",
    "        loss_c = loss_c.view(num, -1)\n",
    "        _,loss_idx = loss_c.sort(1, descending=True)\n",
    "        _,idx_rank = loss_idx.sort(1)\n",
    "        num_pos = pos.long().sum(1,keepdim=True)\n",
    "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
    "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
    "\n",
    "        # Confidence Loss Including Positive and Negative Examples\n",
    "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
    "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
    "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1,self.num_classes)\n",
    "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
    "        loss_c = F.cross_entropy(conf_p, targets_weighted, reduction='sum')\n",
    "\n",
    "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
    "\n",
    "        N = max(num_pos.sum().float(), 1.)\n",
    "        loss_l /= N\n",
    "        loss_c /= N\n",
    "\n",
    "        return loss_l, loss_c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorbox_params={\n",
    "    \"RFB_512\" : {\n",
    "        'feature_maps' : [64, 32, 16, 8, 4, 2, 1], # @@@@@@@@@@@@@@@@@@\n",
    "\n",
    "        'min_dim' : 512,\n",
    "\n",
    "        'steps' : [8, 16, 32, 64, 128, 256, 512],\n",
    "\n",
    "        'min_sizes' : [20.48, 51.2, 133.12, 215.04, 296.96, 378.88, 460.8],\n",
    "\n",
    "        'max_sizes' : [51.2, 133.12, 215.04, 296.96, 378.88, 460.8, 542.72],\n",
    "\n",
    "        'aspect_ratios' : [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "\n",
    "        'variance' : [0.1, 0.2],\n",
    "\n",
    "        'clip' : True,\n",
    "    },\n",
    "\n",
    "\n",
    "    # VOC_300\n",
    "    # RFB VGG for 300x300\n",
    "    \"RFB_300\" : {\n",
    "        'feature_maps' : [38, 19, 10, 5, 3, 1],\n",
    "\n",
    "        'min_dim' : 300,\n",
    "\n",
    "        'steps' : [8, 16, 32, 64, 100, 300],\n",
    "\n",
    "        'min_sizes' : [30, 60, 111, 162, 213, 264],\n",
    "\n",
    "        'max_sizes' : [60, 111, 162, 213, 264, 315],\n",
    "\n",
    "        'aspect_ratios' : [[2, 3], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "\n",
    "        'variance' : [0.1, 0.2],\n",
    "\n",
    "        'clip' : True,\n",
    "    },\n",
    "}\n",
    "\n",
    "class Anchors_W(object):\n",
    "    \"\"\"Compute anchor box coordinates in center-offset form for each source feature map.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg):\n",
    "        self.image_size = cfg['min_dim']                     # 512\n",
    "        self.variance = cfg['variance'] or [0.1, 0.2]        # [0.1, 0.2]\n",
    "        self.feature_maps = cfg['feature_maps']              # [64, 32, 16, 8, 4, 2, 1]\n",
    "        self.min_sizes = cfg['min_sizes']                    # [20.48, 51.2, 133.12, 215.04, 296.96, 378.88, 460.8]\n",
    "        self.max_sizes = cfg['max_sizes']                    # [51.2, 133.12, 215.04, 296.96, 378.88, 460.8, 542.72]\n",
    "        self.steps = cfg['steps']                            # [8, 16, 32, 64, 128, 256, 512]\n",
    "        self.aspect_ratios = cfg['aspect_ratios']            # [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
    "        self.clip = cfg['clip']                              # True\n",
    "\n",
    "        for v in self.variance:\n",
    "            if v <= 0:\n",
    "                raise ValueError('Variances must be greater than 0')\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"Generate the anchor boxes for each feature map.\"\"\"\n",
    "        anchor_boxes = []\n",
    "\n",
    "        for k, f in enumerate(self.feature_maps):  # Iterate through feature maps\n",
    "            for i, j in product(range(f), repeat=2):  # Iterate through grid positions\n",
    "                f_k = self.image_size / self.steps[k]  # e.g., 512 / [8, 16, 32, ...] -> [64.0, 32.0, ...]\n",
    "                cx = (j + 0.5) / f_k  # Center x\n",
    "                cy = (i + 0.5) / f_k  # Center y\n",
    "\n",
    "                # Create anchor for aspect ratio 1\n",
    "                s_k = self.min_sizes[k] / self.image_size  # Scale relative to image size\n",
    "                anchor_boxes += [cx, cy, s_k, s_k]\n",
    "\n",
    "                # Create an additional anchor box for aspect ratio 1 with larger size\n",
    "                s_k_prime = sqrt(s_k * (self.max_sizes[k] / self.image_size))\n",
    "                anchor_boxes += [cx, cy, s_k_prime, s_k_prime]\n",
    "\n",
    "                # Create anchors for other aspect ratios\n",
    "                for ar in self.aspect_ratios[k]:\n",
    "                    anchor_boxes += [cx, cy, s_k * sqrt(ar), s_k / sqrt(ar)]\n",
    "                    anchor_boxes += [cx, cy, s_k / sqrt(ar), s_k * sqrt(ar)]\n",
    "\n",
    "        # Convert the list of anchor boxes into a tensor\n",
    "        output = torch.Tensor(anchor_boxes).view(-1, 4)\n",
    "\n",
    "        # Clip the boxes to ensure coordinates are within [0, 1] range\n",
    "        if self.clip:\n",
    "            output.clamp_(max=1., min=0.)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated voc_eval function\n",
    "def voc_eval(detections, annotations, cid, ovthresh=0.5):\n",
    "    \"\"\"\n",
    "    Top-level function that does the PASCAL VOC evaluation.\n",
    "    Args:\n",
    "        detections: dict of detections for a specific class (keyed on image IDs), where\n",
    "                    each entry is a list of detection bounding boxes and confidences.\n",
    "        annotations: dict of ground truth annotations keyed on image IDs, where each entry\n",
    "                    is a list of ground truth boxes with their respective class IDs.\n",
    "        cid: Class ID to evaluate.\n",
    "        ovthresh: Overlap threshold for considering a detection as True Positive.\n",
    "    Returns:\n",
    "        rec: Recall for the class\n",
    "        prec: Precision for the class\n",
    "        ap: Average Precision (AP) for the class\n",
    "    \"\"\"\n",
    "    use_07_metric = False  # Use 11-point interpolation for PASCAL VOC 2007\n",
    "\n",
    "    # Extract gt objects for this class\n",
    "    class_recs = {}\n",
    "    npos = 0\n",
    "    for imagename in annotations:\n",
    "        # For each image, list of objects: [[x1, y1, x2, y2, class_id], ...]\n",
    "        R = [obj[:4] for obj in annotations[imagename] if int(obj[-1]) == cid]\n",
    "        bbox = np.array(R)\n",
    "        difficult = np.array([0] * len(R)).astype(np.bool)  # No difficult examples\n",
    "        det = [False] * len(R)\n",
    "        npos += sum(~difficult)\n",
    "        class_recs[imagename] = {'bbox': bbox, 'difficult': difficult, 'det': det}\n",
    "\n",
    "    # Detections for this class\n",
    "    image_ids = []\n",
    "    confidence = []\n",
    "    BB = []\n",
    "    class_dict = detections.get(cid, {})\n",
    "    for image_file in class_dict:\n",
    "        dets = class_dict[image_file]\n",
    "        for k in range(dets.shape[0]):\n",
    "            image_ids.append(image_file)\n",
    "            BB.append(dets[k, 0:4])\n",
    "            confidence.append(dets[k, -1])\n",
    "\n",
    "    BB = np.array(BB)\n",
    "    confidence = np.array(confidence)\n",
    "\n",
    "    nd = len(image_ids)\n",
    "    tp = np.zeros(nd)\n",
    "    fp = np.zeros(nd)\n",
    "    if npos == 0:  # Handle case where there are no ground truths for this class\n",
    "        return np.zeros(0), np.zeros(0), 0.0\n",
    "    if BB.shape[0] > 0:\n",
    "        # Sort by confidence\n",
    "        sorted_ind = np.argsort(-confidence)\n",
    "        BB = BB[sorted_ind, :]\n",
    "        image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "        # Go through detections and mark TPs and FPs\n",
    "        for d in range(nd):\n",
    "            # Check if the image ID exists in the ground truth for this class\n",
    "            if image_ids[d] not in class_recs:\n",
    "                fp[d] = 1.  # Count it as a false positive since no ground truth exists for this image\n",
    "                continue\n",
    "\n",
    "            R = class_recs[image_ids[d]]\n",
    "            bb = BB[d, :].astype(float)\n",
    "            ovmax = -np.inf\n",
    "            BBGT = R['bbox'].astype(float)\n",
    "\n",
    "            if BBGT.size > 0:\n",
    "                # Compute IoU overlaps\n",
    "                ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "                iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "                ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "                iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "                iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "                ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "                inters = iw * ih\n",
    "                uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) + \n",
    "                       (BBGT[:, 2] - BBGT[:, 0] + 1.) * (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
    "                overlaps = inters / uni\n",
    "                ovmax = np.max(overlaps)\n",
    "                jmax = np.argmax(overlaps)\n",
    "\n",
    "            if ovmax > ovthresh:\n",
    "                if not R['difficult'][jmax]:\n",
    "                    if not R['det'][jmax]:\n",
    "                        tp[d] = 1.\n",
    "                        R['det'][jmax] = 1\n",
    "                    else:\n",
    "                        fp[d] = 1.\n",
    "            else:\n",
    "                fp[d] = 1.\n",
    "\n",
    "    # Compute precision-recall\n",
    "    fp = np.cumsum(fp)\n",
    "    tp = np.cumsum(tp)\n",
    "    rec = tp / float(npos)\n",
    "    prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "    ap = voc_ap(rec, prec, use_07_metric)\n",
    "\n",
    "    return rec, prec, ap\n",
    "\n",
    "\n",
    "# Helper function to compute AP\n",
    "def voc_ap(rec, prec, use_07_metric=False):\n",
    "    \"\"\" Compute the average precision (AP) using either the 07 or 12/10 metric \"\"\"\n",
    "    if use_07_metric:\n",
    "        ap = 0.\n",
    "        for t in np.arange(0., 1.1, 0.1):\n",
    "            if np.sum(rec >= t) == 0:\n",
    "                p = 0\n",
    "            else:\n",
    "                p = np.max(prec[rec >= t])\n",
    "            ap += p / 11.\n",
    "    else:\n",
    "        mrec = np.concatenate(([0.], rec, [1.]))\n",
    "        mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "        for i in range(mpre.size - 1, 0, -1):\n",
    "            mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "        i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "\n",
    "    return ap\n",
    "\n",
    "# Evaluation function for all classes\n",
    "def evaluate_detections(detections_dict, annotations, classes, iou_thresh=0.5):\n",
    "    aps = []\n",
    "    print('--------------------------------')\n",
    "    print('Class : \\t AP (100)')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    for cid, classname in enumerate(classes):\n",
    "        if classname == '__background__':\n",
    "            continue\n",
    "        # print(f\"$$$$ voc_eval detections_dict= {detections_dict} <> annotations = {annotations} <>  cid = {cid} <> iou_thresh = {iou_thresh}\" )       \n",
    "        rec, prec, ap = voc_eval(detections_dict, annotations, cid, iou_thresh)\n",
    "        # print(f\"@@@@ voc_eval\")  \n",
    "        aps.append(ap)\n",
    "        print(f'{classname} : \\t{100 * ap:.2f}')\n",
    "\n",
    "    mAP = 100 * np.mean(aps)\n",
    "    print('--------------------------------')\n",
    "    print(f'mAP: \\t{mAP:.2f}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    return aps, mAP\n",
    "\n",
    "\n",
    "## gak was here asd\n",
    "\n",
    "\n",
    "# # Print mAP\n",
    "# print(f'Epoch {epoch+1}, mAP: {mAP:.2f}')\n",
    "\n",
    "# # Step the scheduler at the end of each epoch to adjust learning rate\n",
    "# scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_cfg = anchorbox_params[\"RFB_512\"]\n",
    "anchors_w = Anchors_W(anchor_cfg)\n",
    "anchors = anchors_w.forward()  ## qwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered RFB========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaura\\AppData\\Local\\Temp\\ipykernel_18808\\1391546839.py:232: UserWarning: `nn.functional.upsample` is deprecated. Use `nn.functional.interpolate` instead.\n",
      "  s2 = F.upsample(s2, scale_factor=2, mode='bilinear', align_corners=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loc Loss: 159250.46875, Train Conf Loss: 12.415828704833984\n",
      "Epoch 1, Val Loc Loss: 40913.890625, Val Conf Loss: 3.415937900543213\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Assuming img_size=512 and num_classes=5 as per your dataset\n",
    "img_size = 512\n",
    "num_classes = 5\n",
    "batch_size = 2\n",
    "num_epochs = 1\n",
    "\n",
    "# Split data into training and validation sets\n",
    "val_percent = 0.2\n",
    "len_all_data_512 = 10\n",
    "val_start_index = len_all_data_512 - int(len_all_data_512*val_percent)\n",
    "\n",
    "train_data = all_data_512[:val_start_index]  # Example: First 8 images for training\n",
    "val_data = all_data_512[val_start_index:10]    # Example: Last 2 images for validation\n",
    "\n",
    "# Instantiate the RFBNet model\n",
    "this_rfb = RFBNet(img_size, *multibox(img_size, vgg(base[str(img_size)], 3),\n",
    "                                add_extras(img_size, extras[str(img_size)], 1024),\n",
    "                                mbox[str(img_size)], num_classes), num_classes)\n",
    "\n",
    "# Set model to training mode\n",
    "this_rfb.train()\n",
    "\n",
    "# Instantiate the MultiBoxLoss\n",
    "multibox_loss = MultiBoxLoss(num_classes=num_classes, overlap_thresh=0.5, negpos_ratio=3)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(this_rfb.parameters(), lr=0.001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Generate anchor boxes using Anchors_W class\n",
    "anchor_cfg = anchorbox_params[\"RFB_512\"]\n",
    "anchors_w = Anchors_W(anchor_cfg)\n",
    "priors = anchors_w.forward()\n",
    "\n",
    "# Move model and priors to the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "this_rfb.to(device)\n",
    "priors = priors.to(device)\n",
    "\n",
    "### TRAINING AND VALIDATION LOOP ###\n",
    "for epoch in range(num_epochs):\n",
    "    ### TRAINING ###\n",
    "    this_rfb.train()  # Set the model to training mode\n",
    "    for i in range(0, len(train_data), batch_size):\n",
    "        # Gather batch of images, bounding boxes, and class labels from training data\n",
    "        batch_images = [train_data[j][0] for j in range(i, min(i + batch_size, len(train_data)))]\n",
    "        batch_boxes = [train_data[j][1] for j in range(i, min(i + batch_size, len(train_data)))]\n",
    "        batch_labels = [train_data[j][2] for j in range(i, min(i + batch_size, len(train_data)))]\n",
    "        \n",
    "        # Stack the batch of images into a single tensor\n",
    "        batch_images = torch.stack(batch_images).to(device)\n",
    "\n",
    "        # Forward pass through the model to get predictions (loc_preds, conf_preds)\n",
    "        loc_preds, conf_preds = this_rfb(batch_images)\n",
    "\n",
    "        # Prepare ground truth targets (bounding boxes + labels)\n",
    "        targets = [torch.cat([boxes, labels.unsqueeze(1)], dim=1).to(device) for boxes, labels in zip(batch_boxes, batch_labels)]\n",
    "\n",
    "        # Compute the loss\n",
    "        loss_l, loss_c = multibox_loss((loc_preds, conf_preds), priors, targets)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss = loss_l + loss_c\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    ### VALIDATION ###\n",
    "    this_rfb.eval()  # Set the model to evaluation mode\n",
    "    val_loc_loss = 0.0\n",
    "    val_conf_loss = 0.0\n",
    "\n",
    "    # Create dictionary to store detections for evaluation\n",
    "    detections_dict = {cid: {} for cid in range(num_classes)}  # cid corresponds to class ID\n",
    "    with torch.no_grad():  # Disable gradient calculation during validation\n",
    "        for i in range(0, len(val_data), batch_size):\n",
    "            # Gather batch of images, bounding boxes, and class labels from validation data\n",
    "            val_images = [val_data[j][0] for j in range(i, min(i + batch_size, len(val_data)))]\n",
    "            val_boxes = [val_data[j][1] for j in range(i, min(i + batch_size, len(val_data)))]\n",
    "            val_labels = [val_data[j][2] for j in range(i, min(i + batch_size, len(val_data)))]\n",
    "            \n",
    "            # Stack the batch of validation images into a single tensor\n",
    "            val_images = torch.stack(val_images).to(device)\n",
    "\n",
    "            # Forward pass through the model to get predictions (loc_preds, conf_preds)\n",
    "            val_loc_preds, val_conf_preds = this_rfb(val_images)\n",
    "\n",
    "            # Store predictions for evaluation later\n",
    "            for idx, img_id in enumerate(range(i, min(i + batch_size, len(val_data)))):\n",
    "                # Convert predictions into usable format\n",
    "                decoded_boxes = val_loc_preds[idx].cpu().detach().numpy()\n",
    "                confidences = val_conf_preds[idx].cpu().detach().numpy()\n",
    "                for cid in range(1, num_classes):  # Skipping background class (cid=0)\n",
    "                    class_mask = confidences[:, cid] > 0.5  # Apply threshold (0.5 example)\n",
    "                    if class_mask.sum() > 0:\n",
    "                        detections_dict[cid][img_id] = np.hstack([decoded_boxes[class_mask], confidences[class_mask, cid].reshape(-1, 1)])\n",
    "\n",
    "            # Prepare ground truth targets for validation\n",
    "            val_targets = [torch.cat([boxes, labels.unsqueeze(1)], dim=1).to(device) for boxes, labels in zip(val_boxes, val_labels)]\n",
    "\n",
    "            # Compute the loss on the validation set\n",
    "            val_loss_l, val_loss_c = multibox_loss((val_loc_preds, val_conf_preds), priors, val_targets)\n",
    "            val_loc_loss += val_loss_l.item()\n",
    "            val_conf_loss += val_loss_c.item()\n",
    "\n",
    "    # Average validation loss per image\n",
    "    avg_val_loc_loss = val_loc_loss / len(val_data)\n",
    "    avg_val_conf_loss = val_conf_loss / len(val_data)\n",
    "\n",
    "    # Print the losses for the current epoch\n",
    "    print(f'Epoch {epoch+1}, Train Loc Loss: {loss_l.item()}, Train Conf Loss: {loss_c.item()}')\n",
    "    print(f'Epoch {epoch+1}, Val Loc Loss: {avg_val_loc_loss}, Val Conf Loss: {avg_val_conf_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Class : \t AP (100)\n",
      "--------------------------------\n",
      "gun : \t0.00\n",
      "knife : \t0.00\n",
      "plier : \t0.00\n",
      "scissor : \t0.00\n",
      "wrench : \t0.00\n",
      "--------------------------------\n",
      "mAP: \t0.00\n",
      "--------------------------------\n",
      "Epoch 1, mAP: 0.00\n"
     ]
    }
   ],
   "source": [
    "annotations = {img_id: [all_data_512[img_id][1].cpu().numpy()] for img_id in range(val_start_index, val_start_index+len(val_data))}\n",
    "corrected_annotations = {}\n",
    "# print(f\"before correction annotations: {annotations}\")\n",
    "this_val_id = -1\n",
    "for k, v in annotations.items():\n",
    "    this_val_id += 1\n",
    "    final_list = []\n",
    "    true_ids = val_data[this_val_id][2].tolist()\n",
    "    # print(true_ids)\n",
    "    for tri, bb in zip(true_ids, v[0].tolist()):\n",
    "        bb.append(tri)\n",
    "        final_list.append(bb)\n",
    "    corrected_annotations[k] = final_list\n",
    "annotations = corrected_annotations\n",
    "\n",
    "# {0: 'gun', 1: 'knife', 2: 'plier', 3: 'scissor', 4: 'wrench'}\n",
    "\n",
    "aps, mAP = evaluate_detections(detections_dict, annotations, classes=['gun', 'knife', 'plier', 'scissor', 'wrench',  '__background__'])\n",
    "# Print mAP\n",
    "print(f'Epoch {epoch+1}, mAP: {mAP:.2f}')\n",
    "\n",
    "# Step the scheduler at the end of each epoch to adjust learning rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statsml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
